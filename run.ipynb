{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: tape_arxiv_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"arxiv_2023\"\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of parameters: 138632488\n",
      "Start running train at 05-16 00:26:39\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "  0%|                                       | 2/12316 [00:01<1:54:11,  1.80it/s]Traceback (most recent call last):\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/trainLM.py\", line 24, in <module>\n",
      "    run(cfg)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/trainLM.py\", line 12, in run\n",
      "    trainer.train()\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/utils.py\", line 80, in wrapper\n",
      "    ret = func(*args, **kw)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/LMs/lm_trainer.py\", line 119, in train\n",
      "    self.trainer.train()\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/transformers/trainer.py\", line 1859, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/transformers/trainer.py\", line 2203, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/transformers/trainer.py\", line 3138, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/transformers/trainer.py\", line 3161, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/accelerate/utils/operations.py\", line 825, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/accelerate/utils/operations.py\", line 813, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/torch/amp/autocast_mode.py\", line 12, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/LMs/model.py\", line 33, in forward\n",
      "    outputs = self.bert_encoder(input_ids=input_ids,\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py\", line 967, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py\", line 463, in forward\n",
      "    hidden_states = layer_module(\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py\", line 376, in forward\n",
      "    attention_output = self.attention(\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py\", line 309, in forward\n",
      "    self_output = self.self(\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py\", line 654, in forward\n",
      "    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py\", line 716, in disentangled_att_bias\n",
      "    p2c_att = torch.matmul(key_layer, pos_query_layer.transpose(-1, -2).to(dtype=key_layer.dtype))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 10.76 GiB total capacity; 9.71 GiB already allocated; 38.44 MiB free; 9.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  0%|                                       | 2/12316 [00:01<2:50:09,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "!WANDB_DISABLED=True TOKENIZERS_PARALLELISM=False CUDA_VISIBLE_DEVICES=0 python -m core.trainLM dataset $dataset seed $seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpt: gpt_responses/arxiv_2023\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Number of parameters: 124676392\n",
      "Start running train at 05-01 19:52:26\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "{'loss': 2.9329, 'grad_norm': 12.903623580932617, 'learning_rate': 1.2909090909090912e-05, 'epoch': 0.16}\n",
      "{'loss': 2.2975, 'grad_norm': 8.437657356262207, 'learning_rate': 1.9606790230382817e-05, 'epoch': 0.32}\n",
      " 11%|████▎                                 | 1388/12316 [03:02<23:59,  7.59it/s]\n",
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▋                                          | 2/128 [00:00<00:14,  8.80it/s]\u001b[A\n",
      "  2%|█                                          | 3/128 [00:00<00:20,  6.20it/s]\u001b[A\n",
      "  3%|█▎                                         | 4/128 [00:00<00:23,  5.37it/s]\u001b[A\n",
      "  4%|█▋                                         | 5/128 [00:00<00:24,  4.97it/s]\u001b[A\n",
      "  5%|██                                         | 6/128 [00:01<00:25,  4.76it/s]\u001b[A\n",
      "  5%|██▎                                        | 7/128 [00:01<00:26,  4.64it/s]\u001b[A\n",
      "  6%|██▋                                        | 8/128 [00:01<00:26,  4.55it/s]\u001b[A\n",
      "  7%|███                                        | 9/128 [00:01<00:26,  4.49it/s]\u001b[A\n",
      "  8%|███▎                                      | 10/128 [00:02<00:26,  4.45it/s]\u001b[A\n",
      "  9%|███▌                                      | 11/128 [00:02<00:26,  4.44it/s]\u001b[A\n",
      "  9%|███▉                                      | 12/128 [00:02<00:26,  4.42it/s]\u001b[A\n",
      " 10%|████▎                                     | 13/128 [00:02<00:26,  4.41it/s]\u001b[A\n",
      " 11%|████▌                                     | 14/128 [00:02<00:25,  4.40it/s]\u001b[A\n",
      " 12%|████▉                                     | 15/128 [00:03<00:25,  4.40it/s]\u001b[A\n",
      " 12%|█████▎                                    | 16/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 13%|█████▌                                    | 17/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 14%|█████▉                                    | 18/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 15%|██████▏                                   | 19/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▌                                   | 20/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▉                                   | 21/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 17%|███████▏                                  | 22/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 18%|███████▌                                  | 23/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 19%|███████▉                                  | 24/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▏                                 | 25/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▌                                 | 26/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 21%|████████▊                                 | 27/128 [00:05<00:23,  4.37it/s]\u001b[A\n",
      " 22%|█████████▏                                | 28/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▌                                | 29/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▊                                | 30/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 24%|██████████▏                               | 31/128 [00:06<00:22,  4.39it/s]\u001b[A\n",
      " 25%|██████████▌                               | 32/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 26%|██████████▊                               | 33/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▏                              | 34/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▍                              | 35/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 28%|███████████▊                              | 36/128 [00:07<00:20,  4.38it/s]\u001b[A\n",
      " 29%|████████████▏                             | 37/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▍                             | 38/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▊                             | 39/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 31%|█████████████▏                            | 40/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 32%|█████████████▍                            | 41/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 42/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 34%|██████████████                            | 43/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 34%|██████████████▍                           | 44/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 45/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 36%|███████████████                           | 46/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 47/128 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 48/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|████████████████                          | 49/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 50/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 51/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████                         | 52/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████▍                        | 53/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 54/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 43%|██████████████████                        | 55/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 56/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 57/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|███████████████████                       | 58/128 [00:13<00:16,  4.37it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 59/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 60/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████                      | 61/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 62/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 63/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 64/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 65/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 66/128 [00:14<00:14,  4.39it/s]\u001b[A\n",
      " 52%|█████████████████████▉                    | 67/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 68/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 54%|██████████████████████▋                   | 69/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 55%|██████████████████████▉                   | 70/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 55%|███████████████████████▎                  | 71/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 56%|███████████████████████▋                  | 72/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 73/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 74/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 59%|████████████████████████▌                 | 75/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 59%|████████████████████████▉                 | 76/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 60%|█████████████████████████▎                | 77/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 78/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|█████████████████████████▉                | 79/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▎               | 80/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 81/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 82/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 65%|███████████████████████████▏              | 83/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 66%|███████████████████████████▌              | 84/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 66%|███████████████████████████▉              | 85/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 86/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 68%|████████████████████████████▌             | 87/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 88/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 70%|█████████████████████████████▏            | 89/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 70%|█████████████████████████████▌            | 90/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 91/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 92/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 93/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▊           | 94/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 74%|███████████████████████████████▏          | 95/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 96/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 76%|███████████████████████████████▊          | 97/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 77%|████████████████████████████████▏         | 98/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 99/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 100/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 101/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 102/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 103/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 104/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 105/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 106/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 107/128 [00:24<00:04,  4.38it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 108/128 [00:24<00:04,  4.38it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 109/128 [00:24<00:04,  4.38it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▏     | 110/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 111/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 112/128 [00:25<00:03,  4.38it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 113/128 [00:25<00:03,  4.38it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 114/128 [00:25<00:03,  4.38it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▊    | 115/128 [00:26<00:02,  4.38it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 116/128 [00:26<00:02,  4.38it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 117/128 [00:26<00:02,  4.38it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 118/128 [00:26<00:02,  4.38it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 119/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 120/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 121/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████  | 122/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 123/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▋ | 124/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████ | 125/128 [00:28<00:00,  4.38it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 126/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 127/128 [00:28<00:00,  4.38it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.226099967956543, 'eval_accuracy': 0.7394748263888888, 'eval_runtime': 29.3148, 'eval_samples_per_second': 315.199, 'eval_steps_per_second': 4.401, 'epoch': 0.45}\n",
      " 11%|████▎                                 | 1388/12316 [03:32<23:59,  7.59it/s]\n",
      "100%|█████████████████████████████████████████| 128/128 [00:29<00:00,  4.19it/s]\u001b[A\n",
      "{'loss': 2.2541, 'grad_norm': 8.14975643157959, 'learning_rate': 1.874068941624805e-05, 'epoch': 0.49}\n",
      "{'loss': 2.2246, 'grad_norm': 8.472817420959473, 'learning_rate': 1.7874588602113286e-05, 'epoch': 0.65}\n",
      "{'loss': 2.188, 'grad_norm': 9.527338981628418, 'learning_rate': 1.700848778797852e-05, 'epoch': 0.81}\n",
      " 23%|████████▌                             | 2776/12316 [06:35<20:55,  7.60it/s]\n",
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▋                                          | 2/128 [00:00<00:14,  8.79it/s]\u001b[A\n",
      "  2%|█                                          | 3/128 [00:00<00:20,  6.18it/s]\u001b[A\n",
      "  3%|█▎                                         | 4/128 [00:00<00:23,  5.36it/s]\u001b[A\n",
      "  4%|█▋                                         | 5/128 [00:00<00:24,  4.98it/s]\u001b[A\n",
      "  5%|██                                         | 6/128 [00:01<00:25,  4.76it/s]\u001b[A\n",
      "  5%|██▎                                        | 7/128 [00:01<00:26,  4.64it/s]\u001b[A\n",
      "  6%|██▋                                        | 8/128 [00:01<00:26,  4.55it/s]\u001b[A\n",
      "  7%|███                                        | 9/128 [00:01<00:26,  4.50it/s]\u001b[A\n",
      "  8%|███▎                                      | 10/128 [00:02<00:26,  4.46it/s]\u001b[A\n",
      "  9%|███▌                                      | 11/128 [00:02<00:26,  4.44it/s]\u001b[A\n",
      "  9%|███▉                                      | 12/128 [00:02<00:26,  4.42it/s]\u001b[A\n",
      " 10%|████▎                                     | 13/128 [00:02<00:26,  4.40it/s]\u001b[A\n",
      " 11%|████▌                                     | 14/128 [00:02<00:25,  4.40it/s]\u001b[A\n",
      " 12%|████▉                                     | 15/128 [00:03<00:25,  4.40it/s]\u001b[A\n",
      " 12%|█████▎                                    | 16/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 13%|█████▌                                    | 17/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 14%|█████▉                                    | 18/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 15%|██████▏                                   | 19/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▌                                   | 20/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▉                                   | 21/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 17%|███████▏                                  | 22/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 18%|███████▌                                  | 23/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 19%|███████▉                                  | 24/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▏                                 | 25/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▌                                 | 26/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 21%|████████▊                                 | 27/128 [00:05<00:23,  4.37it/s]\u001b[A\n",
      " 22%|█████████▏                                | 28/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▌                                | 29/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▊                                | 30/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 24%|██████████▏                               | 31/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 25%|██████████▌                               | 32/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 26%|██████████▊                               | 33/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▏                              | 34/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▍                              | 35/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 28%|███████████▊                              | 36/128 [00:07<00:20,  4.38it/s]\u001b[A\n",
      " 29%|████████████▏                             | 37/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▍                             | 38/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▊                             | 39/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 31%|█████████████▏                            | 40/128 [00:08<00:20,  4.37it/s]\u001b[A\n",
      " 32%|█████████████▍                            | 41/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 42/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 34%|██████████████                            | 43/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 34%|██████████████▍                           | 44/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 45/128 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 36%|███████████████                           | 46/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 47/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 48/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|████████████████                          | 49/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 50/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 51/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████                         | 52/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████▍                        | 53/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 54/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 43%|██████████████████                        | 55/128 [00:12<00:16,  4.37it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 56/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 57/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|███████████████████                       | 58/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 59/128 [00:13<00:15,  4.37it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 60/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████                      | 61/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 62/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 63/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 64/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 65/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 66/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 52%|█████████████████████▉                    | 67/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 68/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 54%|██████████████████████▋                   | 69/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 55%|██████████████████████▉                   | 70/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 55%|███████████████████████▎                  | 71/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 56%|███████████████████████▋                  | 72/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 73/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 74/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 59%|████████████████████████▌                 | 75/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 59%|████████████████████████▉                 | 76/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 60%|█████████████████████████▎                | 77/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 78/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|█████████████████████████▉                | 79/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▎               | 80/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 81/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 82/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 65%|███████████████████████████▏              | 83/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 66%|███████████████████████████▌              | 84/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 66%|███████████████████████████▉              | 85/128 [00:19<00:09,  4.37it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 86/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 68%|████████████████████████████▌             | 87/128 [00:19<00:09,  4.37it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 88/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 70%|█████████████████████████████▏            | 89/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 70%|█████████████████████████████▌            | 90/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 91/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 92/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 93/128 [00:21<00:08,  4.37it/s]\u001b[A\n",
      " 73%|██████████████████████████████▊           | 94/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 74%|███████████████████████████████▏          | 95/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 96/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 76%|███████████████████████████████▊          | 97/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 77%|████████████████████████████████▏         | 98/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 99/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 100/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 101/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 102/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 103/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 104/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 105/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 106/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 107/128 [00:24<00:04,  4.38it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 108/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 109/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▏     | 110/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 111/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 112/128 [00:25<00:03,  4.38it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 113/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 114/128 [00:25<00:03,  4.38it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▊    | 115/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 116/128 [00:26<00:02,  4.38it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 117/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 118/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 119/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 120/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 121/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████  | 122/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 123/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▋ | 124/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████ | 125/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 126/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 127/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.1447763442993164, 'eval_accuracy': 0.7663845486111112, 'eval_runtime': 29.3259, 'eval_samples_per_second': 315.08, 'eval_steps_per_second': 4.399, 'epoch': 0.9}\n",
      " 23%|████████▌                             | 2776/12316 [07:05<20:55,  7.60it/s]\n",
      "100%|█████████████████████████████████████████| 128/128 [00:29<00:00,  4.18it/s]\u001b[A\n",
      "{'loss': 2.1669, 'grad_norm': 6.981544494628906, 'learning_rate': 1.6144119175472025e-05, 'epoch': 0.97}\n",
      "{'loss': 2.1452, 'grad_norm': 7.5849409103393555, 'learning_rate': 1.527801836133726e-05, 'epoch': 1.14}\n",
      "{'loss': 2.1412, 'grad_norm': 7.904451370239258, 'learning_rate': 1.4411917547202496e-05, 'epoch': 1.3}\n",
      " 34%|████████████▊                         | 4164/12316 [10:09<17:53,  7.60it/s]\n",
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▋                                          | 2/128 [00:00<00:14,  8.76it/s]\u001b[A\n",
      "  2%|█                                          | 3/128 [00:00<00:20,  6.19it/s]\u001b[A\n",
      "  3%|█▎                                         | 4/128 [00:00<00:23,  5.36it/s]\u001b[A\n",
      "  4%|█▋                                         | 5/128 [00:00<00:24,  4.97it/s]\u001b[A\n",
      "  5%|██                                         | 6/128 [00:01<00:25,  4.76it/s]\u001b[A\n",
      "  5%|██▎                                        | 7/128 [00:01<00:26,  4.63it/s]\u001b[A\n",
      "  6%|██▋                                        | 8/128 [00:01<00:26,  4.55it/s]\u001b[A\n",
      "  7%|███                                        | 9/128 [00:01<00:26,  4.50it/s]\u001b[A\n",
      "  8%|███▎                                      | 10/128 [00:02<00:26,  4.46it/s]\u001b[A\n",
      "  9%|███▌                                      | 11/128 [00:02<00:26,  4.43it/s]\u001b[A\n",
      "  9%|███▉                                      | 12/128 [00:02<00:26,  4.42it/s]\u001b[A\n",
      " 10%|████▎                                     | 13/128 [00:02<00:26,  4.41it/s]\u001b[A\n",
      " 11%|████▌                                     | 14/128 [00:02<00:25,  4.40it/s]\u001b[A\n",
      " 12%|████▉                                     | 15/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 12%|█████▎                                    | 16/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 13%|█████▌                                    | 17/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 14%|█████▉                                    | 18/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 15%|██████▏                                   | 19/128 [00:04<00:24,  4.39it/s]\u001b[A\n",
      " 16%|██████▌                                   | 20/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▉                                   | 21/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 17%|███████▏                                  | 22/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 18%|███████▌                                  | 23/128 [00:05<00:23,  4.39it/s]\u001b[A\n",
      " 19%|███████▉                                  | 24/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▏                                 | 25/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▌                                 | 26/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 21%|████████▊                                 | 27/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 22%|█████████▏                                | 28/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▌                                | 29/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▊                                | 30/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 24%|██████████▏                               | 31/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 25%|██████████▌                               | 32/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 26%|██████████▊                               | 33/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▏                              | 34/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▍                              | 35/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 28%|███████████▊                              | 36/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 29%|████████████▏                             | 37/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▍                             | 38/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▊                             | 39/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 31%|█████████████▏                            | 40/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 32%|█████████████▍                            | 41/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 42/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 34%|██████████████                            | 43/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 34%|██████████████▍                           | 44/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 45/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 36%|███████████████                           | 46/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 47/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 48/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|████████████████                          | 49/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 50/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 51/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████                         | 52/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████▍                        | 53/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 54/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 43%|██████████████████                        | 55/128 [00:12<00:16,  4.37it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 56/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 57/128 [00:12<00:16,  4.37it/s]\u001b[A\n",
      " 45%|███████████████████                       | 58/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 59/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 60/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████                      | 61/128 [00:13<00:15,  4.37it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 62/128 [00:13<00:15,  4.37it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 63/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 64/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 65/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 66/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 52%|█████████████████████▉                    | 67/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 68/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 54%|██████████████████████▋                   | 69/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 55%|██████████████████████▉                   | 70/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 55%|███████████████████████▎                  | 71/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 56%|███████████████████████▋                  | 72/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 73/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 74/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 59%|████████████████████████▌                 | 75/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 59%|████████████████████████▉                 | 76/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 60%|█████████████████████████▎                | 77/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 78/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 62%|█████████████████████████▉                | 79/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▎               | 80/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 81/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 82/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 65%|███████████████████████████▏              | 83/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 66%|███████████████████████████▌              | 84/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 66%|███████████████████████████▉              | 85/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 86/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 68%|████████████████████████████▌             | 87/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 88/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 70%|█████████████████████████████▏            | 89/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 70%|█████████████████████████████▌            | 90/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 91/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 92/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 93/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▊           | 94/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 74%|███████████████████████████████▏          | 95/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 96/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 76%|███████████████████████████████▊          | 97/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 77%|████████████████████████████████▏         | 98/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 99/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 100/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 101/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 102/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 103/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 104/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 105/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 106/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 107/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 108/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 109/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▏     | 110/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 111/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 112/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 113/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 114/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▊    | 115/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 116/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 117/128 [00:26<00:02,  4.38it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 118/128 [00:26<00:02,  4.38it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 119/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 120/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 121/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████  | 122/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 123/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▋ | 124/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████ | 125/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 126/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 127/128 [00:28<00:00,  4.38it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.1307859420776367, 'eval_accuracy': 0.7728949652777778, 'eval_runtime': 29.3291, 'eval_samples_per_second': 315.046, 'eval_steps_per_second': 4.398, 'epoch': 1.35}\n",
      " 34%|████████████▊                         | 4164/12316 [10:38<17:53,  7.60it/s]\n",
      "100%|█████████████████████████████████████████| 128/128 [00:29<00:00,  4.18it/s]\u001b[A\n",
      "{'loss': 2.1275, 'grad_norm': 1.685044527053833, 'learning_rate': 1.354581673306773e-05, 'epoch': 1.46}\n",
      "{'loss': 2.1507, 'grad_norm': 6.695990085601807, 'learning_rate': 1.2679715918932964e-05, 'epoch': 1.62}\n",
      "{'loss': 2.1273, 'grad_norm': 7.956732273101807, 'learning_rate': 1.181534730642647e-05, 'epoch': 1.79}\n",
      " 45%|█████████████████▏                    | 5552/12316 [13:42<14:48,  7.61it/s]\n",
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▋                                          | 2/128 [00:00<00:14,  8.78it/s]\u001b[A\n",
      "  2%|█                                          | 3/128 [00:00<00:20,  6.20it/s]\u001b[A\n",
      "  3%|█▎                                         | 4/128 [00:00<00:23,  5.37it/s]\u001b[A\n",
      "  4%|█▋                                         | 5/128 [00:00<00:24,  4.97it/s]\u001b[A\n",
      "  5%|██                                         | 6/128 [00:01<00:25,  4.76it/s]\u001b[A\n",
      "  5%|██▎                                        | 7/128 [00:01<00:26,  4.63it/s]\u001b[A\n",
      "  6%|██▋                                        | 8/128 [00:01<00:26,  4.55it/s]\u001b[A\n",
      "  7%|███                                        | 9/128 [00:01<00:26,  4.49it/s]\u001b[A\n",
      "  8%|███▎                                      | 10/128 [00:02<00:26,  4.46it/s]\u001b[A\n",
      "  9%|███▌                                      | 11/128 [00:02<00:26,  4.44it/s]\u001b[A\n",
      "  9%|███▉                                      | 12/128 [00:02<00:26,  4.41it/s]\u001b[A\n",
      " 10%|████▎                                     | 13/128 [00:02<00:26,  4.41it/s]\u001b[A\n",
      " 11%|████▌                                     | 14/128 [00:02<00:25,  4.40it/s]\u001b[A\n",
      " 12%|████▉                                     | 15/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 12%|█████▎                                    | 16/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 13%|█████▌                                    | 17/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 14%|█████▉                                    | 18/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 15%|██████▏                                   | 19/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▌                                   | 20/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▉                                   | 21/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 17%|███████▏                                  | 22/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 18%|███████▌                                  | 23/128 [00:05<00:24,  4.37it/s]\u001b[A\n",
      " 19%|███████▉                                  | 24/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▏                                 | 25/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▌                                 | 26/128 [00:05<00:23,  4.37it/s]\u001b[A\n",
      " 21%|████████▊                                 | 27/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 22%|█████████▏                                | 28/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▌                                | 29/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▊                                | 30/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 24%|██████████▏                               | 31/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 25%|██████████▌                               | 32/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 26%|██████████▊                               | 33/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▏                              | 34/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▍                              | 35/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 28%|███████████▊                              | 36/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 29%|████████████▏                             | 37/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▍                             | 38/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▊                             | 39/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 31%|█████████████▏                            | 40/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 32%|█████████████▍                            | 41/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 42/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 34%|██████████████                            | 43/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 34%|██████████████▍                           | 44/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 45/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 36%|███████████████                           | 46/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 47/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 48/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|████████████████                          | 49/128 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 50/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 51/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████                         | 52/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████▍                        | 53/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 54/128 [00:12<00:16,  4.37it/s]\u001b[A\n",
      " 43%|██████████████████                        | 55/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 56/128 [00:12<00:16,  4.37it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 57/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|███████████████████                       | 58/128 [00:13<00:16,  4.37it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 59/128 [00:13<00:15,  4.37it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 60/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████                      | 61/128 [00:13<00:15,  4.37it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 62/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 63/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 64/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 65/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 66/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 52%|█████████████████████▉                    | 67/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 68/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 54%|██████████████████████▋                   | 69/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 55%|██████████████████████▉                   | 70/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 55%|███████████████████████▎                  | 71/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 56%|███████████████████████▋                  | 72/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 73/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 74/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 59%|████████████████████████▌                 | 75/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 59%|████████████████████████▉                 | 76/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 60%|█████████████████████████▎                | 77/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 78/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|█████████████████████████▉                | 79/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▎               | 80/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 81/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 82/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 65%|███████████████████████████▏              | 83/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 66%|███████████████████████████▌              | 84/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 66%|███████████████████████████▉              | 85/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 86/128 [00:19<00:09,  4.37it/s]\u001b[A\n",
      " 68%|████████████████████████████▌             | 87/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 88/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 70%|█████████████████████████████▏            | 89/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 70%|█████████████████████████████▌            | 90/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 91/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 92/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 93/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▊           | 94/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 74%|███████████████████████████████▏          | 95/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 96/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 76%|███████████████████████████████▊          | 97/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 77%|████████████████████████████████▏         | 98/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 99/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 100/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 101/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 102/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 103/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 104/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 105/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 106/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 107/128 [00:24<00:04,  4.38it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 108/128 [00:24<00:04,  4.38it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 109/128 [00:24<00:04,  4.38it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▏     | 110/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 111/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 112/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 113/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 114/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▊    | 115/128 [00:26<00:02,  4.38it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 116/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 117/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 118/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 119/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 120/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 121/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████  | 122/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 123/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▋ | 124/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████ | 125/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 126/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 127/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.124336004257202, 'eval_accuracy': 0.7719184027777778, 'eval_runtime': 29.3304, 'eval_samples_per_second': 315.031, 'eval_steps_per_second': 4.398, 'epoch': 1.8}\n",
      " 45%|█████████████████▏                    | 5552/12316 [14:12<14:48,  7.61it/s]\n",
      "100%|█████████████████████████████████████████| 128/128 [00:29<00:00,  4.17it/s]\u001b[A\n",
      "{'loss': 2.1132, 'grad_norm': 9.054150581359863, 'learning_rate': 1.0949246492291703e-05, 'epoch': 1.95}\n",
      "{'loss': 2.0962, 'grad_norm': 18.066970825195312, 'learning_rate': 1.0083145678156937e-05, 'epoch': 2.11}\n",
      " 56%|█████████████████████▍                | 6940/12316 [17:16<11:46,  7.60it/s]\n",
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▋                                          | 2/128 [00:00<00:14,  8.77it/s]\u001b[A\n",
      "  2%|█                                          | 3/128 [00:00<00:20,  6.18it/s]\u001b[A\n",
      "  3%|█▎                                         | 4/128 [00:00<00:23,  5.37it/s]\u001b[A\n",
      "  4%|█▋                                         | 5/128 [00:00<00:24,  4.97it/s]\u001b[A\n",
      "  5%|██                                         | 6/128 [00:01<00:25,  4.77it/s]\u001b[A\n",
      "  5%|██▎                                        | 7/128 [00:01<00:26,  4.63it/s]\u001b[A\n",
      "  6%|██▋                                        | 8/128 [00:01<00:26,  4.54it/s]\u001b[A\n",
      "  7%|███                                        | 9/128 [00:01<00:26,  4.49it/s]\u001b[A\n",
      "  8%|███▎                                      | 10/128 [00:02<00:26,  4.46it/s]\u001b[A\n",
      "  9%|███▌                                      | 11/128 [00:02<00:26,  4.44it/s]\u001b[A\n",
      "  9%|███▉                                      | 12/128 [00:02<00:26,  4.42it/s]\u001b[A\n",
      " 10%|████▎                                     | 13/128 [00:02<00:26,  4.41it/s]\u001b[A\n",
      " 11%|████▌                                     | 14/128 [00:02<00:25,  4.40it/s]\u001b[A\n",
      " 12%|████▉                                     | 15/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 12%|█████▎                                    | 16/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 13%|█████▌                                    | 17/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 14%|█████▉                                    | 18/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 15%|██████▏                                   | 19/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▌                                   | 20/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▉                                   | 21/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 17%|███████▏                                  | 22/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 18%|███████▌                                  | 23/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 19%|███████▉                                  | 24/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▏                                 | 25/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▌                                 | 26/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 21%|████████▊                                 | 27/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 22%|█████████▏                                | 28/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▌                                | 29/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▊                                | 30/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 24%|██████████▏                               | 31/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 25%|██████████▌                               | 32/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 26%|██████████▊                               | 33/128 [00:07<00:21,  4.37it/s]\u001b[A\n",
      " 27%|███████████▏                              | 34/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▍                              | 35/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 28%|███████████▊                              | 36/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 29%|████████████▏                             | 37/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▍                             | 38/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▊                             | 39/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 31%|█████████████▏                            | 40/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 32%|█████████████▍                            | 41/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 42/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 34%|██████████████                            | 43/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 34%|██████████████▍                           | 44/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 45/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 36%|███████████████                           | 46/128 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 47/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 48/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|████████████████                          | 49/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 50/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 51/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 41%|█████████████████                         | 52/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 41%|█████████████████▍                        | 53/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 54/128 [00:12<00:16,  4.37it/s]\u001b[A\n",
      " 43%|██████████████████                        | 55/128 [00:12<00:16,  4.37it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 56/128 [00:12<00:16,  4.37it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 57/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|███████████████████                       | 58/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 59/128 [00:13<00:15,  4.37it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 60/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████                      | 61/128 [00:13<00:15,  4.37it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 62/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 63/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 64/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 65/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 66/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 52%|█████████████████████▉                    | 67/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 68/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 54%|██████████████████████▋                   | 69/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 55%|██████████████████████▉                   | 70/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 55%|███████████████████████▎                  | 71/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 56%|███████████████████████▋                  | 72/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 73/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 74/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 59%|████████████████████████▌                 | 75/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 59%|████████████████████████▉                 | 76/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 60%|█████████████████████████▎                | 77/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 78/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|█████████████████████████▉                | 79/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▎               | 80/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 81/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 82/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 65%|███████████████████████████▏              | 83/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 66%|███████████████████████████▌              | 84/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 66%|███████████████████████████▉              | 85/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 86/128 [00:19<00:09,  4.37it/s]\u001b[A\n",
      " 68%|████████████████████████████▌             | 87/128 [00:19<00:09,  4.37it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 88/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 70%|█████████████████████████████▏            | 89/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 70%|█████████████████████████████▌            | 90/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 91/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 92/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 93/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▊           | 94/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 74%|███████████████████████████████▏          | 95/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 96/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 76%|███████████████████████████████▊          | 97/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 77%|████████████████████████████████▏         | 98/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 99/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 100/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 101/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 102/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 103/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 104/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 105/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 106/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 107/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 108/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 109/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▏     | 110/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 111/128 [00:25<00:03,  4.38it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 112/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 113/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 114/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▊    | 115/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 116/128 [00:26<00:02,  4.38it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 117/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 118/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 119/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 120/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 121/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████  | 122/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 123/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▋ | 124/128 [00:28<00:00,  4.38it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████ | 125/128 [00:28<00:00,  4.38it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 126/128 [00:28<00:00,  4.38it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 127/128 [00:28<00:00,  4.38it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.1092586517333984, 'eval_accuracy': 0.7829861111111112, 'eval_runtime': 29.3282, 'eval_samples_per_second': 315.055, 'eval_steps_per_second': 4.398, 'epoch': 2.25}\n",
      " 56%|█████████████████████▍                | 6940/12316 [17:45<11:46,  7.60it/s]\n",
      "100%|█████████████████████████████████████████| 128/128 [00:29<00:00,  4.18it/s]\u001b[A\n",
      "{'loss': 2.0763, 'grad_norm': 9.200207710266113, 'learning_rate': 9.217044864022174e-06, 'epoch': 2.27}\n",
      "{'loss': 2.0705, 'grad_norm': 11.83515739440918, 'learning_rate': 8.352676251515676e-06, 'epoch': 2.44}\n",
      "{'loss': 2.0814, 'grad_norm': 11.676992416381836, 'learning_rate': 7.486575437380912e-06, 'epoch': 2.6}\n",
      " 68%|█████████████████████████▋            | 8328/12316 [20:49<08:45,  7.59it/s]\n",
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▋                                          | 2/128 [00:00<00:14,  8.79it/s]\u001b[A\n",
      "  2%|█                                          | 3/128 [00:00<00:20,  6.20it/s]\u001b[A\n",
      "  3%|█▎                                         | 4/128 [00:00<00:23,  5.36it/s]\u001b[A\n",
      "  4%|█▋                                         | 5/128 [00:00<00:24,  4.97it/s]\u001b[A\n",
      "  5%|██                                         | 6/128 [00:01<00:25,  4.76it/s]\u001b[A\n",
      "  5%|██▎                                        | 7/128 [00:01<00:26,  4.63it/s]\u001b[A\n",
      "  6%|██▋                                        | 8/128 [00:01<00:26,  4.55it/s]\u001b[A\n",
      "  7%|███                                        | 9/128 [00:01<00:26,  4.49it/s]\u001b[A\n",
      "  8%|███▎                                      | 10/128 [00:02<00:26,  4.46it/s]\u001b[A\n",
      "  9%|███▌                                      | 11/128 [00:02<00:26,  4.44it/s]\u001b[A\n",
      "  9%|███▉                                      | 12/128 [00:02<00:26,  4.42it/s]\u001b[A\n",
      " 10%|████▎                                     | 13/128 [00:02<00:26,  4.41it/s]\u001b[A\n",
      " 11%|████▌                                     | 14/128 [00:02<00:25,  4.40it/s]\u001b[A\n",
      " 12%|████▉                                     | 15/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 12%|█████▎                                    | 16/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 13%|█████▌                                    | 17/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 14%|█████▉                                    | 18/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 15%|██████▏                                   | 19/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▌                                   | 20/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▉                                   | 21/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 17%|███████▏                                  | 22/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 18%|███████▌                                  | 23/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 19%|███████▉                                  | 24/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▏                                 | 25/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▌                                 | 26/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 21%|████████▊                                 | 27/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 22%|█████████▏                                | 28/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▌                                | 29/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▊                                | 30/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 24%|██████████▏                               | 31/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 25%|██████████▌                               | 32/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 26%|██████████▊                               | 33/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▏                              | 34/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▍                              | 35/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 28%|███████████▊                              | 36/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 29%|████████████▏                             | 37/128 [00:08<00:20,  4.37it/s]\u001b[A\n",
      " 30%|████████████▍                             | 38/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▊                             | 39/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 31%|█████████████▏                            | 40/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 32%|█████████████▍                            | 41/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 42/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 34%|██████████████                            | 43/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 34%|██████████████▍                           | 44/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 45/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 36%|███████████████                           | 46/128 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 47/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 48/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|████████████████                          | 49/128 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 50/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 51/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 41%|█████████████████                         | 52/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 41%|█████████████████▍                        | 53/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 54/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 43%|██████████████████                        | 55/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 56/128 [00:12<00:16,  4.37it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 57/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|███████████████████                       | 58/128 [00:13<00:16,  4.37it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 59/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 60/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████                      | 61/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 62/128 [00:13<00:15,  4.37it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 63/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 64/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 65/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 66/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 52%|█████████████████████▉                    | 67/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 68/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 54%|██████████████████████▋                   | 69/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 55%|██████████████████████▉                   | 70/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 55%|███████████████████████▎                  | 71/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 56%|███████████████████████▋                  | 72/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 73/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 74/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 59%|████████████████████████▌                 | 75/128 [00:16<00:12,  4.38it/s]\u001b[A\n",
      " 59%|████████████████████████▉                 | 76/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 60%|█████████████████████████▎                | 77/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 78/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|█████████████████████████▉                | 79/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 62%|██████████████████████████▎               | 80/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 81/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 82/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 65%|███████████████████████████▏              | 83/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 66%|███████████████████████████▌              | 84/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 66%|███████████████████████████▉              | 85/128 [00:19<00:09,  4.37it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 86/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 68%|████████████████████████████▌             | 87/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 88/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 70%|█████████████████████████████▏            | 89/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 70%|█████████████████████████████▌            | 90/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 91/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 92/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 93/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▊           | 94/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 74%|███████████████████████████████▏          | 95/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 96/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 76%|███████████████████████████████▊          | 97/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 77%|████████████████████████████████▏         | 98/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 99/128 [00:22<00:06,  4.38it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 100/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 101/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 102/128 [00:23<00:05,  4.38it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 103/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 104/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 105/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 106/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 107/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 108/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 109/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▏     | 110/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 111/128 [00:25<00:03,  4.36it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 112/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 113/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 114/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▊    | 115/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 116/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 117/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 118/128 [00:26<00:02,  4.38it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 119/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 120/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 121/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████  | 122/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 123/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▋ | 124/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████ | 125/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 126/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 127/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.1087427139282227, 'eval_accuracy': 0.7815755208333334, 'eval_runtime': 29.3339, 'eval_samples_per_second': 314.993, 'eval_steps_per_second': 4.398, 'epoch': 2.7}\n",
      " 68%|█████████████████████████▋            | 8328/12316 [21:19<08:45,  7.59it/s]\n",
      "100%|█████████████████████████████████████████| 128/128 [00:29<00:00,  4.18it/s]\u001b[A\n",
      "{'loss': 2.0727, 'grad_norm': 5.695938587188721, 'learning_rate': 6.620474623246146e-06, 'epoch': 2.76}\n",
      "{'loss': 2.0694, 'grad_norm': 5.461425304412842, 'learning_rate': 5.754373809111381e-06, 'epoch': 2.92}\n",
      "{'loss': 2.0411, 'grad_norm': 3.382394790649414, 'learning_rate': 4.888272994976616e-06, 'epoch': 3.09}\n",
      " 79%|█████████████████████████████▉        | 9716/12316 [24:23<05:42,  7.60it/s]\n",
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▋                                          | 2/128 [00:00<00:14,  8.79it/s]\u001b[A\n",
      "  2%|█                                          | 3/128 [00:00<00:20,  6.19it/s]\u001b[A\n",
      "  3%|█▎                                         | 4/128 [00:00<00:23,  5.36it/s]\u001b[A\n",
      "  4%|█▋                                         | 5/128 [00:00<00:24,  4.97it/s]\u001b[A\n",
      "  5%|██                                         | 6/128 [00:01<00:25,  4.76it/s]\u001b[A\n",
      "  5%|██▎                                        | 7/128 [00:01<00:26,  4.63it/s]\u001b[A\n",
      "  6%|██▋                                        | 8/128 [00:01<00:26,  4.55it/s]\u001b[A\n",
      "  7%|███                                        | 9/128 [00:01<00:26,  4.49it/s]\u001b[A\n",
      "  8%|███▎                                      | 10/128 [00:02<00:26,  4.45it/s]\u001b[A\n",
      "  9%|███▌                                      | 11/128 [00:02<00:26,  4.43it/s]\u001b[A\n",
      "  9%|███▉                                      | 12/128 [00:02<00:26,  4.42it/s]\u001b[A\n",
      " 10%|████▎                                     | 13/128 [00:02<00:26,  4.41it/s]\u001b[A\n",
      " 11%|████▌                                     | 14/128 [00:02<00:25,  4.40it/s]\u001b[A\n",
      " 12%|████▉                                     | 15/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 12%|█████▎                                    | 16/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 13%|█████▌                                    | 17/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 14%|█████▉                                    | 18/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 15%|██████▏                                   | 19/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▌                                   | 20/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▉                                   | 21/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 17%|███████▏                                  | 22/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 18%|███████▌                                  | 23/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 19%|███████▉                                  | 24/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▏                                 | 25/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▌                                 | 26/128 [00:05<00:23,  4.37it/s]\u001b[A\n",
      " 21%|████████▊                                 | 27/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 22%|█████████▏                                | 28/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▌                                | 29/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 23%|█████████▊                                | 30/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 24%|██████████▏                               | 31/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 25%|██████████▌                               | 32/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 26%|██████████▊                               | 33/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▏                              | 34/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 27%|███████████▍                              | 35/128 [00:07<00:21,  4.37it/s]\u001b[A\n",
      " 28%|███████████▊                              | 36/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 29%|████████████▏                             | 37/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▍                             | 38/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▊                             | 39/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 31%|█████████████▏                            | 40/128 [00:08<00:20,  4.37it/s]\u001b[A\n",
      " 32%|█████████████▍                            | 41/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 42/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 34%|██████████████                            | 43/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 34%|██████████████▍                           | 44/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 45/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 36%|███████████████                           | 46/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 47/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 48/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|████████████████                          | 49/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 50/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 51/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████                         | 52/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 41%|█████████████████▍                        | 53/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 54/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 43%|██████████████████                        | 55/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 56/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 57/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|███████████████████                       | 58/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 59/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 60/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████                      | 61/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 62/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 63/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 64/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 65/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 66/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 52%|█████████████████████▉                    | 67/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 68/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 54%|██████████████████████▋                   | 69/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 55%|██████████████████████▉                   | 70/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 55%|███████████████████████▎                  | 71/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 56%|███████████████████████▋                  | 72/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 73/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 74/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 59%|████████████████████████▌                 | 75/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 59%|████████████████████████▉                 | 76/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 60%|█████████████████████████▎                | 77/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 78/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 62%|█████████████████████████▉                | 79/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 62%|██████████████████████████▎               | 80/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 81/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 82/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 65%|███████████████████████████▏              | 83/128 [00:18<00:10,  4.38it/s]\u001b[A\n",
      " 66%|███████████████████████████▌              | 84/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 66%|███████████████████████████▉              | 85/128 [00:19<00:09,  4.37it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 86/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 68%|████████████████████████████▌             | 87/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 88/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 70%|█████████████████████████████▏            | 89/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 70%|█████████████████████████████▌            | 90/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 91/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 92/128 [00:20<00:08,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 93/128 [00:21<00:08,  4.37it/s]\u001b[A\n",
      " 73%|██████████████████████████████▊           | 94/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 74%|███████████████████████████████▏          | 95/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 96/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 76%|███████████████████████████████▊          | 97/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 77%|████████████████████████████████▏         | 98/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 99/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 100/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 101/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 102/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 103/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 104/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 105/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 106/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 107/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 108/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 109/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▏     | 110/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 111/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 112/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 113/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 114/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▊    | 115/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 116/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 117/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 118/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 119/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 120/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 121/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████  | 122/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 123/128 [00:27<00:01,  4.38it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▋ | 124/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████ | 125/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 126/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 127/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.117583990097046, 'eval_accuracy': 0.779296875, 'eval_runtime': 29.3392, 'eval_samples_per_second': 314.937, 'eval_steps_per_second': 4.397, 'epoch': 3.16}\n",
      " 79%|█████████████████████████████▉        | 9716/12316 [24:53<05:42,  7.60it/s]\n",
      "100%|█████████████████████████████████████████| 128/128 [00:29<00:00,  4.18it/s]\u001b[A\n",
      "{'loss': 2.018, 'grad_norm': 7.366450309753418, 'learning_rate': 4.02390438247012e-06, 'epoch': 3.25}\n",
      "{'loss': 2.0375, 'grad_norm': 8.617023468017578, 'learning_rate': 3.1578035683353546e-06, 'epoch': 3.41}\n",
      "{'loss': 2.0213, 'grad_norm': 9.556445121765137, 'learning_rate': 2.291702754200589e-06, 'epoch': 3.57}\n",
      " 90%|█████████████████████████████████▎   | 11104/12316 [27:57<02:39,  7.59it/s]\n",
      "  0%|                                                   | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▋                                          | 2/128 [00:00<00:14,  8.73it/s]\u001b[A\n",
      "  2%|█                                          | 3/128 [00:00<00:20,  6.18it/s]\u001b[A\n",
      "  3%|█▎                                         | 4/128 [00:00<00:23,  5.36it/s]\u001b[A\n",
      "  4%|█▋                                         | 5/128 [00:00<00:24,  4.98it/s]\u001b[A\n",
      "  5%|██                                         | 6/128 [00:01<00:25,  4.76it/s]\u001b[A\n",
      "  5%|██▎                                        | 7/128 [00:01<00:26,  4.63it/s]\u001b[A\n",
      "  6%|██▋                                        | 8/128 [00:01<00:26,  4.55it/s]\u001b[A\n",
      "  7%|███                                        | 9/128 [00:01<00:26,  4.50it/s]\u001b[A\n",
      "  8%|███▎                                      | 10/128 [00:02<00:26,  4.46it/s]\u001b[A\n",
      "  9%|███▌                                      | 11/128 [00:02<00:26,  4.43it/s]\u001b[A\n",
      "  9%|███▉                                      | 12/128 [00:02<00:26,  4.41it/s]\u001b[A\n",
      " 10%|████▎                                     | 13/128 [00:02<00:26,  4.40it/s]\u001b[A\n",
      " 11%|████▌                                     | 14/128 [00:02<00:25,  4.39it/s]\u001b[A\n",
      " 12%|████▉                                     | 15/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 12%|█████▎                                    | 16/128 [00:03<00:25,  4.39it/s]\u001b[A\n",
      " 13%|█████▌                                    | 17/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 14%|█████▉                                    | 18/128 [00:03<00:25,  4.38it/s]\u001b[A\n",
      " 15%|██████▏                                   | 19/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▌                                   | 20/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 16%|██████▉                                   | 21/128 [00:04<00:24,  4.37it/s]\u001b[A\n",
      " 17%|███████▏                                  | 22/128 [00:04<00:24,  4.38it/s]\u001b[A\n",
      " 18%|███████▌                                  | 23/128 [00:05<00:24,  4.37it/s]\u001b[A\n",
      " 19%|███████▉                                  | 24/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▏                                 | 25/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 20%|████████▌                                 | 26/128 [00:05<00:23,  4.38it/s]\u001b[A\n",
      " 21%|████████▊                                 | 27/128 [00:05<00:23,  4.37it/s]\u001b[A\n",
      " 22%|█████████▏                                | 28/128 [00:06<00:22,  4.37it/s]\u001b[A\n",
      " 23%|█████████▌                                | 29/128 [00:06<00:22,  4.37it/s]\u001b[A\n",
      " 23%|█████████▊                                | 30/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 24%|██████████▏                               | 31/128 [00:06<00:22,  4.38it/s]\u001b[A\n",
      " 25%|██████████▌                               | 32/128 [00:07<00:21,  4.37it/s]\u001b[A\n",
      " 26%|██████████▊                               | 33/128 [00:07<00:21,  4.37it/s]\u001b[A\n",
      " 27%|███████████▏                              | 34/128 [00:07<00:21,  4.37it/s]\u001b[A\n",
      " 27%|███████████▍                              | 35/128 [00:07<00:21,  4.37it/s]\u001b[A\n",
      " 28%|███████████▊                              | 36/128 [00:07<00:21,  4.38it/s]\u001b[A\n",
      " 29%|████████████▏                             | 37/128 [00:08<00:20,  4.37it/s]\u001b[A\n",
      " 30%|████████████▍                             | 38/128 [00:08<00:20,  4.38it/s]\u001b[A\n",
      " 30%|████████████▊                             | 39/128 [00:08<00:20,  4.37it/s]\u001b[A\n",
      " 31%|█████████████▏                            | 40/128 [00:08<00:20,  4.37it/s]\u001b[A\n",
      " 32%|█████████████▍                            | 41/128 [00:09<00:19,  4.38it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 42/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 34%|██████████████                            | 43/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 34%|██████████████▍                           | 44/128 [00:09<00:19,  4.37it/s]\u001b[A\n",
      " 35%|██████████████▊                           | 45/128 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 36%|███████████████                           | 46/128 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 37%|███████████████▍                          | 47/128 [00:10<00:18,  4.38it/s]\u001b[A\n",
      " 38%|███████████████▊                          | 48/128 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 38%|████████████████                          | 49/128 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 50/128 [00:11<00:17,  4.37it/s]\u001b[A\n",
      " 40%|████████████████▋                         | 51/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████                         | 52/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 41%|█████████████████▍                        | 53/128 [00:11<00:17,  4.38it/s]\u001b[A\n",
      " 42%|█████████████████▋                        | 54/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 43%|██████████████████                        | 55/128 [00:12<00:16,  4.37it/s]\u001b[A\n",
      " 44%|██████████████████▍                       | 56/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|██████████████████▋                       | 57/128 [00:12<00:16,  4.38it/s]\u001b[A\n",
      " 45%|███████████████████                       | 58/128 [00:13<00:16,  4.37it/s]\u001b[A\n",
      " 46%|███████████████████▎                      | 59/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 60/128 [00:13<00:15,  4.37it/s]\u001b[A\n",
      " 48%|████████████████████                      | 61/128 [00:13<00:15,  4.37it/s]\u001b[A\n",
      " 48%|████████████████████▎                     | 62/128 [00:13<00:15,  4.38it/s]\u001b[A\n",
      " 49%|████████████████████▋                     | 63/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 50%|█████████████████████                     | 64/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 51%|█████████████████████▎                    | 65/128 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 52%|█████████████████████▋                    | 66/128 [00:14<00:14,  4.38it/s]\u001b[A\n",
      " 52%|█████████████████████▉                    | 67/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 68/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 54%|██████████████████████▋                   | 69/128 [00:15<00:13,  4.38it/s]\u001b[A\n",
      " 55%|██████████████████████▉                   | 70/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 55%|███████████████████████▎                  | 71/128 [00:15<00:13,  4.37it/s]\u001b[A\n",
      " 56%|███████████████████████▋                  | 72/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 73/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 58%|████████████████████████▎                 | 74/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 59%|████████████████████████▌                 | 75/128 [00:16<00:12,  4.37it/s]\u001b[A\n",
      " 59%|████████████████████████▉                 | 76/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 60%|█████████████████████████▎                | 77/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 78/128 [00:17<00:11,  4.38it/s]\u001b[A\n",
      " 62%|█████████████████████████▉                | 79/128 [00:17<00:11,  4.37it/s]\u001b[A\n",
      " 62%|██████████████████████████▎               | 80/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 63%|██████████████████████████▌               | 81/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 64%|██████████████████████████▉               | 82/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 65%|███████████████████████████▏              | 83/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 66%|███████████████████████████▌              | 84/128 [00:18<00:10,  4.37it/s]\u001b[A\n",
      " 66%|███████████████████████████▉              | 85/128 [00:19<00:09,  4.37it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 86/128 [00:19<00:09,  4.37it/s]\u001b[A\n",
      " 68%|████████████████████████████▌             | 87/128 [00:19<00:09,  4.37it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 88/128 [00:19<00:09,  4.38it/s]\u001b[A\n",
      " 70%|█████████████████████████████▏            | 89/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 70%|█████████████████████████████▌            | 90/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 91/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 72%|██████████████████████████████▏           | 92/128 [00:20<00:08,  4.37it/s]\u001b[A\n",
      " 73%|██████████████████████████████▌           | 93/128 [00:21<00:07,  4.38it/s]\u001b[A\n",
      " 73%|██████████████████████████████▊           | 94/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 74%|███████████████████████████████▏          | 95/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 96/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 76%|███████████████████████████████▊          | 97/128 [00:21<00:07,  4.37it/s]\u001b[A\n",
      " 77%|████████████████████████████████▏         | 98/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 77%|████████████████████████████████▍         | 99/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 78%|████████████████████████████████         | 100/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 101/128 [00:22<00:06,  4.37it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 102/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 103/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 81%|█████████████████████████████████▎       | 104/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▋       | 105/128 [00:23<00:05,  4.37it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 106/128 [00:24<00:05,  4.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 107/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 108/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 109/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 86%|███████████████████████████████████▏     | 110/128 [00:24<00:04,  4.37it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 111/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 112/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 88%|████████████████████████████████████▏    | 113/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 114/128 [00:25<00:03,  4.37it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▊    | 115/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 116/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 117/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 118/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 119/128 [00:26<00:02,  4.37it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▍  | 120/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 121/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████  | 122/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 123/128 [00:27<00:01,  4.37it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▋ | 124/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████ | 125/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 126/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████▋| 127/128 [00:28<00:00,  4.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.1195712089538574, 'eval_accuracy': 0.7843967013888888, 'eval_runtime': 29.3466, 'eval_samples_per_second': 314.858, 'eval_steps_per_second': 4.396, 'epoch': 3.61}\n",
      " 90%|█████████████████████████████████▎   | 11104/12316 [28:26<02:39,  7.59it/s]\n",
      "100%|█████████████████████████████████████████| 128/128 [00:29<00:00,  4.18it/s]\u001b[A\n",
      "{'loss': 2.004, 'grad_norm': 10.961739540100098, 'learning_rate': 1.4256019400658238e-06, 'epoch': 3.73}\n",
      "{'loss': 2.0217, 'grad_norm': 12.884828567504883, 'learning_rate': 5.612333275593279e-07, 'epoch': 3.9}\n",
      "{'train_runtime': 1867.6281, 'train_samples_per_second': 59.365, 'train_steps_per_second': 6.594, 'train_loss': 2.1416689806768128, 'epoch': 4.0}\n",
      "100%|█████████████████████████████████████| 12316/12316 [31:07<00:00,  6.59it/s]\n",
      "LM saved to prt_lm/arxiv_20232/FacebookAI/roberta-base-seed0.ckpt\n",
      "Finished running train at 05-01 20:23:35, running time = 31.16min.\n",
      "Start running eval_and_save at 05-01 20:23:35\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "100%|█████████████████████████████████████████| 642/642 [02:20<00:00,  4.57it/s]\n",
      "/media/test/noppanat/tmp/TAPE-CaS/core/LMs/lm_trainer.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"y_true\": torch.tensor(labels).view(-1, 1),\n",
      "[LM] TrainAcc: 0.8252, ValAcc: 0.7814, TestAcc: 0.7766\n",
      "\n",
      "Finished running eval_and_save at 05-01 20:25:56, running time = 2.35min.\n"
     ]
    }
   ],
   "source": [
    "!WANDB_DISABLED=True TOKENIZERS_PARALLELISM=False CUDA_VISIBLE_DEVICES=0 python -m core.trainLM dataset $dataset seed $seed lm.train.use_gpt True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/arxiv_2023/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 137384\n",
      "Start running train at 05-01 20:32:43\n",
      "Epoch: 0, Time: 0.3551, Loss: 3.8438, TrainAcc: 0.0227, ValAcc: 0.2221, ES: 00/50 | BestVal=0.2221@E0\n",
      "Epoch: 10, Time: 0.0116, Loss: 1.7353, TrainAcc: 0.6439, ValAcc: 0.6524, ES: 06/50 | BestVal=0.6529@E4\n",
      "Epoch: 20, Time: 0.0121, Loss: 1.3519, TrainAcc: 0.6909, ValAcc: 0.6881, ES: 00/50 | BestVal=0.6881@E20\n",
      "Epoch: 30, Time: 0.0120, Loss: 1.1709, TrainAcc: 0.7241, ValAcc: 0.7100, ES: 00/50 | BestVal=0.7100@E30\n",
      "Epoch: 40, Time: 0.0125, Loss: 1.0612, TrainAcc: 0.7529, ValAcc: 0.7397, ES: 00/50 | BestVal=0.7397@E40\n",
      "Epoch: 50, Time: 0.0127, Loss: 0.9818, TrainAcc: 0.7754, ValAcc: 0.7497, ES: 00/50 | BestVal=0.7497@E50\n",
      "Epoch: 60, Time: 0.0108, Loss: 0.9156, TrainAcc: 0.7901, ValAcc: 0.7542, ES: 01/50 | BestVal=0.7543@E59\n",
      "Epoch: 70, Time: 0.0107, Loss: 0.8768, TrainAcc: 0.7960, ValAcc: 0.7547, ES: 03/50 | BestVal=0.7556@E67\n",
      "Epoch: 80, Time: 0.0121, Loss: 0.8417, TrainAcc: 0.8010, ValAcc: 0.7578, ES: 00/50 | BestVal=0.7578@E80\n",
      "Epoch: 90, Time: 0.0125, Loss: 0.8133, TrainAcc: 0.8066, ValAcc: 0.7610, ES: 00/50 | BestVal=0.7610@E90\n",
      "Epoch: 100, Time: 0.0110, Loss: 0.7751, TrainAcc: 0.8120, ValAcc: 0.7629, ES: 01/50 | BestVal=0.7640@E99\n",
      "Epoch: 110, Time: 0.0107, Loss: 0.7502, TrainAcc: 0.8160, ValAcc: 0.7668, ES: 03/50 | BestVal=0.7670@E107\n",
      "Epoch: 120, Time: 0.0107, Loss: 0.7315, TrainAcc: 0.8191, ValAcc: 0.7679, ES: 03/50 | BestVal=0.7689@E117\n",
      "Epoch: 130, Time: 0.0108, Loss: 0.7059, TrainAcc: 0.8258, ValAcc: 0.7690, ES: 07/50 | BestVal=0.7693@E123\n",
      "Epoch: 140, Time: 0.0102, Loss: 0.6850, TrainAcc: 0.8281, ValAcc: 0.7698, ES: 09/50 | BestVal=0.7699@E131\n",
      "Epoch: 150, Time: 0.0126, Loss: 0.6636, TrainAcc: 0.8340, ValAcc: 0.7726, ES: 00/50 | BestVal=0.7726@E150\n",
      "Epoch: 160, Time: 0.0109, Loss: 0.6442, TrainAcc: 0.8405, ValAcc: 0.7713, ES: 02/50 | BestVal=0.7731@E158\n",
      "Epoch: 170, Time: 0.0102, Loss: 0.6224, TrainAcc: 0.8438, ValAcc: 0.7734, ES: 04/50 | BestVal=0.7747@E166\n",
      "Epoch: 180, Time: 0.0103, Loss: 0.6097, TrainAcc: 0.8471, ValAcc: 0.7712, ES: 14/50 | BestVal=0.7747@E166\n",
      "Epoch: 190, Time: 0.0109, Loss: 0.5952, TrainAcc: 0.8490, ValAcc: 0.7741, ES: 24/50 | BestVal=0.7747@E166\n",
      "Finished running train at 05-01 20:32:45, running time = 2.68s.\n",
      "[MLP + TA] ValAcc: 0.7747, TestAcc: 0.7736\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/arxiv_2023.csv\n",
      "\n",
      "Number of parameters: 126248\n",
      "Start running train at 05-01 20:32:47\n",
      "Epoch: 0, Time: 0.0146, Loss: 3.8845, TrainAcc: 0.0162, ValAcc: 0.0301, ES: 00/50 | BestVal=0.0301@E0\n",
      "Epoch: 10, Time: 0.0145, Loss: 1.9943, TrainAcc: 0.5804, ValAcc: 0.6084, ES: 00/50 | BestVal=0.6084@E10\n",
      "Epoch: 20, Time: 0.0143, Loss: 1.6527, TrainAcc: 0.6143, ValAcc: 0.6325, ES: 00/50 | BestVal=0.6325@E20\n",
      "Epoch: 30, Time: 0.0146, Loss: 1.5014, TrainAcc: 0.6355, ValAcc: 0.6576, ES: 00/50 | BestVal=0.6576@E30\n",
      "Epoch: 40, Time: 0.0141, Loss: 1.3962, TrainAcc: 0.6569, ValAcc: 0.6808, ES: 00/50 | BestVal=0.6808@E40\n",
      "Epoch: 50, Time: 0.0144, Loss: 1.3164, TrainAcc: 0.6738, ValAcc: 0.7144, ES: 00/50 | BestVal=0.7144@E50\n",
      "Epoch: 60, Time: 0.0124, Loss: 1.2543, TrainAcc: 0.6852, ValAcc: 0.7265, ES: 01/50 | BestVal=0.7266@E59\n",
      "Epoch: 70, Time: 0.0142, Loss: 1.1946, TrainAcc: 0.6982, ValAcc: 0.7352, ES: 00/50 | BestVal=0.7352@E70\n",
      "Epoch: 80, Time: 0.0121, Loss: 1.1492, TrainAcc: 0.7098, ValAcc: 0.7410, ES: 01/50 | BestVal=0.7415@E79\n",
      "Epoch: 90, Time: 0.0129, Loss: 1.1185, TrainAcc: 0.7144, ValAcc: 0.7422, ES: 03/50 | BestVal=0.7429@E87\n",
      "Epoch: 100, Time: 0.0145, Loss: 1.0840, TrainAcc: 0.7219, ValAcc: 0.7476, ES: 00/50 | BestVal=0.7476@E100\n",
      "Epoch: 110, Time: 0.0144, Loss: 1.0657, TrainAcc: 0.7241, ValAcc: 0.7487, ES: 00/50 | BestVal=0.7487@E110\n",
      "Epoch: 120, Time: 0.0120, Loss: 1.0465, TrainAcc: 0.7248, ValAcc: 0.7487, ES: 09/50 | BestVal=0.7488@E111\n",
      "Epoch: 130, Time: 0.0145, Loss: 1.0263, TrainAcc: 0.7278, ValAcc: 0.7506, ES: 00/50 | BestVal=0.7506@E130\n",
      "Epoch: 140, Time: 0.0123, Loss: 1.0120, TrainAcc: 0.7319, ValAcc: 0.7514, ES: 04/50 | BestVal=0.7518@E136\n",
      "Epoch: 150, Time: 0.0143, Loss: 1.0030, TrainAcc: 0.7323, ValAcc: 0.7541, ES: 00/50 | BestVal=0.7541@E150\n",
      "Epoch: 160, Time: 0.0121, Loss: 0.9910, TrainAcc: 0.7380, ValAcc: 0.7554, ES: 01/50 | BestVal=0.7560@E159\n",
      "Epoch: 170, Time: 0.0128, Loss: 0.9716, TrainAcc: 0.7373, ValAcc: 0.7563, ES: 05/50 | BestVal=0.7570@E165\n",
      "Epoch: 180, Time: 0.0124, Loss: 0.9660, TrainAcc: 0.7385, ValAcc: 0.7580, ES: 01/50 | BestVal=0.7584@E179\n",
      "Epoch: 190, Time: 0.0146, Loss: 0.9591, TrainAcc: 0.7411, ValAcc: 0.7591, ES: 00/50 | BestVal=0.7591@E190\n",
      "Finished running train at 05-01 20:32:50, running time = 2.72s.\n",
      "[MLP + P] ValAcc: 0.7595, TestAcc: 0.7498\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/arxiv_20232/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 137384\n",
      "Start running train at 05-01 20:32:51\n",
      "Epoch: 0, Time: 0.0122, Loss: 3.8744, TrainAcc: 0.0201, ValAcc: 0.3213, ES: 00/50 | BestVal=0.3213@E0\n",
      "Epoch: 10, Time: 0.0132, Loss: 1.7607, TrainAcc: 0.6483, ValAcc: 0.6736, ES: 00/50 | BestVal=0.6736@E10\n",
      "Epoch: 20, Time: 0.0126, Loss: 1.3973, TrainAcc: 0.6802, ValAcc: 0.6871, ES: 00/50 | BestVal=0.6871@E20\n",
      "Epoch: 30, Time: 0.0125, Loss: 1.2415, TrainAcc: 0.7078, ValAcc: 0.7189, ES: 00/50 | BestVal=0.7189@E30\n",
      "Epoch: 40, Time: 0.0120, Loss: 1.1482, TrainAcc: 0.7379, ValAcc: 0.7429, ES: 00/50 | BestVal=0.7429@E40\n",
      "Epoch: 50, Time: 0.0125, Loss: 1.0762, TrainAcc: 0.7593, ValAcc: 0.7564, ES: 00/50 | BestVal=0.7564@E50\n",
      "Epoch: 60, Time: 0.0102, Loss: 1.0283, TrainAcc: 0.7658, ValAcc: 0.7557, ES: 06/50 | BestVal=0.7577@E54\n",
      "Epoch: 70, Time: 0.0106, Loss: 0.9831, TrainAcc: 0.7717, ValAcc: 0.7557, ES: 16/50 | BestVal=0.7577@E54\n",
      "Epoch: 80, Time: 0.0107, Loss: 0.9549, TrainAcc: 0.7731, ValAcc: 0.7558, ES: 26/50 | BestVal=0.7577@E54\n",
      "Epoch: 90, Time: 0.0121, Loss: 0.9263, TrainAcc: 0.7781, ValAcc: 0.7592, ES: 00/50 | BestVal=0.7592@E90\n",
      "Epoch: 100, Time: 0.0121, Loss: 0.9041, TrainAcc: 0.7832, ValAcc: 0.7652, ES: 00/50 | BestVal=0.7652@E100\n",
      "Epoch: 110, Time: 0.0103, Loss: 0.8792, TrainAcc: 0.7856, ValAcc: 0.7676, ES: 02/50 | BestVal=0.7679@E108\n",
      "Epoch: 120, Time: 0.0124, Loss: 0.8605, TrainAcc: 0.7903, ValAcc: 0.7688, ES: 00/50 | BestVal=0.7688@E120\n",
      "Epoch: 130, Time: 0.0103, Loss: 0.8422, TrainAcc: 0.7929, ValAcc: 0.7694, ES: 01/50 | BestVal=0.7697@E129\n",
      "Epoch: 140, Time: 0.0104, Loss: 0.8298, TrainAcc: 0.7986, ValAcc: 0.7725, ES: 04/50 | BestVal=0.7726@E136\n",
      "Epoch: 150, Time: 0.0102, Loss: 0.8111, TrainAcc: 0.8001, ValAcc: 0.7742, ES: 05/50 | BestVal=0.7744@E145\n",
      "Epoch: 160, Time: 0.0107, Loss: 0.7985, TrainAcc: 0.8024, ValAcc: 0.7738, ES: 06/50 | BestVal=0.7748@E154\n",
      "Epoch: 170, Time: 0.0107, Loss: 0.7765, TrainAcc: 0.8071, ValAcc: 0.7746, ES: 16/50 | BestVal=0.7748@E154\n",
      "Epoch: 180, Time: 0.0107, Loss: 0.7696, TrainAcc: 0.8078, ValAcc: 0.7765, ES: 07/50 | BestVal=0.7771@E173\n",
      "Epoch: 190, Time: 0.0102, Loss: 0.7651, TrainAcc: 0.8105, ValAcc: 0.7794, ES: 02/50 | BestVal=0.7808@E188\n",
      "Finished running train at 05-01 20:32:53, running time = 2.28s.\n",
      "[MLP + E] ValAcc: 0.7808, TestAcc: 0.7784\n",
      "\n",
      "(TA_P_E) ValAcc: 0.7956, TestAcc: 0.7936\n",
      "\n",
      "Running time: 12.79s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name MLP gnn.train.lr 0.002 gnn.train.dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/arxiv_2023/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 137384\n",
      "Start running train at 05-01 20:32:57\n",
      "Epoch: 0, Time: 0.3724, Loss: 3.9942, TrainAcc: 0.0178, ValAcc: 0.3116, ES: 00/50 | BestVal=0.3116@E0\n",
      "Epoch: 10, Time: 0.0203, Loss: 0.9346, TrainAcc: 0.7852, ValAcc: 0.7264, ES: 00/50 | BestVal=0.7264@E10\n",
      "Epoch: 20, Time: 0.0184, Loss: 0.8066, TrainAcc: 0.8072, ValAcc: 0.7412, ES: 04/50 | BestVal=0.7416@E16\n",
      "Epoch: 30, Time: 0.0183, Loss: 0.7419, TrainAcc: 0.8143, ValAcc: 0.7440, ES: 01/50 | BestVal=0.7453@E29\n",
      "Epoch: 40, Time: 0.0200, Loss: 0.7024, TrainAcc: 0.8178, ValAcc: 0.7487, ES: 00/50 | BestVal=0.7487@E40\n",
      "Epoch: 50, Time: 0.0190, Loss: 0.6703, TrainAcc: 0.8230, ValAcc: 0.7516, ES: 01/50 | BestVal=0.7523@E49\n",
      "Epoch: 60, Time: 0.0182, Loss: 0.6406, TrainAcc: 0.8274, ValAcc: 0.7512, ES: 11/50 | BestVal=0.7523@E49\n",
      "Epoch: 70, Time: 0.0182, Loss: 0.6122, TrainAcc: 0.8315, ValAcc: 0.7524, ES: 06/50 | BestVal=0.7528@E64\n",
      "Epoch: 80, Time: 0.0183, Loss: 0.6181, TrainAcc: 0.8332, ValAcc: 0.7528, ES: 02/50 | BestVal=0.7536@E78\n",
      "Epoch: 90, Time: 0.0181, Loss: 0.5691, TrainAcc: 0.8396, ValAcc: 0.7518, ES: 07/50 | BestVal=0.7553@E83\n",
      "Epoch: 100, Time: 0.0182, Loss: 0.5381, TrainAcc: 0.8479, ValAcc: 0.7505, ES: 17/50 | BestVal=0.7553@E83\n",
      "Epoch: 110, Time: 0.0181, Loss: 0.5336, TrainAcc: 0.8488, ValAcc: 0.7434, ES: 27/50 | BestVal=0.7553@E83\n",
      "Epoch: 120, Time: 0.0187, Loss: 0.4882, TrainAcc: 0.8591, ValAcc: 0.7482, ES: 37/50 | BestVal=0.7553@E83\n",
      "Epoch: 130, Time: 0.0182, Loss: 0.4565, TrainAcc: 0.8677, ValAcc: 0.7491, ES: 47/50 | BestVal=0.7553@E83\n",
      "Early stopped, loading model from epoch-83\n",
      "Finished running train at 05-01 20:33:00, running time = 2.92s.\n",
      "[GCN + TA] ValAcc: 0.7553, TestAcc: 0.7514\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/arxiv_2023.csv\n",
      "\n",
      "Number of parameters: 126248\n",
      "Start running train at 05-01 20:33:01\n",
      "Epoch: 0, Time: 0.0239, Loss: 4.0803, TrainAcc: 0.0222, ValAcc: 0.1255, ES: 00/50 | BestVal=0.1255@E0\n",
      "Epoch: 10, Time: 0.0220, Loss: 1.2222, TrainAcc: 0.7087, ValAcc: 0.6930, ES: 00/50 | BestVal=0.6930@E10\n",
      "Epoch: 20, Time: 0.0225, Loss: 1.0325, TrainAcc: 0.7332, ValAcc: 0.7202, ES: 00/50 | BestVal=0.7202@E20\n",
      "Epoch: 30, Time: 0.0219, Loss: 0.9218, TrainAcc: 0.7479, ValAcc: 0.7287, ES: 00/50 | BestVal=0.7287@E30\n",
      "Epoch: 40, Time: 0.0216, Loss: 0.8467, TrainAcc: 0.7617, ValAcc: 0.7305, ES: 00/50 | BestVal=0.7305@E40\n",
      "Epoch: 50, Time: 0.0198, Loss: 0.7797, TrainAcc: 0.7740, ValAcc: 0.7266, ES: 06/50 | BestVal=0.7311@E44\n",
      "Epoch: 60, Time: 0.0199, Loss: 0.7148, TrainAcc: 0.7867, ValAcc: 0.7253, ES: 16/50 | BestVal=0.7311@E44\n",
      "Epoch: 70, Time: 0.0208, Loss: 0.6715, TrainAcc: 0.7966, ValAcc: 0.7163, ES: 26/50 | BestVal=0.7311@E44\n",
      "Epoch: 80, Time: 0.0197, Loss: 0.6112, TrainAcc: 0.8106, ValAcc: 0.7203, ES: 36/50 | BestVal=0.7311@E44\n",
      "Epoch: 90, Time: 0.0201, Loss: 0.5418, TrainAcc: 0.8293, ValAcc: 0.7160, ES: 46/50 | BestVal=0.7311@E44\n",
      "Early stopped, loading model from epoch-44\n",
      "Finished running train at 05-01 20:33:03, running time = 1.99s.\n",
      "[GCN + P] ValAcc: 0.7311, TestAcc: 0.7295\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/arxiv_20232/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 137384\n",
      "Start running train at 05-01 20:33:04\n",
      "Epoch: 0, Time: 0.0214, Loss: 4.2061, TrainAcc: 0.0109, ValAcc: 0.2304, ES: 00/50 | BestVal=0.2304@E0\n",
      "Epoch: 10, Time: 0.0198, Loss: 1.0450, TrainAcc: 0.7644, ValAcc: 0.7027, ES: 00/50 | BestVal=0.7027@E10\n",
      "Epoch: 20, Time: 0.0203, Loss: 0.9087, TrainAcc: 0.7848, ValAcc: 0.7463, ES: 00/50 | BestVal=0.7463@E20\n",
      "Epoch: 30, Time: 0.0183, Loss: 0.8391, TrainAcc: 0.7912, ValAcc: 0.7512, ES: 05/50 | BestVal=0.7526@E25\n",
      "Epoch: 40, Time: 0.0203, Loss: 0.7988, TrainAcc: 0.7949, ValAcc: 0.7532, ES: 00/50 | BestVal=0.7532@E40\n",
      "Epoch: 50, Time: 0.0199, Loss: 0.7685, TrainAcc: 0.7979, ValAcc: 0.7541, ES: 00/50 | BestVal=0.7541@E50\n",
      "Epoch: 60, Time: 0.0183, Loss: 0.7430, TrainAcc: 0.8023, ValAcc: 0.7570, ES: 03/50 | BestVal=0.7574@E57\n",
      "Epoch: 70, Time: 0.0189, Loss: 0.7184, TrainAcc: 0.8072, ValAcc: 0.7561, ES: 07/50 | BestVal=0.7576@E63\n",
      "Epoch: 80, Time: 0.0184, Loss: 0.7291, TrainAcc: 0.8022, ValAcc: 0.7549, ES: 02/50 | BestVal=0.7589@E78\n",
      "Epoch: 90, Time: 0.0187, Loss: 0.6983, TrainAcc: 0.8085, ValAcc: 0.7537, ES: 12/50 | BestVal=0.7589@E78\n",
      "Epoch: 100, Time: 0.0182, Loss: 0.6708, TrainAcc: 0.8129, ValAcc: 0.7569, ES: 22/50 | BestVal=0.7589@E78\n",
      "Epoch: 110, Time: 0.0183, Loss: 0.6490, TrainAcc: 0.8161, ValAcc: 0.7544, ES: 04/50 | BestVal=0.7589@E106\n",
      "Epoch: 120, Time: 0.0182, Loss: 0.6478, TrainAcc: 0.8176, ValAcc: 0.7513, ES: 14/50 | BestVal=0.7589@E106\n",
      "Epoch: 130, Time: 0.0185, Loss: 0.6325, TrainAcc: 0.8212, ValAcc: 0.7556, ES: 24/50 | BestVal=0.7589@E106\n",
      "Epoch: 140, Time: 0.0183, Loss: 0.5888, TrainAcc: 0.8294, ValAcc: 0.7544, ES: 34/50 | BestVal=0.7589@E106\n",
      "Epoch: 150, Time: 0.0183, Loss: 0.5835, TrainAcc: 0.8315, ValAcc: 0.7484, ES: 44/50 | BestVal=0.7589@E106\n",
      "Early stopped, loading model from epoch-106\n",
      "Finished running train at 05-01 20:33:07, running time = 2.96s.\n",
      "[GCN + E] ValAcc: 0.7589, TestAcc: 0.7558\n",
      "\n",
      "(TA_P_E) ValAcc: 0.7755, TestAcc: 0.7741\n",
      "\n",
      "Running time: 13.14s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/arxiv_2023/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 273576\n",
      "Start running train at 05-01 20:33:12\n",
      "Epoch: 0, Time: 0.3857, Loss: 3.6831, TrainAcc: 0.0203, ValAcc: 0.5810, ES: 00/50 | BestVal=0.5810@E0\n",
      "Epoch: 10, Time: 0.0279, Loss: 1.0083, TrainAcc: 0.7606, ValAcc: 0.7043, ES: 00/50 | BestVal=0.7043@E10\n",
      "Epoch: 20, Time: 0.0280, Loss: 0.7276, TrainAcc: 0.8125, ValAcc: 0.7464, ES: 00/50 | BestVal=0.7464@E20\n",
      "Epoch: 30, Time: 0.0271, Loss: 0.6075, TrainAcc: 0.8378, ValAcc: 0.7598, ES: 00/50 | BestVal=0.7598@E30\n",
      "Epoch: 40, Time: 0.0271, Loss: 0.5350, TrainAcc: 0.8550, ValAcc: 0.7684, ES: 00/50 | BestVal=0.7684@E40\n",
      "Epoch: 50, Time: 0.0278, Loss: 0.4893, TrainAcc: 0.8643, ValAcc: 0.7739, ES: 00/50 | BestVal=0.7739@E50\n",
      "Epoch: 60, Time: 0.0284, Loss: 0.4558, TrainAcc: 0.8687, ValAcc: 0.7748, ES: 00/50 | BestVal=0.7748@E60\n",
      "Epoch: 70, Time: 0.0251, Loss: 0.4277, TrainAcc: 0.8748, ValAcc: 0.7758, ES: 01/50 | BestVal=0.7762@E69\n",
      "Epoch: 80, Time: 0.0255, Loss: 0.4043, TrainAcc: 0.8794, ValAcc: 0.7716, ES: 11/50 | BestVal=0.7762@E69\n",
      "Epoch: 90, Time: 0.0248, Loss: 0.3858, TrainAcc: 0.8838, ValAcc: 0.7719, ES: 21/50 | BestVal=0.7762@E69\n",
      "Epoch: 100, Time: 0.0246, Loss: 0.3562, TrainAcc: 0.8919, ValAcc: 0.7693, ES: 31/50 | BestVal=0.7762@E69\n",
      "Epoch: 110, Time: 0.0255, Loss: 0.3503, TrainAcc: 0.8920, ValAcc: 0.7689, ES: 41/50 | BestVal=0.7762@E69\n",
      "Early stopped, loading model from epoch-69\n",
      "Finished running train at 05-01 20:33:15, running time = 3.55s.\n",
      "[SAGE + TA] ValAcc: 0.7762, TestAcc: 0.7755\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/arxiv_2023.csv\n",
      "\n",
      "Number of parameters: 246056\n",
      "Start running train at 05-01 20:33:17\n",
      "Epoch: 0, Time: 0.0325, Loss: 3.7244, TrainAcc: 0.0276, ValAcc: 0.4328, ES: 00/50 | BestVal=0.4328@E0\n",
      "Epoch: 10, Time: 0.0313, Loss: 1.3092, TrainAcc: 0.6824, ValAcc: 0.6556, ES: 00/50 | BestVal=0.6556@E10\n",
      "Epoch: 20, Time: 0.0319, Loss: 1.0146, TrainAcc: 0.7358, ValAcc: 0.7242, ES: 00/50 | BestVal=0.7242@E20\n",
      "Epoch: 30, Time: 0.0322, Loss: 0.8594, TrainAcc: 0.7607, ValAcc: 0.7384, ES: 00/50 | BestVal=0.7384@E30\n",
      "Epoch: 40, Time: 0.0305, Loss: 0.7511, TrainAcc: 0.7816, ValAcc: 0.7491, ES: 01/50 | BestVal=0.7495@E39\n",
      "Epoch: 50, Time: 0.0296, Loss: 0.6687, TrainAcc: 0.8011, ValAcc: 0.7521, ES: 01/50 | BestVal=0.7531@E49\n",
      "Epoch: 60, Time: 0.0289, Loss: 0.5921, TrainAcc: 0.8228, ValAcc: 0.7488, ES: 08/50 | BestVal=0.7536@E52\n",
      "Epoch: 70, Time: 0.0295, Loss: 0.5607, TrainAcc: 0.8225, ValAcc: 0.7314, ES: 18/50 | BestVal=0.7536@E52\n",
      "Epoch: 80, Time: 0.0297, Loss: 0.4846, TrainAcc: 0.8489, ValAcc: 0.7306, ES: 28/50 | BestVal=0.7536@E52\n",
      "Epoch: 90, Time: 0.0296, Loss: 0.4119, TrainAcc: 0.8709, ValAcc: 0.7241, ES: 38/50 | BestVal=0.7536@E52\n",
      "Epoch: 100, Time: 0.0300, Loss: 0.3765, TrainAcc: 0.8767, ValAcc: 0.7264, ES: 48/50 | BestVal=0.7536@E52\n",
      "Early stopped, loading model from epoch-52\n",
      "Finished running train at 05-01 20:33:20, running time = 3.16s.\n",
      "[SAGE + P] ValAcc: 0.7536, TestAcc: 0.7427\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/arxiv_20232/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 273576\n",
      "Start running train at 05-01 20:33:21\n",
      "Epoch: 0, Time: 0.0366, Loss: 3.7440, TrainAcc: 0.0276, ValAcc: 0.3902, ES: 00/50 | BestVal=0.3902@E0\n",
      "Epoch: 10, Time: 0.0282, Loss: 1.0619, TrainAcc: 0.7557, ValAcc: 0.7202, ES: 00/50 | BestVal=0.7202@E10\n",
      "Epoch: 20, Time: 0.0249, Loss: 0.8245, TrainAcc: 0.7927, ValAcc: 0.7508, ES: 01/50 | BestVal=0.7512@E19\n",
      "Epoch: 30, Time: 0.0288, Loss: 0.7227, TrainAcc: 0.8093, ValAcc: 0.7570, ES: 00/50 | BestVal=0.7570@E30\n",
      "Epoch: 40, Time: 0.0280, Loss: 0.6556, TrainAcc: 0.8238, ValAcc: 0.7714, ES: 00/50 | BestVal=0.7714@E40\n",
      "Epoch: 50, Time: 0.0254, Loss: 0.6041, TrainAcc: 0.8338, ValAcc: 0.7778, ES: 02/50 | BestVal=0.7791@E48\n",
      "Epoch: 60, Time: 0.0255, Loss: 0.5692, TrainAcc: 0.8396, ValAcc: 0.7791, ES: 01/50 | BestVal=0.7793@E59\n",
      "Epoch: 70, Time: 0.0255, Loss: 0.5421, TrainAcc: 0.8444, ValAcc: 0.7776, ES: 11/50 | BestVal=0.7793@E59\n",
      "Epoch: 80, Time: 0.0255, Loss: 0.5194, TrainAcc: 0.8499, ValAcc: 0.7779, ES: 21/50 | BestVal=0.7793@E59\n",
      "Epoch: 90, Time: 0.0259, Loss: 0.4965, TrainAcc: 0.8539, ValAcc: 0.7690, ES: 31/50 | BestVal=0.7793@E59\n",
      "Epoch: 100, Time: 0.0248, Loss: 0.4844, TrainAcc: 0.8553, ValAcc: 0.7727, ES: 41/50 | BestVal=0.7793@E59\n",
      "Early stopped, loading model from epoch-59\n",
      "Finished running train at 05-01 20:33:24, running time = 2.90s.\n",
      "[SAGE + E] ValAcc: 0.7793, TestAcc: 0.7763\n",
      "\n",
      "(TA_P_E) ValAcc: 0.7985, TestAcc: 0.7978\n",
      "\n",
      "Running time: 14.6s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name SAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/arxiv_2023/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 920528\n",
      "Start running train at 05-01 20:33:28\n",
      "Epoch: 0, Time: 0.6961, Loss: 5.4349, TrainAcc: 0.0297, ValAcc: 0.0137, ES: 00/50 | BestVal=0.0137@E0\n",
      "Epoch: 10, Time: 0.1595, Loss: 1.3972, TrainAcc: 0.6988, ValAcc: 0.7518, ES: 00/50 | BestVal=0.7518@E10\n",
      "Epoch: 20, Time: 0.1553, Loss: 0.8963, TrainAcc: 0.7969, ValAcc: 0.7728, ES: 00/50 | BestVal=0.7728@E20\n",
      "Epoch: 30, Time: 0.1586, Loss: 0.7424, TrainAcc: 0.8210, ValAcc: 0.7753, ES: 00/50 | BestVal=0.7753@E30\n",
      "Epoch: 40, Time: 0.1499, Loss: 0.6670, TrainAcc: 0.8345, ValAcc: 0.7768, ES: 02/50 | BestVal=0.7773@E38\n",
      "Epoch: 50, Time: 0.1622, Loss: 0.6201, TrainAcc: 0.8396, ValAcc: 0.7764, ES: 01/50 | BestVal=0.7773@E49\n",
      "Epoch: 60, Time: 0.1072, Loss: 0.5893, TrainAcc: 0.8467, ValAcc: 0.7761, ES: 11/50 | BestVal=0.7773@E49\n",
      "Epoch: 70, Time: 0.1037, Loss: 0.5563, TrainAcc: 0.8483, ValAcc: 0.7759, ES: 21/50 | BestVal=0.7773@E49\n",
      "Epoch: 80, Time: 0.1318, Loss: 0.5349, TrainAcc: 0.8531, ValAcc: 0.7759, ES: 31/50 | BestVal=0.7773@E49\n",
      "Epoch: 90, Time: 0.1547, Loss: 0.5201, TrainAcc: 0.8540, ValAcc: 0.7766, ES: 41/50 | BestVal=0.7773@E49\n",
      "Epoch: 100, Time: 0.1022, Loss: 0.5071, TrainAcc: 0.8569, ValAcc: 0.7778, ES: 01/50 | BestVal=0.7784@E99\n",
      "Epoch: 110, Time: 0.1391, Loss: 0.4979, TrainAcc: 0.8588, ValAcc: 0.7776, ES: 08/50 | BestVal=0.7800@E102\n",
      "Epoch: 120, Time: 0.1033, Loss: 0.4844, TrainAcc: 0.8616, ValAcc: 0.7792, ES: 18/50 | BestVal=0.7800@E102\n",
      "Epoch: 130, Time: 0.1058, Loss: 0.4722, TrainAcc: 0.8622, ValAcc: 0.7782, ES: 28/50 | BestVal=0.7800@E102\n",
      "Epoch: 140, Time: 0.1048, Loss: 0.4603, TrainAcc: 0.8653, ValAcc: 0.7798, ES: 02/50 | BestVal=0.7805@E138\n",
      "Epoch: 150, Time: 0.1063, Loss: 0.4499, TrainAcc: 0.8675, ValAcc: 0.7790, ES: 12/50 | BestVal=0.7805@E138\n",
      "Epoch: 160, Time: 0.1068, Loss: 0.4545, TrainAcc: 0.8664, ValAcc: 0.7768, ES: 22/50 | BestVal=0.7805@E138\n",
      "Epoch: 170, Time: 0.1044, Loss: 0.4366, TrainAcc: 0.8698, ValAcc: 0.7784, ES: 32/50 | BestVal=0.7805@E138\n",
      "Epoch: 180, Time: 0.1035, Loss: 0.4363, TrainAcc: 0.8726, ValAcc: 0.7771, ES: 42/50 | BestVal=0.7805@E138\n",
      "Early stopped, loading model from epoch-138\n",
      "Finished running train at 05-01 20:33:52, running time = 24.47s.\n",
      "[RevGAT + TA] ValAcc: 0.7805, TestAcc: 0.7825\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/arxiv_2023.csv\n",
      "\n",
      "Number of parameters: 827216\n",
      "Start running train at 05-01 20:33:54\n",
      "Epoch: 0, Time: 0.1114, Loss: 5.3537, TrainAcc: 0.0222, ValAcc: 0.0106, ES: 00/50 | BestVal=0.0106@E0\n",
      "Epoch: 10, Time: 0.1142, Loss: 2.4404, TrainAcc: 0.4883, ValAcc: 0.6584, ES: 00/50 | BestVal=0.6584@E10\n",
      "Epoch: 20, Time: 0.1693, Loss: 1.5266, TrainAcc: 0.6590, ValAcc: 0.7380, ES: 00/50 | BestVal=0.7380@E20\n",
      "Epoch: 30, Time: 0.1814, Loss: 1.2402, TrainAcc: 0.7024, ValAcc: 0.7538, ES: 00/50 | BestVal=0.7538@E30\n",
      "Epoch: 40, Time: 0.1024, Loss: 1.0837, TrainAcc: 0.7257, ValAcc: 0.7581, ES: 01/50 | BestVal=0.7603@E39\n",
      "Epoch: 50, Time: 0.1024, Loss: 1.0245, TrainAcc: 0.7319, ValAcc: 0.7558, ES: 03/50 | BestVal=0.7611@E47\n",
      "Epoch: 60, Time: 0.1106, Loss: 0.9352, TrainAcc: 0.7416, ValAcc: 0.7600, ES: 07/50 | BestVal=0.7636@E53\n",
      "Epoch: 70, Time: 0.1069, Loss: 0.8967, TrainAcc: 0.7485, ValAcc: 0.7630, ES: 03/50 | BestVal=0.7636@E67\n",
      "Epoch: 80, Time: 0.1140, Loss: 0.8590, TrainAcc: 0.7569, ValAcc: 0.7648, ES: 00/50 | BestVal=0.7648@E80\n",
      "Epoch: 90, Time: 0.1588, Loss: 0.8356, TrainAcc: 0.7575, ValAcc: 0.7654, ES: 01/50 | BestVal=0.7656@E89\n",
      "Epoch: 100, Time: 0.1680, Loss: 0.8134, TrainAcc: 0.7629, ValAcc: 0.7663, ES: 07/50 | BestVal=0.7667@E93\n",
      "Epoch: 110, Time: 0.1426, Loss: 0.7986, TrainAcc: 0.7656, ValAcc: 0.7666, ES: 08/50 | BestVal=0.7672@E102\n",
      "Epoch: 120, Time: 0.1525, Loss: 0.7748, TrainAcc: 0.7677, ValAcc: 0.7668, ES: 06/50 | BestVal=0.7682@E114\n",
      "Epoch: 130, Time: 0.1606, Loss: 0.7621, TrainAcc: 0.7720, ValAcc: 0.7657, ES: 16/50 | BestVal=0.7682@E114\n",
      "Epoch: 140, Time: 0.1547, Loss: 0.7472, TrainAcc: 0.7755, ValAcc: 0.7679, ES: 02/50 | BestVal=0.7683@E138\n",
      "Epoch: 150, Time: 0.1057, Loss: 0.7334, TrainAcc: 0.7799, ValAcc: 0.7673, ES: 12/50 | BestVal=0.7683@E138\n",
      "Epoch: 160, Time: 0.1057, Loss: 0.7250, TrainAcc: 0.7802, ValAcc: 0.7675, ES: 04/50 | BestVal=0.7687@E156\n",
      "Epoch: 170, Time: 0.1365, Loss: 0.7157, TrainAcc: 0.7795, ValAcc: 0.7675, ES: 14/50 | BestVal=0.7687@E156\n",
      "Epoch: 180, Time: 0.1502, Loss: 0.7015, TrainAcc: 0.7838, ValAcc: 0.7667, ES: 24/50 | BestVal=0.7687@E156\n",
      "Epoch: 190, Time: 0.1323, Loss: 0.6963, TrainAcc: 0.7861, ValAcc: 0.7663, ES: 34/50 | BestVal=0.7687@E156\n",
      "Finished running train at 05-01 20:34:21, running time = 27.57s.\n",
      "[RevGAT + P] ValAcc: 0.7687, TestAcc: 0.7574\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/arxiv_20232/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 920528\n",
      "Start running train at 05-01 20:34:23\n",
      "Epoch: 0, Time: 0.1438, Loss: 5.4675, TrainAcc: 0.0202, ValAcc: 0.0429, ES: 00/50 | BestVal=0.0429@E0\n",
      "Epoch: 10, Time: 0.1737, Loss: 1.4629, TrainAcc: 0.6945, ValAcc: 0.7593, ES: 00/50 | BestVal=0.7593@E10\n",
      "Epoch: 20, Time: 0.1446, Loss: 0.9969, TrainAcc: 0.7850, ValAcc: 0.7801, ES: 01/50 | BestVal=0.7802@E19\n",
      "Epoch: 30, Time: 0.1464, Loss: 0.8629, TrainAcc: 0.8013, ValAcc: 0.7826, ES: 01/50 | BestVal=0.7834@E29\n",
      "Epoch: 40, Time: 0.1650, Loss: 0.7948, TrainAcc: 0.8091, ValAcc: 0.7821, ES: 07/50 | BestVal=0.7839@E33\n",
      "Epoch: 50, Time: 0.1592, Loss: 0.7678, TrainAcc: 0.8109, ValAcc: 0.7799, ES: 17/50 | BestVal=0.7839@E33\n",
      "Epoch: 60, Time: 0.1553, Loss: 0.7015, TrainAcc: 0.8192, ValAcc: 0.7818, ES: 27/50 | BestVal=0.7839@E33\n",
      "Epoch: 70, Time: 0.1717, Loss: 0.6815, TrainAcc: 0.8200, ValAcc: 0.7811, ES: 37/50 | BestVal=0.7839@E33\n",
      "Epoch: 80, Time: 0.1566, Loss: 0.6610, TrainAcc: 0.8223, ValAcc: 0.7817, ES: 47/50 | BestVal=0.7839@E33\n",
      "Early stopped, loading model from epoch-33\n",
      "Finished running train at 05-01 20:34:36, running time = 13.44s.\n",
      "[RevGAT + E] ValAcc: 0.7839, TestAcc: 0.7806\n",
      "\n",
      "(TA_P_E) ValAcc: 0.8026, TestAcc: 0.7975\n",
      "\n",
      "Running time: 71.01s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name RevGAT gnn.train.lr 0.002 gnn.train.dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1692.55it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 16878.49it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1823.91it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 8098.05it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 2052.87it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 15148.45it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1920.10it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 19353.56it/s]\n",
      "[FacebookAI/roberta-base+MLP+E] TrainACC: 0.8407 ± nan, ValACC: 0.7808 ± nan, TestACC: 0.7784 ± nan\n",
      "[FacebookAI/roberta-base+MLP+E+C&S] TrainACC: 0.9542 ± nan, ValACC: 0.7419 ± nan, TestACC: 0.7307 ± nan\n",
      "[FacebookAI/roberta-base+MLP+None] TrainACC: 0.8581 ± nan, ValACC: 0.7956 ± nan, TestACC: 0.7936 ± nan\n",
      "[FacebookAI/roberta-base+MLP+None+C&S] TrainACC: 0.9569 ± nan, ValACC: 0.7517 ± nan, TestACC: 0.7421 ± nan\n",
      "[FacebookAI/roberta-base+MLP+P] TrainACC: 0.7756 ± nan, ValACC: 0.7595 ± nan, TestACC: 0.7498 ± nan\n",
      "[FacebookAI/roberta-base+MLP+P+C&S] TrainACC: 0.9557 ± nan, ValACC: 0.7264 ± nan, TestACC: 0.7106 ± nan\n",
      "[FacebookAI/roberta-base+MLP+TA] TrainACC: 0.8769 ± nan, ValACC: 0.7747 ± nan, TestACC: 0.7736 ± nan\n",
      "[FacebookAI/roberta-base+MLP+TA+C&S] TrainACC: 0.9537 ± nan, ValACC: 0.7457 ± nan, TestACC: 0.7371 ± nan\n",
      "Running time: 5.11s\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1814.27it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 18925.66it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1413.34it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 7384.08it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1714.73it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 18958.16it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 2069.71it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 18509.73it/s]\n",
      "[FacebookAI/roberta-base+GCN+E] TrainACC: 0.8131 ± nan, ValACC: 0.7589 ± nan, TestACC: 0.7558 ± nan\n",
      "[FacebookAI/roberta-base+GCN+E+C&S] TrainACC: 0.9540 ± nan, ValACC: 0.7295 ± nan, TestACC: 0.7234 ± nan\n",
      "[FacebookAI/roberta-base+GCN+None] TrainACC: 0.8344 ± nan, ValACC: 0.7755 ± nan, TestACC: 0.7741 ± nan\n",
      "[FacebookAI/roberta-base+GCN+None+C&S] TrainACC: 0.9568 ± nan, ValACC: 0.7373 ± nan, TestACC: 0.7347 ± nan\n",
      "[FacebookAI/roberta-base+GCN+P] TrainACC: 0.7647 ± nan, ValACC: 0.7311 ± nan, TestACC: 0.7295 ± nan\n",
      "[FacebookAI/roberta-base+GCN+P+C&S] TrainACC: 0.9567 ± nan, ValACC: 0.7123 ± nan, TestACC: 0.7076 ± nan\n",
      "[FacebookAI/roberta-base+GCN+TA] TrainACC: 0.8378 ± nan, ValACC: 0.7553 ± nan, TestACC: 0.7514 ± nan\n",
      "[FacebookAI/roberta-base+GCN+TA+C&S] TrainACC: 0.9558 ± nan, ValACC: 0.7200 ± nan, TestACC: 0.7163 ± nan\n",
      "Running time: 5.45s\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1769.72it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 17939.71it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1494.46it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 3688.21it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1523.81it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 3581.08it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1518.85it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 17909.07it/s]\n",
      "[FacebookAI/roberta-base+SAGE+E] TrainACC: 0.8379 ± nan, ValACC: 0.7793 ± nan, TestACC: 0.7763 ± nan\n",
      "[FacebookAI/roberta-base+SAGE+E+C&S] TrainACC: 0.9529 ± nan, ValACC: 0.7420 ± nan, TestACC: 0.7314 ± nan\n",
      "[FacebookAI/roberta-base+SAGE+None] TrainACC: 0.8656 ± nan, ValACC: 0.7985 ± nan, TestACC: 0.7978 ± nan\n",
      "[FacebookAI/roberta-base+SAGE+None+C&S] TrainACC: 0.9531 ± nan, ValACC: 0.7527 ± nan, TestACC: 0.7475 ± nan\n",
      "[FacebookAI/roberta-base+SAGE+P] TrainACC: 0.8032 ± nan, ValACC: 0.7536 ± nan, TestACC: 0.7427 ± nan\n",
      "[FacebookAI/roberta-base+SAGE+P+C&S] TrainACC: 0.9502 ± nan, ValACC: 0.7245 ± nan, TestACC: 0.7090 ± nan\n",
      "[FacebookAI/roberta-base+SAGE+TA] TrainACC: 0.8739 ± nan, ValACC: 0.7762 ± nan, TestACC: 0.7755 ± nan\n",
      "[FacebookAI/roberta-base+SAGE+TA+C&S] TrainACC: 0.9517 ± nan, ValACC: 0.7446 ± nan, TestACC: 0.7380 ± nan\n",
      "Running time: 5.09s\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1835.90it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 17863.30it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1610.09it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 17990.49it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 2206.81it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 18404.14it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1915.42it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 18661.26it/s]\n",
      "[FacebookAI/roberta-base+RevGAT+E] TrainACC: 0.8302 ± nan, ValACC: 0.7839 ± nan, TestACC: 0.7806 ± nan\n",
      "[FacebookAI/roberta-base+RevGAT+E+C&S] TrainACC: 0.9547 ± nan, ValACC: 0.7356 ± nan, TestACC: 0.7227 ± nan\n",
      "[FacebookAI/roberta-base+RevGAT+None] TrainACC: 0.8664 ± nan, ValACC: 0.8026 ± nan, TestACC: 0.7975 ± nan\n",
      "[FacebookAI/roberta-base+RevGAT+None+C&S] TrainACC: 0.9563 ± nan, ValACC: 0.7506 ± nan, TestACC: 0.7420 ± nan\n",
      "[FacebookAI/roberta-base+RevGAT+P] TrainACC: 0.8125 ± nan, ValACC: 0.7687 ± nan, TestACC: 0.7574 ± nan\n",
      "[FacebookAI/roberta-base+RevGAT+P+C&S] TrainACC: 0.9556 ± nan, ValACC: 0.7303 ± nan, TestACC: 0.7199 ± nan\n",
      "[FacebookAI/roberta-base+RevGAT+TA] TrainACC: 0.8831 ± nan, ValACC: 0.7805 ± nan, TestACC: 0.7825 ± nan\n",
      "[FacebookAI/roberta-base+RevGAT+TA+C&S] TrainACC: 0.9510 ± nan, ValACC: 0.7479 ± nan, TestACC: 0.7419 ± nan\n",
      "Running time: 5.22s\n"
     ]
    }
   ],
   "source": [
    "for model_name in [\"MLP\", \"GCN\", \"SAGE\", \"RevGAT\"]:\n",
    "    !python -m core.runCaS dataset $dataset gnn.model.name $model_name seed $seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LM predictions (title and abstract) ...\n",
      "LM_pred_path: prt_lm/arxiv_2023/FacebookAI/roberta-base-seed0.pred\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1890.79it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 18608.27it/s]\n",
      "Loading top-k predictions ...\n",
      "Loading topk preds from gpt_preds/arxiv_2023.csv\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 2125.70it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 19660.19it/s]\n",
      "Loading LM predictions (explanations) ...\n",
      "LM_pred_path: prt_lm/arxiv_20232/FacebookAI/roberta-base-seed0.pred\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1710.12it/s]\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 6315.96it/s]\n",
      "Loading an ensemble of LM predictions ...\n",
      "Loading topk preds from gpt_preds/arxiv_2023.csv\n",
      "100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 1697.98it/s]\n",
      "100%|████████████████████████████████████████| 50/50 [00:00<00:00, 19546.57it/s]\n",
      "[FacebookAI/roberta-base+E] TrainACC: 0.8252 ± nan, ValACC: 0.7814 ± nan, TestACC: 0.7766 ± nan\n",
      "[FacebookAI/roberta-base+E+C&S] TrainACC: 0.9549 ± nan, ValACC: 0.7490 ± nan, TestACC: 0.7408 ± nan\n",
      "[FacebookAI/roberta-base+None] TrainACC: 0.8489 ± nan, ValACC: 0.7959 ± nan, TestACC: 0.7929 ± nan\n",
      "[FacebookAI/roberta-base+None+C&S] TrainACC: 0.9566 ± nan, ValACC: 0.7561 ± nan, TestACC: 0.7522 ± nan\n",
      "[FacebookAI/roberta-base+P] TrainACC: 0.7344 ± nan, ValACC: 0.7360 ± nan, TestACC: 0.7393 ± nan\n",
      "[FacebookAI/roberta-base+P+C&S] TrainACC: 0.9580 ± nan, ValACC: 0.7183 ± nan, TestACC: 0.7159 ± nan\n",
      "[FacebookAI/roberta-base+TA] TrainACC: 0.8440 ± nan, ValACC: 0.7682 ± nan, TestACC: 0.7680 ± nan\n",
      "[FacebookAI/roberta-base+TA+C&S] TrainACC: 0.9512 ± nan, ValACC: 0.7393 ± nan, TestACC: 0.7355 ± nan\n",
      "Running time: 5.83s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.runCaS dataset $dataset gnn.model.name RevGAT seed $seed cas.use_lm_pred True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: ogbn-products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ogbn-products\"\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Number of parameters: 124681775\n",
      "Start running train at 05-15 17:47:20\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "{'loss': 2.8625, 'grad_norm': 18.217697143554688, 'learning_rate': 1.1044444444444444e-05, 'epoch': 0.31}\n",
      "{'loss': 2.1532, 'grad_norm': 15.561079978942871, 'learning_rate': 1.965933286018453e-05, 'epoch': 0.61}\n",
      " 21%|████████▎                              | 1388/6536 [04:19<16:02,  5.35it/s]\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▏                                       | 2/21 [00:00<00:03,  5.53it/s]\u001b[A\n",
      " 14%|██████▎                                     | 3/21 [00:00<00:04,  3.90it/s]\u001b[A\n",
      " 19%|████████▍                                   | 4/21 [00:01<00:05,  3.37it/s]\u001b[A\n",
      " 24%|██████████▍                                 | 5/21 [00:01<00:05,  3.13it/s]\u001b[A\n",
      " 29%|████████████▌                               | 6/21 [00:01<00:05,  3.00it/s]\u001b[A\n",
      " 33%|██████████████▋                             | 7/21 [00:02<00:04,  2.91it/s]\u001b[A\n",
      " 38%|████████████████▊                           | 8/21 [00:02<00:04,  2.86it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 9/21 [00:02<00:04,  2.83it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 10/21 [00:03<00:03,  2.81it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 11/21 [00:03<00:03,  2.79it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 12/21 [00:03<00:03,  2.78it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 13/21 [00:04<00:02,  2.78it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 14/21 [00:04<00:02,  2.77it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 15/21 [00:05<00:02,  2.77it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 16/21 [00:05<00:01,  2.76it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 17/21 [00:05<00:01,  2.76it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 18/21 [00:06<00:01,  2.76it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 19/21 [00:06<00:00,  2.76it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 20/21 [00:06<00:00,  2.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.024127960205078, 'eval_accuracy': 0.8578042328042328, 'eval_runtime': 7.7484, 'eval_samples_per_second': 202.881, 'eval_steps_per_second': 2.839, 'epoch': 0.85}\n",
      " 21%|████████▎                              | 1388/6536 [04:26<16:02,  5.35it/s]\n",
      "100%|███████████████████████████████████████████| 21/21 [00:07<00:00,  2.63it/s]\u001b[A\n",
      "{'loss': 2.0921, 'grad_norm': 7.84464693069458, 'learning_rate': 1.788502484031228e-05, 'epoch': 0.92}\n",
      "{'loss': 2.0083, 'grad_norm': 14.094156265258789, 'learning_rate': 1.6110716820440028e-05, 'epoch': 1.22}\n",
      "{'loss': 1.9848, 'grad_norm': 1.1480146646499634, 'learning_rate': 1.433640880056778e-05, 'epoch': 1.53}\n",
      " 42%|████████████████▌                      | 2776/6536 [08:48<11:45,  5.33it/s]\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▏                                       | 2/21 [00:00<00:03,  5.51it/s]\u001b[A\n",
      " 14%|██████▎                                     | 3/21 [00:00<00:04,  3.89it/s]\u001b[A\n",
      " 19%|████████▍                                   | 4/21 [00:01<00:05,  3.37it/s]\u001b[A\n",
      " 24%|██████████▍                                 | 5/21 [00:01<00:05,  3.13it/s]\u001b[A\n",
      " 29%|████████████▌                               | 6/21 [00:01<00:05,  2.99it/s]\u001b[A\n",
      " 33%|██████████████▋                             | 7/21 [00:02<00:04,  2.91it/s]\u001b[A\n",
      " 38%|████████████████▊                           | 8/21 [00:02<00:04,  2.86it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 9/21 [00:02<00:04,  2.82it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 10/21 [00:03<00:03,  2.80it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 11/21 [00:03<00:03,  2.79it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 12/21 [00:03<00:03,  2.78it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 13/21 [00:04<00:02,  2.77it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 14/21 [00:04<00:02,  2.77it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 15/21 [00:05<00:02,  2.76it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 16/21 [00:05<00:01,  2.76it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 17/21 [00:05<00:01,  2.76it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 18/21 [00:06<00:01,  2.76it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 19/21 [00:06<00:00,  2.75it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 20/21 [00:06<00:00,  2.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.0242693424224854, 'eval_accuracy': 0.8736772486772487, 'eval_runtime': 7.7554, 'eval_samples_per_second': 202.699, 'eval_steps_per_second': 2.837, 'epoch': 1.7}\n",
      " 42%|████████████████▌                      | 2776/6536 [08:56<11:45,  5.33it/s]\n",
      "100%|███████████████████████████████████████████| 21/21 [00:07<00:00,  2.63it/s]\u001b[A\n",
      "{'loss': 1.9721, 'grad_norm': 11.50062084197998, 'learning_rate': 1.2565649396735273e-05, 'epoch': 1.84}\n",
      "{'loss': 1.935, 'grad_norm': 0.7839635610580444, 'learning_rate': 1.0791341376863025e-05, 'epoch': 2.14}\n",
      "{'loss': 1.9137, 'grad_norm': 31.352399826049805, 'learning_rate': 9.017033356990774e-06, 'epoch': 2.45}\n",
      " 64%|████████████████████████▊              | 4164/6536 [13:17<07:24,  5.33it/s]\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▏                                       | 2/21 [00:00<00:03,  5.51it/s]\u001b[A\n",
      " 14%|██████▎                                     | 3/21 [00:00<00:04,  3.89it/s]\u001b[A\n",
      " 19%|████████▍                                   | 4/21 [00:01<00:05,  3.37it/s]\u001b[A\n",
      " 24%|██████████▍                                 | 5/21 [00:01<00:05,  3.13it/s]\u001b[A\n",
      " 29%|████████████▌                               | 6/21 [00:01<00:05,  2.99it/s]\u001b[A\n",
      " 33%|██████████████▋                             | 7/21 [00:02<00:04,  2.91it/s]\u001b[A\n",
      " 38%|████████████████▊                           | 8/21 [00:02<00:04,  2.86it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 9/21 [00:02<00:04,  2.82it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 10/21 [00:03<00:03,  2.80it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 11/21 [00:03<00:03,  2.79it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 12/21 [00:03<00:03,  2.78it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 13/21 [00:04<00:02,  2.77it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 14/21 [00:04<00:02,  2.77it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 15/21 [00:05<00:02,  2.76it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 16/21 [00:05<00:01,  2.76it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 17/21 [00:05<00:01,  2.76it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 18/21 [00:06<00:01,  2.76it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 19/21 [00:06<00:00,  2.76it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 20/21 [00:06<00:00,  2.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.0207457542419434, 'eval_accuracy': 0.8862433862433863, 'eval_runtime': 7.7553, 'eval_samples_per_second': 202.701, 'eval_steps_per_second': 2.837, 'epoch': 2.55}\n",
      " 64%|████████████████████████▊              | 4164/6536 [13:25<07:24,  5.33it/s]\n",
      "100%|███████████████████████████████████████████| 21/21 [00:07<00:00,  2.62it/s]\u001b[A\n",
      "{'loss': 1.8998, 'grad_norm': 8.33268928527832, 'learning_rate': 7.2427253371185245e-06, 'epoch': 2.75}\n",
      "{'loss': 1.8862, 'grad_norm': 0.2097870111465454, 'learning_rate': 5.468417317246274e-06, 'epoch': 3.06}\n",
      "{'loss': 1.8477, 'grad_norm': 0.343911737203598, 'learning_rate': 3.697657913413769e-06, 'epoch': 3.37}\n",
      " 85%|█████████████████████████████████▏     | 5552/6536 [17:47<03:05,  5.32it/s]\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▏                                       | 2/21 [00:00<00:03,  5.52it/s]\u001b[A\n",
      " 14%|██████▎                                     | 3/21 [00:00<00:04,  3.90it/s]\u001b[A\n",
      " 19%|████████▍                                   | 4/21 [00:01<00:05,  3.37it/s]\u001b[A\n",
      " 24%|██████████▍                                 | 5/21 [00:01<00:05,  3.13it/s]\u001b[A\n",
      " 29%|████████████▌                               | 6/21 [00:01<00:05,  2.99it/s]\u001b[A\n",
      " 33%|██████████████▋                             | 7/21 [00:02<00:04,  2.91it/s]\u001b[A\n",
      " 38%|████████████████▊                           | 8/21 [00:02<00:04,  2.86it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 9/21 [00:02<00:04,  2.83it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 10/21 [00:03<00:03,  2.80it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 11/21 [00:03<00:03,  2.79it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 12/21 [00:03<00:03,  2.78it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 13/21 [00:04<00:02,  2.77it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 14/21 [00:04<00:02,  2.77it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 15/21 [00:05<00:02,  2.76it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 16/21 [00:05<00:01,  2.76it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 17/21 [00:05<00:01,  2.76it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 18/21 [00:06<00:01,  2.76it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 19/21 [00:06<00:00,  2.76it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 20/21 [00:06<00:00,  2.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.009751796722412, 'eval_accuracy': 0.8822751322751323, 'eval_runtime': 7.7552, 'eval_samples_per_second': 202.704, 'eval_steps_per_second': 2.837, 'epoch': 3.4}\n",
      " 85%|█████████████████████████████████▏     | 5552/6536 [17:55<03:05,  5.32it/s]\n",
      "100%|███████████████████████████████████████████| 21/21 [00:07<00:00,  2.62it/s]\u001b[A\n",
      "{'loss': 1.8394, 'grad_norm': 25.13414764404297, 'learning_rate': 1.923349893541519e-06, 'epoch': 3.67}\n",
      "{'loss': 1.8547, 'grad_norm': 2.973339080810547, 'learning_rate': 1.4904187366926898e-07, 'epoch': 3.98}\n",
      "{'train_runtime': 1261.4862, 'train_samples_per_second': 46.637, 'train_steps_per_second': 5.181, 'train_loss': 2.0185046598698255, 'epoch': 4.0}\n",
      "100%|███████████████████████████████████████| 6536/6536 [21:01<00:00,  5.18it/s]\n",
      "Created directory prt_lm/ogbn-products/FacebookAI/\n",
      "LM saved to prt_lm/ogbn-products/FacebookAI/roberta-base-seed0.ckpt\n",
      "Finished running train at 05-15 18:08:23, running time = 21.05min.\n",
      "Start running eval_and_save at 05-15 18:08:23\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "100%|█████████████████████████████████████████| 751/751 [04:19<00:00,  2.90it/s]\n",
      "/media/test/noppanat/tmp/TAPE-CaS/core/LMs/lm_trainer.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"y_true\": torch.tensor(labels).view(-1, 1),\n",
      "[LM] TrainAcc: 0.9574, ValAcc: 0.8849, TestAcc: 0.7434\n",
      "\n",
      "Finished running eval_and_save at 05-15 18:12:43, running time = 4.33min.\n"
     ]
    }
   ],
   "source": [
    "!WANDB_DISABLED=True TOKENIZERS_PARALLELISM=False CUDA_VISIBLE_DEVICES=0 python -m core.trainLM dataset $dataset seed $seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpt: gpt_responses/ogbn-products\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Number of parameters: 124681775\n",
      "Start running train at 05-15 18:19:41\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "{'loss': 2.7503, 'grad_norm': 10.44007682800293, 'learning_rate': 1.1044444444444444e-05, 'epoch': 0.31}\n",
      "{'loss': 2.0624, 'grad_norm': 2.371774435043335, 'learning_rate': 1.9655784244144784e-05, 'epoch': 0.61}\n",
      " 21%|████████▎                              | 1388/6536 [03:02<11:19,  7.57it/s]\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▏                                       | 2/21 [00:00<00:02,  8.77it/s]\u001b[A\n",
      " 14%|██████▎                                     | 3/21 [00:00<00:02,  6.18it/s]\u001b[A\n",
      " 19%|████████▍                                   | 4/21 [00:00<00:03,  5.36it/s]\u001b[A\n",
      " 24%|██████████▍                                 | 5/21 [00:00<00:03,  4.98it/s]\u001b[A\n",
      " 29%|████████████▌                               | 6/21 [00:01<00:03,  4.77it/s]\u001b[A\n",
      " 33%|██████████████▋                             | 7/21 [00:01<00:03,  4.64it/s]\u001b[A\n",
      " 38%|████████████████▊                           | 8/21 [00:01<00:02,  4.56it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 9/21 [00:01<00:02,  4.50it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 10/21 [00:02<00:02,  4.46it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 11/21 [00:02<00:02,  4.44it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 12/21 [00:02<00:02,  4.42it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 13/21 [00:02<00:01,  4.41it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 14/21 [00:02<00:01,  4.40it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 15/21 [00:03<00:01,  4.39it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 16/21 [00:03<00:01,  4.39it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 17/21 [00:03<00:00,  4.39it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 18/21 [00:03<00:00,  4.39it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 19/21 [00:04<00:00,  4.38it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 20/21 [00:04<00:00,  4.38it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.013207197189331, 'eval_accuracy': 0.876984126984127, 'eval_runtime': 4.8976, 'eval_samples_per_second': 320.976, 'eval_steps_per_second': 4.492, 'epoch': 0.85}\n",
      " 21%|████████▎                              | 1388/6536 [03:07<11:19,  7.57it/s]\n",
      "100%|███████████████████████████████████████████| 21/21 [00:04<00:00,  4.13it/s]\u001b[A\n",
      "{'loss': 2.0354, 'grad_norm': 11.057921409606934, 'learning_rate': 1.7881476224272535e-05, 'epoch': 0.92}\n",
      "{'loss': 1.9736, 'grad_norm': 8.01615047454834, 'learning_rate': 1.6107168204400285e-05, 'epoch': 1.22}\n",
      "{'loss': 1.9642, 'grad_norm': 19.48992347717285, 'learning_rate': 1.4332860184528035e-05, 'epoch': 1.53}\n",
      " 42%|████████████████▌                      | 2776/6536 [06:12<08:17,  7.55it/s]\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▏                                       | 2/21 [00:00<00:02,  8.76it/s]\u001b[A\n",
      " 14%|██████▎                                     | 3/21 [00:00<00:02,  6.17it/s]\u001b[A\n",
      " 19%|████████▍                                   | 4/21 [00:00<00:03,  5.34it/s]\u001b[A\n",
      " 24%|██████████▍                                 | 5/21 [00:00<00:03,  4.96it/s]\u001b[A\n",
      " 29%|████████████▌                               | 6/21 [00:01<00:03,  4.75it/s]\u001b[A\n",
      " 33%|██████████████▋                             | 7/21 [00:01<00:03,  4.61it/s]\u001b[A\n",
      " 38%|████████████████▊                           | 8/21 [00:01<00:02,  4.53it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 9/21 [00:01<00:02,  4.48it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 10/21 [00:02<00:02,  4.44it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 11/21 [00:02<00:02,  4.42it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 12/21 [00:02<00:02,  4.40it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 13/21 [00:02<00:01,  4.39it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 14/21 [00:02<00:01,  4.38it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 15/21 [00:03<00:01,  4.38it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 16/21 [00:03<00:01,  4.38it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 17/21 [00:03<00:00,  4.37it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 18/21 [00:03<00:00,  4.37it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 19/21 [00:04<00:00,  4.37it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 20/21 [00:04<00:00,  4.36it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.9981472492218018, 'eval_accuracy': 0.8842592592592593, 'eval_runtime': 4.9192, 'eval_samples_per_second': 319.565, 'eval_steps_per_second': 4.472, 'epoch': 1.7}\n",
      " 42%|████████████████▌                      | 2776/6536 [06:17<08:17,  7.55it/s]\n",
      "100%|███████████████████████████████████████████| 21/21 [00:04<00:00,  4.10it/s]\u001b[A\n",
      "{'loss': 1.9629, 'grad_norm': 4.639580249786377, 'learning_rate': 1.256210078069553e-05, 'epoch': 1.84}\n",
      "{'loss': 1.9248, 'grad_norm': 0.606977641582489, 'learning_rate': 1.0787792760823279e-05, 'epoch': 2.14}\n",
      "{'loss': 1.9183, 'grad_norm': 8.121331214904785, 'learning_rate': 9.01348474095103e-06, 'epoch': 2.45}\n",
      " 64%|████████████████████████▊              | 4164/6536 [09:23<05:14,  7.54it/s]\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▏                                       | 2/21 [00:00<00:02,  8.72it/s]\u001b[A\n",
      " 14%|██████▎                                     | 3/21 [00:00<00:02,  6.16it/s]\u001b[A\n",
      " 19%|████████▍                                   | 4/21 [00:00<00:03,  5.35it/s]\u001b[A\n",
      " 24%|██████████▍                                 | 5/21 [00:00<00:03,  4.96it/s]\u001b[A\n",
      " 29%|████████████▌                               | 6/21 [00:01<00:03,  4.74it/s]\u001b[A\n",
      " 33%|██████████████▋                             | 7/21 [00:01<00:03,  4.61it/s]\u001b[A\n",
      " 38%|████████████████▊                           | 8/21 [00:01<00:02,  4.53it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 9/21 [00:01<00:02,  4.48it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 10/21 [00:02<00:02,  4.44it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 11/21 [00:02<00:02,  4.42it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 12/21 [00:02<00:02,  4.40it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 13/21 [00:02<00:01,  4.39it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 14/21 [00:02<00:01,  4.38it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 15/21 [00:03<00:01,  4.38it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 16/21 [00:03<00:01,  4.37it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 17/21 [00:03<00:00,  4.37it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 18/21 [00:03<00:00,  4.37it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 19/21 [00:04<00:00,  4.37it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 20/21 [00:04<00:00,  4.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.019169330596924, 'eval_accuracy': 0.8855820105820106, 'eval_runtime': 4.919, 'eval_samples_per_second': 319.579, 'eval_steps_per_second': 4.472, 'epoch': 2.55}\n",
      " 64%|████████████████████████▊              | 4164/6536 [09:28<05:14,  7.54it/s]\n",
      "100%|███████████████████████████████████████████| 21/21 [00:04<00:00,  4.10it/s]\u001b[A\n",
      "{'loss': 1.9055, 'grad_norm': 3.220438241958618, 'learning_rate': 7.23917672107878e-06, 'epoch': 2.75}\n",
      "{'loss': 1.8906, 'grad_norm': 0.28036588430404663, 'learning_rate': 5.464868701206529e-06, 'epoch': 3.06}\n",
      "{'loss': 1.8625, 'grad_norm': 0.4087775945663452, 'learning_rate': 3.6941092973740244e-06, 'epoch': 3.37}\n",
      " 85%|█████████████████████████████████▏     | 5552/6536 [12:33<02:10,  7.54it/s]\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▏                                       | 2/21 [00:00<00:02,  8.75it/s]\u001b[A\n",
      " 14%|██████▎                                     | 3/21 [00:00<00:02,  6.17it/s]\u001b[A\n",
      " 19%|████████▍                                   | 4/21 [00:00<00:03,  5.34it/s]\u001b[A\n",
      " 24%|██████████▍                                 | 5/21 [00:00<00:03,  4.95it/s]\u001b[A\n",
      " 29%|████████████▌                               | 6/21 [00:01<00:03,  4.74it/s]\u001b[A\n",
      " 33%|██████████████▋                             | 7/21 [00:01<00:03,  4.62it/s]\u001b[A\n",
      " 38%|████████████████▊                           | 8/21 [00:01<00:02,  4.54it/s]\u001b[A\n",
      " 43%|██████████████████▊                         | 9/21 [00:01<00:02,  4.48it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 10/21 [00:02<00:02,  4.44it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 11/21 [00:02<00:02,  4.42it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 12/21 [00:02<00:02,  4.40it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 13/21 [00:02<00:01,  4.39it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 14/21 [00:02<00:01,  4.38it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 15/21 [00:03<00:01,  4.38it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 16/21 [00:03<00:01,  4.37it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 17/21 [00:03<00:00,  4.37it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 18/21 [00:03<00:00,  4.37it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 19/21 [00:04<00:00,  4.37it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 20/21 [00:04<00:00,  4.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.0074615478515625, 'eval_accuracy': 0.8921957671957672, 'eval_runtime': 4.917, 'eval_samples_per_second': 319.705, 'eval_steps_per_second': 4.474, 'epoch': 3.4}\n",
      " 85%|█████████████████████████████████▏     | 5552/6536 [12:38<02:10,  7.54it/s]\n",
      "100%|███████████████████████████████████████████| 21/21 [00:04<00:00,  4.10it/s]\u001b[A\n",
      "{'loss': 1.8588, 'grad_norm': 35.26054382324219, 'learning_rate': 1.9198012775017744e-06, 'epoch': 3.67}\n",
      "{'loss': 1.8698, 'grad_norm': 21.224706649780273, 'learning_rate': 1.454932576295245e-07, 'epoch': 3.98}\n",
      "{'train_runtime': 890.2637, 'train_samples_per_second': 66.084, 'train_steps_per_second': 7.342, 'train_loss': 1.9978755638094543, 'epoch': 4.0}\n",
      "100%|███████████████████████████████████████| 6536/6536 [14:50<00:00,  7.34it/s]\n",
      "Created directory prt_lm/ogbn-products2/FacebookAI/\n",
      "LM saved to prt_lm/ogbn-products2/FacebookAI/roberta-base-seed0.ckpt\n",
      "Finished running train at 05-15 18:34:33, running time = 14.87min.\n",
      "Start running eval_and_save at 05-15 18:34:33\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "100%|█████████████████████████████████████████| 751/751 [02:45<00:00,  4.55it/s]\n",
      "/media/test/noppanat/tmp/TAPE-CaS/core/LMs/lm_trainer.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"y_true\": torch.tensor(labels).view(-1, 1),\n",
      "[LM] TrainAcc: 0.9154, ValAcc: 0.8861, TestAcc: 0.7678\n",
      "\n",
      "Finished running eval_and_save at 05-15 18:37:18, running time = 2.76min.\n"
     ]
    }
   ],
   "source": [
    "!WANDB_DISABLED=True TOKENIZERS_PARALLELISM=False CUDA_VISIBLE_DEVICES=0 python -m core.trainLM dataset $dataset seed $seed lm.train.use_gpt True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/ogbn-products/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 138287\n",
      "Start running train at 05-15 18:37:32\n",
      "Epoch: 0, Time: 0.3595, Loss: 4.0364, TrainAcc: 0.0121, ValAcc: 0.3416, ES: 00/50 | BestVal=0.3416@E0\n",
      "Epoch: 10, Time: 0.0151, Loss: 1.3860, TrainAcc: 0.8221, ValAcc: 0.8492, ES: 00/50 | BestVal=0.8492@E10\n",
      "Epoch: 20, Time: 0.0131, Loss: 0.7997, TrainAcc: 0.8810, ValAcc: 0.8511, ES: 00/50 | BestVal=0.8511@E20\n",
      "Epoch: 30, Time: 0.0137, Loss: 0.6118, TrainAcc: 0.8954, ValAcc: 0.8588, ES: 00/50 | BestVal=0.8588@E30\n",
      "Epoch: 40, Time: 0.0137, Loss: 0.5086, TrainAcc: 0.9090, ValAcc: 0.8753, ES: 00/50 | BestVal=0.8753@E40\n",
      "Epoch: 50, Time: 0.0134, Loss: 0.4440, TrainAcc: 0.9246, ValAcc: 0.8772, ES: 00/50 | BestVal=0.8772@E50\n",
      "Epoch: 60, Time: 0.0112, Loss: 0.4017, TrainAcc: 0.9292, ValAcc: 0.8766, ES: 03/50 | BestVal=0.8785@E57\n",
      "Epoch: 70, Time: 0.0130, Loss: 0.3774, TrainAcc: 0.9321, ValAcc: 0.8798, ES: 00/50 | BestVal=0.8798@E70\n",
      "Epoch: 80, Time: 0.0120, Loss: 0.3571, TrainAcc: 0.9336, ValAcc: 0.8804, ES: 05/50 | BestVal=0.8810@E75\n",
      "Epoch: 90, Time: 0.0117, Loss: 0.3396, TrainAcc: 0.9367, ValAcc: 0.8791, ES: 15/50 | BestVal=0.8810@E75\n",
      "Epoch: 100, Time: 0.0114, Loss: 0.3202, TrainAcc: 0.9404, ValAcc: 0.8798, ES: 25/50 | BestVal=0.8810@E75\n",
      "Epoch: 110, Time: 0.0130, Loss: 0.3103, TrainAcc: 0.9411, ValAcc: 0.8810, ES: 00/50 | BestVal=0.8810@E110\n",
      "Epoch: 120, Time: 0.0119, Loss: 0.2951, TrainAcc: 0.9424, ValAcc: 0.8785, ES: 07/50 | BestVal=0.8810@E113\n",
      "Epoch: 130, Time: 0.0113, Loss: 0.2846, TrainAcc: 0.9436, ValAcc: 0.8798, ES: 17/50 | BestVal=0.8810@E113\n",
      "Epoch: 140, Time: 0.0137, Loss: 0.2744, TrainAcc: 0.9465, ValAcc: 0.8861, ES: 00/50 | BestVal=0.8861@E140\n",
      "Epoch: 150, Time: 0.0119, Loss: 0.2646, TrainAcc: 0.9483, ValAcc: 0.8842, ES: 07/50 | BestVal=0.8880@E143\n",
      "Epoch: 160, Time: 0.0117, Loss: 0.2525, TrainAcc: 0.9521, ValAcc: 0.8830, ES: 17/50 | BestVal=0.8880@E143\n",
      "Epoch: 170, Time: 0.0119, Loss: 0.2449, TrainAcc: 0.9542, ValAcc: 0.8842, ES: 27/50 | BestVal=0.8880@E143\n",
      "Epoch: 180, Time: 0.0117, Loss: 0.2370, TrainAcc: 0.9565, ValAcc: 0.8823, ES: 37/50 | BestVal=0.8880@E143\n",
      "Epoch: 190, Time: 0.0119, Loss: 0.2272, TrainAcc: 0.9582, ValAcc: 0.8817, ES: 47/50 | BestVal=0.8880@E143\n",
      "Early stopped, loading model from epoch-143\n",
      "Finished running train at 05-15 18:37:34, running time = 2.75s.\n",
      "[MLP + TA] ValAcc: 0.8880, TestAcc: 0.7390\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/ogbn-products.csv\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/trainEnsemble.py\", line 46, in <module>\n",
      "    run(cfg)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/trainEnsemble.py\", line 18, in run\n",
      "    pred, acc = ensembler.train()\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/GNNs/ensemble_trainer.py\", line 70, in train\n",
      "    trainer = self.TRAINER(self.cfg, feature_type)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/GNNs/gnn_trainer.py\", line 62, in __init__\n",
      "    features = load_gpt_preds(self.dataset_name, topk)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/data_utils/load.py\", line 12, in load_gpt_preds\n",
      "    with open(fn, 'r') as file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'gpt_preds/ogbn-products.csv'\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name MLP gnn.train.lr 0.002 gnn.train.dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/ogbn-products/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 138287\n",
      "Start running train at 05-15 18:37:38\n",
      "Epoch: 0, Time: 0.3730, Loss: 4.3634, TrainAcc: 0.0010, ValAcc: 0.7449, ES: 00/50 | BestVal=0.7449@E0\n",
      "Epoch: 10, Time: 0.0209, Loss: 0.6051, TrainAcc: 0.8643, ValAcc: 0.8798, ES: 00/50 | BestVal=0.8798@E10\n",
      "Epoch: 20, Time: 0.0161, Loss: 0.5361, TrainAcc: 0.8663, ValAcc: 0.8804, ES: 05/50 | BestVal=0.8830@E15\n",
      "Epoch: 30, Time: 0.0160, Loss: 0.4924, TrainAcc: 0.8720, ValAcc: 0.8810, ES: 05/50 | BestVal=0.8830@E25\n",
      "Epoch: 40, Time: 0.0155, Loss: 0.4565, TrainAcc: 0.8747, ValAcc: 0.8849, ES: 04/50 | BestVal=0.8861@E36\n",
      "Epoch: 50, Time: 0.0159, Loss: 0.4221, TrainAcc: 0.8799, ValAcc: 0.8817, ES: 14/50 | BestVal=0.8861@E36\n",
      "Epoch: 60, Time: 0.0161, Loss: 0.3870, TrainAcc: 0.8871, ValAcc: 0.8772, ES: 24/50 | BestVal=0.8861@E36\n",
      "Epoch: 70, Time: 0.0160, Loss: 0.3491, TrainAcc: 0.8979, ValAcc: 0.8760, ES: 34/50 | BestVal=0.8861@E36\n",
      "Epoch: 80, Time: 0.0161, Loss: 0.3165, TrainAcc: 0.9072, ValAcc: 0.8785, ES: 44/50 | BestVal=0.8861@E36\n",
      "Early stopped, loading model from epoch-36\n",
      "Finished running train at 05-15 18:37:39, running time = 1.83s.\n",
      "[GCN + TA] ValAcc: 0.8861, TestAcc: 0.7416\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/ogbn-products.csv\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/trainEnsemble.py\", line 46, in <module>\n",
      "    run(cfg)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/trainEnsemble.py\", line 18, in run\n",
      "    pred, acc = ensembler.train()\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/GNNs/ensemble_trainer.py\", line 70, in train\n",
      "    trainer = self.TRAINER(self.cfg, feature_type)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/GNNs/gnn_trainer.py\", line 62, in __init__\n",
      "    features = load_gpt_preds(self.dataset_name, topk)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/data_utils/load.py\", line 12, in load_gpt_preds\n",
      "    with open(fn, 'r') as file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'gpt_preds/ogbn-products.csv'\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/ogbn-products/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 275375\n",
      "Start running train at 05-15 18:37:43\n",
      "Epoch: 0, Time: 0.3797, Loss: 3.7371, TrainAcc: 0.0358, ValAcc: 0.7125, ES: 00/50 | BestVal=0.7125@E0\n",
      "Epoch: 10, Time: 0.0277, Loss: 0.6014, TrainAcc: 0.8580, ValAcc: 0.8690, ES: 00/50 | BestVal=0.8690@E10\n",
      "Epoch: 20, Time: 0.0274, Loss: 0.4358, TrainAcc: 0.8888, ValAcc: 0.8899, ES: 00/50 | BestVal=0.8899@E20\n",
      "Epoch: 30, Time: 0.0254, Loss: 0.3057, TrainAcc: 0.9296, ValAcc: 0.8925, ES: 06/50 | BestVal=0.8938@E24\n",
      "Epoch: 40, Time: 0.0257, Loss: 0.2478, TrainAcc: 0.9464, ValAcc: 0.8906, ES: 07/50 | BestVal=0.8944@E33\n",
      "Epoch: 50, Time: 0.0255, Loss: 0.2050, TrainAcc: 0.9543, ValAcc: 0.8849, ES: 17/50 | BestVal=0.8944@E33\n",
      "Epoch: 60, Time: 0.0257, Loss: 0.1744, TrainAcc: 0.9595, ValAcc: 0.8836, ES: 27/50 | BestVal=0.8944@E33\n",
      "Epoch: 70, Time: 0.0256, Loss: 0.1517, TrainAcc: 0.9642, ValAcc: 0.8861, ES: 37/50 | BestVal=0.8944@E33\n",
      "Epoch: 80, Time: 0.0256, Loss: 0.1320, TrainAcc: 0.9683, ValAcc: 0.8842, ES: 47/50 | BestVal=0.8944@E33\n",
      "Early stopped, loading model from epoch-33\n",
      "Finished running train at 05-15 18:37:45, running time = 2.56s.\n",
      "[SAGE + TA] ValAcc: 0.8944, TestAcc: 0.7363\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/ogbn-products.csv\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/trainEnsemble.py\", line 46, in <module>\n",
      "    run(cfg)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/trainEnsemble.py\", line 18, in run\n",
      "    pred, acc = ensembler.train()\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/GNNs/ensemble_trainer.py\", line 70, in train\n",
      "    trainer = self.TRAINER(self.cfg, feature_type)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/GNNs/gnn_trainer.py\", line 62, in __init__\n",
      "    features = load_gpt_preds(self.dataset_name, topk)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/data_utils/load.py\", line 12, in load_gpt_preds\n",
      "    with open(fn, 'r') as file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'gpt_preds/ogbn-products.csv'\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name SAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n",
      "Using GAT based methods,total edges before adding self-loop 144638\n",
      "Total edges after adding self-loop 198663\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/ogbn-products/FacebookAI/roberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 925918\n",
      "Start running train at 05-15 18:37:49\n",
      "Epoch: 0, Time: 0.7472, Loss: 5.8035, TrainAcc: 0.0473, ValAcc: 0.0833, ES: 00/50 | BestVal=0.0833@E0\n",
      "Epoch: 10, Time: 0.1249, Loss: 0.6297, TrainAcc: 0.8769, ValAcc: 0.9008, ES: 00/50 | BestVal=0.9008@E10\n",
      "Epoch: 20, Time: 0.1180, Loss: 0.3613, TrainAcc: 0.9299, ValAcc: 0.8989, ES: 07/50 | BestVal=0.9046@E13\n",
      "Epoch: 30, Time: 0.1177, Loss: 0.2779, TrainAcc: 0.9474, ValAcc: 0.8957, ES: 17/50 | BestVal=0.9046@E13\n",
      "Epoch: 40, Time: 0.1188, Loss: 0.2355, TrainAcc: 0.9537, ValAcc: 0.8931, ES: 27/50 | BestVal=0.9046@E13\n",
      "Epoch: 50, Time: 0.1176, Loss: 0.2166, TrainAcc: 0.9582, ValAcc: 0.8912, ES: 37/50 | BestVal=0.9046@E13\n",
      "Epoch: 60, Time: 0.1164, Loss: 0.2020, TrainAcc: 0.9595, ValAcc: 0.8938, ES: 47/50 | BestVal=0.9046@E13\n",
      "Early stopped, loading model from epoch-13\n",
      "Finished running train at 05-15 18:37:57, running time = 8.58s.\n",
      "[RevGAT + TA] ValAcc: 0.9046, TestAcc: 0.7489\n",
      "\n",
      "/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n",
      "Using GAT based methods,total edges before adding self-loop 144638\n",
      "Total edges after adding self-loop 198663\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/ogbn-products.csv\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/trainEnsemble.py\", line 46, in <module>\n",
      "    run(cfg)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/trainEnsemble.py\", line 18, in run\n",
      "    pred, acc = ensembler.train()\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/GNNs/ensemble_trainer.py\", line 70, in train\n",
      "    trainer = self.TRAINER(self.cfg, feature_type)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/GNNs/dgl_gnn_trainer.py\", line 78, in __init__\n",
      "    features = load_gpt_preds(self.dataset_name, topk)\n",
      "  File \"/media/test/noppanat/tmp/TAPE-CaS/core/data_utils/load.py\", line 12, in load_gpt_preds\n",
      "    with open(fn, 'r') as file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'gpt_preds/ogbn-products.csv'\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name RevGAT gnn.train.lr 0.002 gnn.train.dropout 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: ogbn-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ogbn-arxiv\"\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/ogbn-arxiv/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 137384\n",
      "Start running train at 05-15 19:06:49\n",
      "Epoch: 0, Time: 0.3602, Loss: 3.8541, TrainAcc: 0.0249, ValAcc: 0.3612, ES: 00/50 | BestVal=0.3612@E0\n",
      "Epoch: 10, Time: 0.0362, Loss: 1.9142, TrainAcc: 0.5993, ValAcc: 0.6411, ES: 00/50 | BestVal=0.6411@E10\n",
      "Epoch: 20, Time: 0.0362, Loss: 1.4643, TrainAcc: 0.6673, ValAcc: 0.6886, ES: 00/50 | BestVal=0.6886@E20\n",
      "Epoch: 30, Time: 0.0361, Loss: 1.2003, TrainAcc: 0.7327, ValAcc: 0.7034, ES: 00/50 | BestVal=0.7034@E30\n",
      "Epoch: 40, Time: 0.0359, Loss: 1.0415, TrainAcc: 0.7710, ValAcc: 0.7195, ES: 00/50 | BestVal=0.7195@E40\n",
      "Epoch: 50, Time: 0.0345, Loss: 0.9340, TrainAcc: 0.7967, ValAcc: 0.7241, ES: 02/50 | BestVal=0.7246@E48\n",
      "Epoch: 60, Time: 0.0357, Loss: 0.8578, TrainAcc: 0.8114, ValAcc: 0.7252, ES: 00/50 | BestVal=0.7252@E60\n",
      "Epoch: 70, Time: 0.0359, Loss: 0.8086, TrainAcc: 0.8180, ValAcc: 0.7260, ES: 00/50 | BestVal=0.7260@E70\n",
      "Epoch: 80, Time: 0.0360, Loss: 0.7742, TrainAcc: 0.8239, ValAcc: 0.7280, ES: 00/50 | BestVal=0.7280@E80\n",
      "Epoch: 90, Time: 0.0359, Loss: 0.7488, TrainAcc: 0.8289, ValAcc: 0.7331, ES: 00/50 | BestVal=0.7331@E90\n",
      "Epoch: 100, Time: 0.0362, Loss: 0.7215, TrainAcc: 0.8357, ValAcc: 0.7386, ES: 00/50 | BestVal=0.7386@E100\n",
      "Epoch: 110, Time: 0.0360, Loss: 0.7007, TrainAcc: 0.8407, ValAcc: 0.7403, ES: 00/50 | BestVal=0.7403@E110\n",
      "Epoch: 120, Time: 0.0343, Loss: 0.6839, TrainAcc: 0.8445, ValAcc: 0.7410, ES: 02/50 | BestVal=0.7412@E118\n",
      "Epoch: 130, Time: 0.0342, Loss: 0.6620, TrainAcc: 0.8481, ValAcc: 0.7417, ES: 01/50 | BestVal=0.7417@E129\n",
      "Epoch: 140, Time: 0.0361, Loss: 0.6476, TrainAcc: 0.8504, ValAcc: 0.7434, ES: 00/50 | BestVal=0.7434@E140\n",
      "Epoch: 150, Time: 0.0360, Loss: 0.6311, TrainAcc: 0.8547, ValAcc: 0.7444, ES: 00/50 | BestVal=0.7444@E150\n",
      "Epoch: 160, Time: 0.0341, Loss: 0.6188, TrainAcc: 0.8573, ValAcc: 0.7441, ES: 05/50 | BestVal=0.7448@E155\n",
      "Epoch: 170, Time: 0.0343, Loss: 0.6062, TrainAcc: 0.8587, ValAcc: 0.7433, ES: 15/50 | BestVal=0.7448@E155\n",
      "Epoch: 180, Time: 0.0342, Loss: 0.5958, TrainAcc: 0.8610, ValAcc: 0.7441, ES: 25/50 | BestVal=0.7448@E155\n",
      "Epoch: 190, Time: 0.0342, Loss: 0.5822, TrainAcc: 0.8647, ValAcc: 0.7440, ES: 35/50 | BestVal=0.7448@E155\n",
      "Finished running train at 05-15 19:06:56, running time = 7.40s.\n",
      "[MLP + TA] ValAcc: 0.7448, TestAcc: 0.7266\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/ogbn-arxiv.csv\n",
      "\n",
      "Number of parameters: 126248\n",
      "Start running train at 05-15 19:06:59\n",
      "Epoch: 0, Time: 0.0503, Loss: 3.8509, TrainAcc: 0.0251, ValAcc: 0.2825, ES: 00/50 | BestVal=0.2825@E0\n",
      "Epoch: 10, Time: 0.0416, Loss: 2.2287, TrainAcc: 0.5124, ValAcc: 0.6058, ES: 00/50 | BestVal=0.6058@E10\n",
      "Epoch: 20, Time: 0.0418, Loss: 1.8471, TrainAcc: 0.5758, ValAcc: 0.6461, ES: 00/50 | BestVal=0.6461@E20\n",
      "Epoch: 30, Time: 0.0421, Loss: 1.6576, TrainAcc: 0.6127, ValAcc: 0.6791, ES: 00/50 | BestVal=0.6791@E30\n",
      "Epoch: 40, Time: 0.0426, Loss: 1.5208, TrainAcc: 0.6445, ValAcc: 0.7097, ES: 00/50 | BestVal=0.7097@E40\n",
      "Epoch: 50, Time: 0.0419, Loss: 1.4239, TrainAcc: 0.6639, ValAcc: 0.7211, ES: 00/50 | BestVal=0.7211@E50\n",
      "Epoch: 60, Time: 0.0415, Loss: 1.3600, TrainAcc: 0.6727, ValAcc: 0.7253, ES: 00/50 | BestVal=0.7253@E60\n",
      "Epoch: 70, Time: 0.0427, Loss: 1.3160, TrainAcc: 0.6779, ValAcc: 0.7273, ES: 00/50 | BestVal=0.7273@E70\n",
      "Epoch: 80, Time: 0.0418, Loss: 1.2846, TrainAcc: 0.6830, ValAcc: 0.7281, ES: 00/50 | BestVal=0.7281@E80\n",
      "Epoch: 90, Time: 0.0428, Loss: 1.2583, TrainAcc: 0.6838, ValAcc: 0.7312, ES: 00/50 | BestVal=0.7312@E90\n",
      "Epoch: 100, Time: 0.0425, Loss: 1.2283, TrainAcc: 0.6910, ValAcc: 0.7361, ES: 00/50 | BestVal=0.7361@E100\n",
      "Epoch: 110, Time: 0.0421, Loss: 1.2127, TrainAcc: 0.6930, ValAcc: 0.7411, ES: 00/50 | BestVal=0.7411@E110\n",
      "Epoch: 120, Time: 0.0408, Loss: 1.1949, TrainAcc: 0.6971, ValAcc: 0.7425, ES: 02/50 | BestVal=0.7425@E118\n",
      "Epoch: 130, Time: 0.0425, Loss: 1.1814, TrainAcc: 0.6976, ValAcc: 0.7428, ES: 00/50 | BestVal=0.7428@E130\n",
      "Epoch: 140, Time: 0.0399, Loss: 1.1645, TrainAcc: 0.7013, ValAcc: 0.7429, ES: 07/50 | BestVal=0.7437@E133\n",
      "Epoch: 150, Time: 0.0402, Loss: 1.1506, TrainAcc: 0.7019, ValAcc: 0.7434, ES: 17/50 | BestVal=0.7437@E133\n",
      "Epoch: 160, Time: 0.0406, Loss: 1.1475, TrainAcc: 0.7028, ValAcc: 0.7433, ES: 09/50 | BestVal=0.7437@E151\n",
      "Epoch: 170, Time: 0.0425, Loss: 1.1391, TrainAcc: 0.7064, ValAcc: 0.7440, ES: 00/50 | BestVal=0.7440@E170\n",
      "Epoch: 180, Time: 0.0404, Loss: 1.1323, TrainAcc: 0.7039, ValAcc: 0.7435, ES: 10/50 | BestVal=0.7440@E170\n",
      "Epoch: 190, Time: 0.0403, Loss: 1.1229, TrainAcc: 0.7058, ValAcc: 0.7441, ES: 01/50 | BestVal=0.7443@E189\n",
      "Finished running train at 05-15 19:07:07, running time = 8.31s.\n",
      "[MLP + P] ValAcc: 0.7443, TestAcc: 0.7389\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/ogbn-arxiv2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 137384\n",
      "Start running train at 05-15 19:07:07\n",
      "Epoch: 0, Time: 0.0361, Loss: 3.8908, TrainAcc: 0.0193, ValAcc: 0.2946, ES: 00/50 | BestVal=0.2946@E0\n",
      "Epoch: 10, Time: 0.0360, Loss: 2.0135, TrainAcc: 0.5742, ValAcc: 0.6689, ES: 00/50 | BestVal=0.6689@E10\n",
      "Epoch: 20, Time: 0.0361, Loss: 1.6036, TrainAcc: 0.6355, ValAcc: 0.6928, ES: 00/50 | BestVal=0.6928@E20\n",
      "Epoch: 30, Time: 0.0360, Loss: 1.3924, TrainAcc: 0.6774, ValAcc: 0.7025, ES: 00/50 | BestVal=0.7025@E30\n",
      "Epoch: 40, Time: 0.0360, Loss: 1.2640, TrainAcc: 0.7007, ValAcc: 0.7086, ES: 00/50 | BestVal=0.7086@E40\n",
      "Epoch: 50, Time: 0.0362, Loss: 1.1799, TrainAcc: 0.7173, ValAcc: 0.7217, ES: 00/50 | BestVal=0.7217@E50\n",
      "Epoch: 60, Time: 0.0363, Loss: 1.1120, TrainAcc: 0.7331, ValAcc: 0.7275, ES: 00/50 | BestVal=0.7275@E60\n",
      "Epoch: 70, Time: 0.0361, Loss: 1.0625, TrainAcc: 0.7414, ValAcc: 0.7294, ES: 00/50 | BestVal=0.7294@E70\n",
      "Epoch: 80, Time: 0.0340, Loss: 1.0213, TrainAcc: 0.7509, ValAcc: 0.7339, ES: 02/50 | BestVal=0.7339@E78\n",
      "Epoch: 90, Time: 0.0363, Loss: 0.9970, TrainAcc: 0.7537, ValAcc: 0.7373, ES: 00/50 | BestVal=0.7373@E90\n",
      "Epoch: 100, Time: 0.0368, Loss: 0.9697, TrainAcc: 0.7593, ValAcc: 0.7380, ES: 00/50 | BestVal=0.7380@E100\n",
      "Epoch: 110, Time: 0.0345, Loss: 0.9441, TrainAcc: 0.7644, ValAcc: 0.7378, ES: 08/50 | BestVal=0.7381@E102\n",
      "Epoch: 120, Time: 0.0337, Loss: 0.9285, TrainAcc: 0.7683, ValAcc: 0.7386, ES: 04/50 | BestVal=0.7395@E116\n",
      "Epoch: 130, Time: 0.0343, Loss: 0.9122, TrainAcc: 0.7717, ValAcc: 0.7378, ES: 14/50 | BestVal=0.7395@E116\n",
      "Epoch: 140, Time: 0.0343, Loss: 0.8917, TrainAcc: 0.7743, ValAcc: 0.7394, ES: 01/50 | BestVal=0.7397@E139\n",
      "Epoch: 150, Time: 0.0361, Loss: 0.8776, TrainAcc: 0.7780, ValAcc: 0.7403, ES: 00/50 | BestVal=0.7403@E150\n",
      "Epoch: 160, Time: 0.0338, Loss: 0.8660, TrainAcc: 0.7796, ValAcc: 0.7399, ES: 09/50 | BestVal=0.7406@E151\n",
      "Epoch: 170, Time: 0.0343, Loss: 0.8528, TrainAcc: 0.7831, ValAcc: 0.7399, ES: 04/50 | BestVal=0.7411@E166\n",
      "Epoch: 180, Time: 0.0344, Loss: 0.8389, TrainAcc: 0.7840, ValAcc: 0.7405, ES: 14/50 | BestVal=0.7411@E166\n",
      "Epoch: 190, Time: 0.0344, Loss: 0.8278, TrainAcc: 0.7872, ValAcc: 0.7397, ES: 24/50 | BestVal=0.7411@E166\n",
      "Finished running train at 05-15 19:07:14, running time = 7.07s.\n",
      "[MLP + E] ValAcc: 0.7411, TestAcc: 0.7280\n",
      "\n",
      "(TA_P_E) ValAcc: 0.7658, TestAcc: 0.7552\n",
      "\n",
      "Running time: 27.54s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name MLP gnn.train.lr 0.002 gnn.train.dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/ogbn-arxiv/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 137384\n",
      "Start running train at 05-15 19:07:18\n",
      "Epoch: 0, Time: 0.4251, Loss: 3.9770, TrainAcc: 0.0262, ValAcc: 0.5018, ES: 00/50 | BestVal=0.5018@E0\n",
      "Epoch: 10, Time: 0.0742, Loss: 1.0687, TrainAcc: 0.7367, ValAcc: 0.6938, ES: 00/50 | BestVal=0.6938@E10\n",
      "Epoch: 20, Time: 0.0742, Loss: 0.9232, TrainAcc: 0.7516, ValAcc: 0.7156, ES: 00/50 | BestVal=0.7156@E20\n",
      "Epoch: 30, Time: 0.0746, Loss: 0.8467, TrainAcc: 0.7617, ValAcc: 0.7253, ES: 00/50 | BestVal=0.7253@E30\n",
      "Epoch: 40, Time: 0.0727, Loss: 0.7976, TrainAcc: 0.7685, ValAcc: 0.7299, ES: 04/50 | BestVal=0.7323@E36\n",
      "Epoch: 50, Time: 0.0720, Loss: 0.7614, TrainAcc: 0.7734, ValAcc: 0.7356, ES: 01/50 | BestVal=0.7362@E49\n",
      "Epoch: 60, Time: 0.0744, Loss: 0.7318, TrainAcc: 0.7782, ValAcc: 0.7425, ES: 00/50 | BestVal=0.7425@E60\n",
      "Epoch: 70, Time: 0.0743, Loss: 0.7280, TrainAcc: 0.7802, ValAcc: 0.7417, ES: 03/50 | BestVal=0.7449@E67\n",
      "Epoch: 80, Time: 0.0728, Loss: 0.6964, TrainAcc: 0.7839, ValAcc: 0.7440, ES: 02/50 | BestVal=0.7470@E78\n",
      "Epoch: 90, Time: 0.0727, Loss: 0.6775, TrainAcc: 0.7875, ValAcc: 0.7451, ES: 02/50 | BestVal=0.7480@E88\n",
      "Epoch: 100, Time: 0.0726, Loss: 0.6614, TrainAcc: 0.7907, ValAcc: 0.7457, ES: 12/50 | BestVal=0.7480@E88\n",
      "Epoch: 110, Time: 0.0728, Loss: 0.6459, TrainAcc: 0.7939, ValAcc: 0.7462, ES: 22/50 | BestVal=0.7480@E88\n",
      "Epoch: 120, Time: 0.0746, Loss: 0.6333, TrainAcc: 0.7965, ValAcc: 0.7503, ES: 00/50 | BestVal=0.7503@E120\n",
      "Epoch: 130, Time: 0.0731, Loss: 0.6339, TrainAcc: 0.7978, ValAcc: 0.7466, ES: 04/50 | BestVal=0.7514@E126\n",
      "Epoch: 140, Time: 0.0728, Loss: 0.6142, TrainAcc: 0.8017, ValAcc: 0.7478, ES: 06/50 | BestVal=0.7520@E134\n",
      "Epoch: 150, Time: 0.0722, Loss: 0.6031, TrainAcc: 0.8033, ValAcc: 0.7312, ES: 16/50 | BestVal=0.7520@E134\n",
      "Epoch: 160, Time: 0.0727, Loss: 0.6053, TrainAcc: 0.8018, ValAcc: 0.7500, ES: 26/50 | BestVal=0.7520@E134\n",
      "Epoch: 170, Time: 0.0725, Loss: 0.5820, TrainAcc: 0.8089, ValAcc: 0.7526, ES: 04/50 | BestVal=0.7537@E166\n",
      "Epoch: 180, Time: 0.0736, Loss: 0.5906, TrainAcc: 0.8056, ValAcc: 0.7422, ES: 14/50 | BestVal=0.7537@E166\n",
      "Epoch: 190, Time: 0.0728, Loss: 0.5925, TrainAcc: 0.8071, ValAcc: 0.7440, ES: 24/50 | BestVal=0.7537@E166\n",
      "Finished running train at 05-15 19:07:33, running time = 15.07s.\n",
      "[GCN + TA] ValAcc: 0.7537, TestAcc: 0.7394\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/ogbn-arxiv.csv\n",
      "\n",
      "Number of parameters: 126248\n",
      "Start running train at 05-15 19:07:35\n",
      "Epoch: 0, Time: 0.1151, Loss: 4.0299, TrainAcc: 0.0306, ValAcc: 0.4888, ES: 00/50 | BestVal=0.4888@E0\n",
      "Epoch: 10, Time: 0.0810, Loss: 1.2052, TrainAcc: 0.7041, ValAcc: 0.6881, ES: 00/50 | BestVal=0.6881@E10\n",
      "Epoch: 20, Time: 0.0806, Loss: 1.0445, TrainAcc: 0.7216, ValAcc: 0.7114, ES: 00/50 | BestVal=0.7114@E20\n",
      "Epoch: 30, Time: 0.0809, Loss: 0.9602, TrainAcc: 0.7303, ValAcc: 0.7188, ES: 00/50 | BestVal=0.7188@E30\n",
      "Epoch: 40, Time: 0.0806, Loss: 0.9063, TrainAcc: 0.7375, ValAcc: 0.7244, ES: 00/50 | BestVal=0.7244@E40\n",
      "Epoch: 50, Time: 0.0787, Loss: 0.8650, TrainAcc: 0.7441, ValAcc: 0.7286, ES: 01/50 | BestVal=0.7289@E49\n",
      "Epoch: 60, Time: 0.0788, Loss: 0.8310, TrainAcc: 0.7498, ValAcc: 0.7311, ES: 04/50 | BestVal=0.7312@E56\n",
      "Epoch: 70, Time: 0.0794, Loss: 0.8039, TrainAcc: 0.7549, ValAcc: 0.7334, ES: 01/50 | BestVal=0.7357@E69\n",
      "Epoch: 80, Time: 0.0793, Loss: 0.7790, TrainAcc: 0.7595, ValAcc: 0.7375, ES: 02/50 | BestVal=0.7381@E78\n",
      "Epoch: 90, Time: 0.0793, Loss: 0.7653, TrainAcc: 0.7612, ValAcc: 0.7353, ES: 12/50 | BestVal=0.7381@E78\n",
      "Epoch: 100, Time: 0.0786, Loss: 0.7377, TrainAcc: 0.7689, ValAcc: 0.7365, ES: 02/50 | BestVal=0.7386@E98\n",
      "Epoch: 110, Time: 0.0793, Loss: 0.7164, TrainAcc: 0.7728, ValAcc: 0.7257, ES: 01/50 | BestVal=0.7397@E109\n",
      "Epoch: 120, Time: 0.0786, Loss: 0.7102, TrainAcc: 0.7729, ValAcc: 0.7355, ES: 01/50 | BestVal=0.7406@E119\n",
      "Epoch: 130, Time: 0.0793, Loss: 0.6929, TrainAcc: 0.7777, ValAcc: 0.7317, ES: 11/50 | BestVal=0.7406@E119\n",
      "Epoch: 140, Time: 0.0787, Loss: 0.6678, TrainAcc: 0.7852, ValAcc: 0.7393, ES: 21/50 | BestVal=0.7406@E119\n",
      "Epoch: 150, Time: 0.0787, Loss: 0.6758, TrainAcc: 0.7816, ValAcc: 0.7355, ES: 09/50 | BestVal=0.7414@E141\n",
      "Epoch: 160, Time: 0.0791, Loss: 0.6622, TrainAcc: 0.7846, ValAcc: 0.7333, ES: 19/50 | BestVal=0.7414@E141\n",
      "Epoch: 170, Time: 0.0795, Loss: 0.6301, TrainAcc: 0.7934, ValAcc: 0.7377, ES: 29/50 | BestVal=0.7414@E141\n",
      "Epoch: 180, Time: 0.0792, Loss: 0.6174, TrainAcc: 0.7956, ValAcc: 0.7385, ES: 39/50 | BestVal=0.7414@E141\n",
      "Epoch: 190, Time: 0.0794, Loss: 0.6307, TrainAcc: 0.7916, ValAcc: 0.7281, ES: 49/50 | BestVal=0.7414@E141\n",
      "Early stopped, loading model from epoch-141\n",
      "Finished running train at 05-15 19:07:51, running time = 15.35s.\n",
      "[GCN + P] ValAcc: 0.7414, TestAcc: 0.7273\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/ogbn-arxiv2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 137384\n",
      "Start running train at 05-15 19:07:51\n",
      "Epoch: 0, Time: 0.0943, Loss: 4.0108, TrainAcc: 0.0262, ValAcc: 0.4874, ES: 00/50 | BestVal=0.4874@E0\n",
      "Epoch: 10, Time: 0.0748, Loss: 1.0994, TrainAcc: 0.7268, ValAcc: 0.6935, ES: 00/50 | BestVal=0.6935@E10\n",
      "Epoch: 20, Time: 0.0731, Loss: 0.9571, TrainAcc: 0.7397, ValAcc: 0.7113, ES: 01/50 | BestVal=0.7121@E19\n",
      "Epoch: 30, Time: 0.0747, Loss: 0.8892, TrainAcc: 0.7479, ValAcc: 0.7129, ES: 00/50 | BestVal=0.7129@E30\n",
      "Epoch: 40, Time: 0.0737, Loss: 0.8427, TrainAcc: 0.7531, ValAcc: 0.7270, ES: 02/50 | BestVal=0.7273@E38\n",
      "Epoch: 50, Time: 0.0738, Loss: 0.8077, TrainAcc: 0.7582, ValAcc: 0.7335, ES: 02/50 | BestVal=0.7337@E48\n",
      "Epoch: 60, Time: 0.0755, Loss: 0.7837, TrainAcc: 0.7620, ValAcc: 0.7397, ES: 00/50 | BestVal=0.7397@E60\n",
      "Epoch: 70, Time: 0.0727, Loss: 0.7573, TrainAcc: 0.7664, ValAcc: 0.7387, ES: 04/50 | BestVal=0.7408@E66\n",
      "Epoch: 80, Time: 0.0731, Loss: 0.7417, TrainAcc: 0.7690, ValAcc: 0.7402, ES: 14/50 | BestVal=0.7408@E66\n",
      "Epoch: 90, Time: 0.0729, Loss: 0.7315, TrainAcc: 0.7718, ValAcc: 0.7321, ES: 24/50 | BestVal=0.7408@E66\n",
      "Epoch: 100, Time: 0.0730, Loss: 0.7136, TrainAcc: 0.7746, ValAcc: 0.7386, ES: 01/50 | BestVal=0.7414@E99\n",
      "Epoch: 110, Time: 0.0729, Loss: 0.7101, TrainAcc: 0.7743, ValAcc: 0.7377, ES: 07/50 | BestVal=0.7441@E103\n",
      "Epoch: 120, Time: 0.0735, Loss: 0.7027, TrainAcc: 0.7777, ValAcc: 0.7394, ES: 17/50 | BestVal=0.7441@E103\n",
      "Epoch: 130, Time: 0.0762, Loss: 0.6800, TrainAcc: 0.7813, ValAcc: 0.7452, ES: 00/50 | BestVal=0.7452@E130\n",
      "Epoch: 140, Time: 0.0733, Loss: 0.6714, TrainAcc: 0.7826, ValAcc: 0.7438, ES: 04/50 | BestVal=0.7458@E136\n",
      "Epoch: 150, Time: 0.0732, Loss: 0.6601, TrainAcc: 0.7857, ValAcc: 0.7405, ES: 01/50 | BestVal=0.7482@E149\n",
      "Epoch: 160, Time: 0.0734, Loss: 0.6637, TrainAcc: 0.7832, ValAcc: 0.7270, ES: 11/50 | BestVal=0.7482@E149\n",
      "Epoch: 170, Time: 0.0739, Loss: 0.6439, TrainAcc: 0.7899, ValAcc: 0.7463, ES: 21/50 | BestVal=0.7482@E149\n",
      "Epoch: 180, Time: 0.0730, Loss: 0.6468, TrainAcc: 0.7884, ValAcc: 0.7369, ES: 31/50 | BestVal=0.7482@E149\n",
      "Epoch: 190, Time: 0.0736, Loss: 0.6293, TrainAcc: 0.7936, ValAcc: 0.7419, ES: 41/50 | BestVal=0.7482@E149\n",
      "Finished running train at 05-15 19:08:06, running time = 14.76s.\n",
      "[GCN + E] ValAcc: 0.7502, TestAcc: 0.7415\n",
      "\n",
      "(TA_P_E) ValAcc: 0.7574, TestAcc: 0.7447\n",
      "\n",
      "Running time: 49.98s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/ogbn-arxiv/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 273576\n",
      "Start running train at 05-15 19:08:09\n",
      "Epoch: 0, Time: 0.4401, Loss: 3.8438, TrainAcc: 0.0072, ValAcc: 0.4937, ES: 00/50 | BestVal=0.4937@E0\n",
      "Epoch: 10, Time: 0.1194, Loss: 1.0036, TrainAcc: 0.7461, ValAcc: 0.7013, ES: 00/50 | BestVal=0.7013@E10\n",
      "Epoch: 20, Time: 0.1194, Loss: 0.7896, TrainAcc: 0.7825, ValAcc: 0.7205, ES: 00/50 | BestVal=0.7205@E20\n",
      "Epoch: 30, Time: 0.1169, Loss: 0.6463, TrainAcc: 0.8190, ValAcc: 0.7422, ES: 02/50 | BestVal=0.7442@E28\n",
      "Epoch: 40, Time: 0.1172, Loss: 0.5449, TrainAcc: 0.8468, ValAcc: 0.7460, ES: 01/50 | BestVal=0.7461@E39\n",
      "Epoch: 50, Time: 0.1165, Loss: 0.4834, TrainAcc: 0.8632, ValAcc: 0.7495, ES: 01/50 | BestVal=0.7498@E49\n",
      "Epoch: 60, Time: 0.1162, Loss: 0.4468, TrainAcc: 0.8719, ValAcc: 0.7490, ES: 07/50 | BestVal=0.7499@E53\n",
      "Epoch: 70, Time: 0.1171, Loss: 0.4227, TrainAcc: 0.8768, ValAcc: 0.7465, ES: 17/50 | BestVal=0.7499@E53\n",
      "Epoch: 80, Time: 0.1162, Loss: 0.4040, TrainAcc: 0.8815, ValAcc: 0.7475, ES: 27/50 | BestVal=0.7499@E53\n",
      "Epoch: 90, Time: 0.1167, Loss: 0.3889, TrainAcc: 0.8848, ValAcc: 0.7474, ES: 37/50 | BestVal=0.7499@E53\n",
      "Epoch: 100, Time: 0.1171, Loss: 0.3759, TrainAcc: 0.8882, ValAcc: 0.7474, ES: 47/50 | BestVal=0.7499@E53\n",
      "Early stopped, loading model from epoch-53\n",
      "Finished running train at 05-15 19:08:22, running time = 12.61s.\n",
      "[SAGE + TA] ValAcc: 0.7499, TestAcc: 0.7342\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/ogbn-arxiv.csv\n",
      "\n",
      "Number of parameters: 246056\n",
      "Start running train at 05-15 19:08:24\n",
      "Epoch: 0, Time: 0.1588, Loss: 3.8929, TrainAcc: 0.0216, ValAcc: 0.4558, ES: 00/50 | BestVal=0.4558@E0\n",
      "Epoch: 10, Time: 0.1413, Loss: 1.1791, TrainAcc: 0.6983, ValAcc: 0.6787, ES: 00/50 | BestVal=0.6787@E10\n",
      "Epoch: 20, Time: 0.1412, Loss: 0.9963, TrainAcc: 0.7251, ValAcc: 0.7058, ES: 00/50 | BestVal=0.7058@E20\n",
      "Epoch: 30, Time: 0.1412, Loss: 0.9088, TrainAcc: 0.7352, ValAcc: 0.7269, ES: 00/50 | BestVal=0.7269@E30\n",
      "Epoch: 40, Time: 0.1408, Loss: 0.8532, TrainAcc: 0.7454, ValAcc: 0.7416, ES: 00/50 | BestVal=0.7416@E40\n",
      "Epoch: 50, Time: 0.1387, Loss: 0.8063, TrainAcc: 0.7536, ValAcc: 0.7473, ES: 02/50 | BestVal=0.7480@E48\n",
      "Epoch: 60, Time: 0.1397, Loss: 0.7660, TrainAcc: 0.7609, ValAcc: 0.7496, ES: 02/50 | BestVal=0.7500@E58\n",
      "Epoch: 70, Time: 0.1384, Loss: 0.7426, TrainAcc: 0.7636, ValAcc: 0.7510, ES: 05/50 | BestVal=0.7513@E65\n",
      "Epoch: 80, Time: 0.1401, Loss: 0.7072, TrainAcc: 0.7727, ValAcc: 0.7529, ES: 02/50 | BestVal=0.7557@E78\n",
      "Epoch: 90, Time: 0.1389, Loss: 0.6749, TrainAcc: 0.7818, ValAcc: 0.7474, ES: 12/50 | BestVal=0.7557@E78\n",
      "Epoch: 100, Time: 0.1389, Loss: 0.6499, TrainAcc: 0.7873, ValAcc: 0.7508, ES: 22/50 | BestVal=0.7557@E78\n",
      "Epoch: 110, Time: 0.1391, Loss: 0.6238, TrainAcc: 0.7944, ValAcc: 0.7412, ES: 32/50 | BestVal=0.7557@E78\n",
      "Epoch: 120, Time: 0.1385, Loss: 0.6015, TrainAcc: 0.8003, ValAcc: 0.7294, ES: 42/50 | BestVal=0.7557@E78\n",
      "Early stopped, loading model from epoch-78\n",
      "Finished running train at 05-15 19:08:42, running time = 18.11s.\n",
      "[SAGE + P] ValAcc: 0.7557, TestAcc: 0.7502\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/ogbn-arxiv2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 273576\n",
      "Start running train at 05-15 19:08:43\n",
      "Epoch: 0, Time: 0.1282, Loss: 3.9580, TrainAcc: 0.0225, ValAcc: 0.1500, ES: 00/50 | BestVal=0.1500@E0\n",
      "Epoch: 10, Time: 0.1198, Loss: 1.1133, TrainAcc: 0.7163, ValAcc: 0.6778, ES: 00/50 | BestVal=0.6778@E10\n",
      "Epoch: 20, Time: 0.1199, Loss: 0.9313, TrainAcc: 0.7430, ValAcc: 0.7132, ES: 00/50 | BestVal=0.7132@E20\n",
      "Epoch: 30, Time: 0.1200, Loss: 0.8239, TrainAcc: 0.7623, ValAcc: 0.7346, ES: 00/50 | BestVal=0.7346@E30\n",
      "Epoch: 40, Time: 0.1172, Loss: 0.7493, TrainAcc: 0.7772, ValAcc: 0.7469, ES: 03/50 | BestVal=0.7471@E37\n",
      "Epoch: 50, Time: 0.1175, Loss: 0.6890, TrainAcc: 0.7912, ValAcc: 0.7507, ES: 04/50 | BestVal=0.7511@E46\n",
      "Epoch: 60, Time: 0.1203, Loss: 0.6484, TrainAcc: 0.8016, ValAcc: 0.7545, ES: 00/50 | BestVal=0.7545@E60\n",
      "Epoch: 70, Time: 0.1172, Loss: 0.6151, TrainAcc: 0.8092, ValAcc: 0.7530, ES: 07/50 | BestVal=0.7545@E63\n",
      "Epoch: 80, Time: 0.1167, Loss: 0.5895, TrainAcc: 0.8141, ValAcc: 0.7531, ES: 17/50 | BestVal=0.7545@E63\n",
      "Epoch: 90, Time: 0.1179, Loss: 0.5706, TrainAcc: 0.8182, ValAcc: 0.7529, ES: 27/50 | BestVal=0.7545@E63\n",
      "Epoch: 100, Time: 0.1208, Loss: 0.5518, TrainAcc: 0.8234, ValAcc: 0.7553, ES: 00/50 | BestVal=0.7553@E100\n",
      "Epoch: 110, Time: 0.1180, Loss: 0.5462, TrainAcc: 0.8219, ValAcc: 0.7467, ES: 10/50 | BestVal=0.7553@E100\n",
      "Epoch: 120, Time: 0.1170, Loss: 0.5276, TrainAcc: 0.8287, ValAcc: 0.7511, ES: 20/50 | BestVal=0.7553@E100\n",
      "Epoch: 130, Time: 0.1167, Loss: 0.5151, TrainAcc: 0.8317, ValAcc: 0.7479, ES: 30/50 | BestVal=0.7553@E100\n",
      "Epoch: 140, Time: 0.1179, Loss: 0.5014, TrainAcc: 0.8354, ValAcc: 0.7437, ES: 40/50 | BestVal=0.7553@E100\n",
      "Early stopped, loading model from epoch-100\n",
      "Finished running train at 05-15 19:09:01, running time = 17.87s.\n",
      "[SAGE + E] ValAcc: 0.7553, TestAcc: 0.7469\n",
      "\n",
      "(TA_P_E) ValAcc: 0.7760, TestAcc: 0.7674\n",
      "\n",
      "Running time: 53.46s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name SAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n",
      "Using GAT based methods,total edges before adding self-loop 2315598\n",
      "Total edges after adding self-loop 2484941\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/ogbn-arxiv/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 920528\n",
      "Start running train at 05-15 19:09:05\n",
      "Epoch: 0, Time: 1.2282, Loss: 6.3740, TrainAcc: 0.0171, ValAcc: 0.0031, ES: 00/50 | BestVal=0.0031@E0\n",
      "Epoch: 10, Time: 0.7356, Loss: 1.2535, TrainAcc: 0.7073, ValAcc: 0.7512, ES: 00/50 | BestVal=0.7512@E10\n",
      "Epoch: 20, Time: 0.7235, Loss: 0.8311, TrainAcc: 0.7967, ValAcc: 0.7667, ES: 01/50 | BestVal=0.7672@E19\n",
      "Epoch: 30, Time: 0.7335, Loss: 0.6811, TrainAcc: 0.8260, ValAcc: 0.7627, ES: 11/50 | BestVal=0.7672@E19\n",
      "Epoch: 40, Time: 0.7341, Loss: 0.6249, TrainAcc: 0.8394, ValAcc: 0.7532, ES: 21/50 | BestVal=0.7672@E19\n",
      "Epoch: 50, Time: 0.7331, Loss: 0.5749, TrainAcc: 0.8476, ValAcc: 0.7562, ES: 31/50 | BestVal=0.7672@E19\n",
      "Epoch: 60, Time: 0.7352, Loss: 0.5393, TrainAcc: 0.8556, ValAcc: 0.7602, ES: 41/50 | BestVal=0.7672@E19\n",
      "Early stopped, loading model from epoch-19\n",
      "Finished running train at 05-15 19:09:57, running time = 52.14s.\n",
      "[RevGAT + TA] ValAcc: 0.7672, TestAcc: 0.7576\n",
      "\n",
      "/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n",
      "Using GAT based methods,total edges before adding self-loop 2315598\n",
      "Total edges after adding self-loop 2484941\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/ogbn-arxiv.csv\n",
      "\n",
      "Number of parameters: 827216\n",
      "Start running train at 05-15 19:10:00\n",
      "Epoch: 0, Time: 0.7626, Loss: 5.8188, TrainAcc: 0.0275, ValAcc: 0.0218, ES: 00/50 | BestVal=0.0218@E0\n",
      "Epoch: 10, Time: 0.7324, Loss: 2.0905, TrainAcc: 0.5409, ValAcc: 0.7002, ES: 00/50 | BestVal=0.7002@E10\n",
      "Epoch: 20, Time: 0.7373, Loss: 1.4111, TrainAcc: 0.6618, ValAcc: 0.7393, ES: 00/50 | BestVal=0.7393@E20\n",
      "Epoch: 30, Time: 0.7378, Loss: 1.2376, TrainAcc: 0.6931, ValAcc: 0.7479, ES: 00/50 | BestVal=0.7479@E30\n",
      "Epoch: 40, Time: 0.7381, Loss: 1.1409, TrainAcc: 0.7036, ValAcc: 0.7528, ES: 00/50 | BestVal=0.7528@E40\n",
      "Epoch: 50, Time: 0.7210, Loss: 1.0991, TrainAcc: 0.7030, ValAcc: 0.7128, ES: 10/50 | BestVal=0.7528@E40\n",
      "Epoch: 60, Time: 0.7326, Loss: 0.9880, TrainAcc: 0.7204, ValAcc: 0.7544, ES: 00/50 | BestVal=0.7544@E60\n",
      "Epoch: 70, Time: 0.7396, Loss: 0.9599, TrainAcc: 0.7221, ValAcc: 0.7525, ES: 08/50 | BestVal=0.7547@E62\n",
      "Epoch: 80, Time: 0.7456, Loss: 0.9289, TrainAcc: 0.7292, ValAcc: 0.7566, ES: 00/50 | BestVal=0.7566@E80\n",
      "Epoch: 90, Time: 0.7347, Loss: 0.9169, TrainAcc: 0.7309, ValAcc: 0.7564, ES: 06/50 | BestVal=0.7567@E84\n",
      "Epoch: 100, Time: 0.7322, Loss: 0.8920, TrainAcc: 0.7358, ValAcc: 0.7567, ES: 16/50 | BestVal=0.7567@E84\n",
      "Epoch: 110, Time: 0.7274, Loss: 0.8919, TrainAcc: 0.7356, ValAcc: 0.7562, ES: 07/50 | BestVal=0.7579@E103\n",
      "Epoch: 120, Time: 0.7459, Loss: 0.8709, TrainAcc: 0.7389, ValAcc: 0.7578, ES: 02/50 | BestVal=0.7579@E118\n",
      "Epoch: 130, Time: 0.7380, Loss: 0.8777, TrainAcc: 0.7355, ValAcc: 0.7577, ES: 02/50 | BestVal=0.7587@E128\n",
      "Epoch: 140, Time: 0.7510, Loss: 0.8565, TrainAcc: 0.7422, ValAcc: 0.7606, ES: 00/50 | BestVal=0.7606@E140\n",
      "Epoch: 150, Time: 0.7340, Loss: 0.8429, TrainAcc: 0.7435, ValAcc: 0.7606, ES: 02/50 | BestVal=0.7609@E148\n",
      "Epoch: 160, Time: 0.7315, Loss: 0.8514, TrainAcc: 0.7422, ValAcc: 0.7590, ES: 12/50 | BestVal=0.7609@E148\n",
      "Epoch: 170, Time: 0.7241, Loss: 0.8341, TrainAcc: 0.7460, ValAcc: 0.7550, ES: 22/50 | BestVal=0.7609@E148\n",
      "Epoch: 180, Time: 0.7325, Loss: 0.8290, TrainAcc: 0.7467, ValAcc: 0.7559, ES: 01/50 | BestVal=0.7613@E179\n",
      "Epoch: 190, Time: 0.7316, Loss: 0.8238, TrainAcc: 0.7463, ValAcc: 0.7579, ES: 07/50 | BestVal=0.7616@E183\n",
      "Finished running train at 05-15 19:12:27, running time = 2.45min.\n",
      "[RevGAT + P] ValAcc: 0.7616, TestAcc: 0.7611\n",
      "\n",
      "/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n",
      "Using GAT based methods,total edges before adding self-loop 2315598\n",
      "Total edges after adding self-loop 2484941\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/ogbn-arxiv2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 920528\n",
      "Start running train at 05-15 19:12:29\n",
      "Epoch: 0, Time: 0.7439, Loss: 6.0809, TrainAcc: 0.0214, ValAcc: 0.0982, ES: 00/50 | BestVal=0.0982@E0\n",
      "Epoch: 10, Time: 0.7400, Loss: 1.4199, TrainAcc: 0.6689, ValAcc: 0.7432, ES: 00/50 | BestVal=0.7432@E10\n",
      "Epoch: 20, Time: 0.7352, Loss: 1.0386, TrainAcc: 0.7420, ValAcc: 0.7606, ES: 01/50 | BestVal=0.7612@E19\n",
      "Epoch: 30, Time: 0.7383, Loss: 0.9152, TrainAcc: 0.7627, ValAcc: 0.7468, ES: 07/50 | BestVal=0.7649@E23\n",
      "Epoch: 40, Time: 0.7410, Loss: 0.8248, TrainAcc: 0.7769, ValAcc: 0.7603, ES: 03/50 | BestVal=0.7649@E37\n",
      "Epoch: 50, Time: 0.7394, Loss: 0.8052, TrainAcc: 0.7785, ValAcc: 0.7629, ES: 13/50 | BestVal=0.7649@E37\n",
      "Epoch: 60, Time: 0.7355, Loss: 0.7270, TrainAcc: 0.7944, ValAcc: 0.7598, ES: 23/50 | BestVal=0.7649@E37\n",
      "Epoch: 70, Time: 0.7253, Loss: 0.7099, TrainAcc: 0.7962, ValAcc: 0.7619, ES: 33/50 | BestVal=0.7649@E37\n",
      "Epoch: 80, Time: 0.7360, Loss: 0.6874, TrainAcc: 0.7992, ValAcc: 0.7616, ES: 43/50 | BestVal=0.7649@E37\n",
      "Early stopped, loading model from epoch-37\n",
      "Finished running train at 05-15 19:13:34, running time = 1.08min.\n",
      "[RevGAT + E] ValAcc: 0.7649, TestAcc: 0.7590\n",
      "\n",
      "(TA_P_E) ValAcc: 0.7795, TestAcc: 0.7741\n",
      "\n",
      "Running time: 271.22s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name RevGAT gnn.train.lr 0.002 gnn.train.dropout 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"pubmed\"\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
      "Processing...\n",
      "Done!\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/pubmed/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 132611\n",
      "Start running train at 05-15 19:18:20\n",
      "Epoch: 0, Time: 0.3593, Loss: 1.2193, TrainAcc: 0.3225, ValAcc: 0.9084, ES: 00/50 | BestVal=0.9084@E0\n",
      "Epoch: 10, Time: 0.0057, Loss: 0.1611, TrainAcc: 0.9724, ValAcc: 0.9533, ES: 01/50 | BestVal=0.9538@E9\n",
      "Epoch: 20, Time: 0.0057, Loss: 0.1125, TrainAcc: 0.9746, ValAcc: 0.9536, ES: 11/50 | BestVal=0.9538@E9\n",
      "Epoch: 30, Time: 0.0057, Loss: 0.1055, TrainAcc: 0.9753, ValAcc: 0.9536, ES: 07/50 | BestVal=0.9541@E23\n",
      "Epoch: 40, Time: 0.0049, Loss: 0.1028, TrainAcc: 0.9754, ValAcc: 0.9523, ES: 17/50 | BestVal=0.9541@E23\n",
      "Epoch: 50, Time: 0.0054, Loss: 0.0969, TrainAcc: 0.9761, ValAcc: 0.9523, ES: 27/50 | BestVal=0.9541@E23\n",
      "Epoch: 60, Time: 0.0050, Loss: 0.0908, TrainAcc: 0.9763, ValAcc: 0.9518, ES: 37/50 | BestVal=0.9541@E23\n",
      "Epoch: 70, Time: 0.0049, Loss: 0.0898, TrainAcc: 0.9773, ValAcc: 0.9533, ES: 47/50 | BestVal=0.9541@E23\n",
      "Early stopped, loading model from epoch-23\n",
      "Finished running train at 05-15 19:18:21, running time = 0.78s.\n",
      "[MLP + TA] ValAcc: 0.9541, TestAcc: 0.9480\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/pubmed.csv\n",
      "\n",
      "Number of parameters: 83971\n",
      "Start running train at 05-15 19:18:22\n",
      "Epoch: 0, Time: 0.0078, Loss: 1.2173, TrainAcc: 0.2870, ValAcc: 0.7542, ES: 00/50 | BestVal=0.7542@E0\n",
      "Epoch: 10, Time: 0.0072, Loss: 0.2961, TrainAcc: 0.9303, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E10\n",
      "Epoch: 20, Time: 0.0073, Loss: 0.2772, TrainAcc: 0.9314, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E20\n",
      "Epoch: 30, Time: 0.0072, Loss: 0.2680, TrainAcc: 0.9316, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E30\n",
      "Epoch: 40, Time: 0.0073, Loss: 0.2640, TrainAcc: 0.9328, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E40\n",
      "Epoch: 50, Time: 0.0077, Loss: 0.2639, TrainAcc: 0.9314, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E50\n",
      "Epoch: 60, Time: 0.0073, Loss: 0.2624, TrainAcc: 0.9327, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E60\n",
      "Epoch: 70, Time: 0.0072, Loss: 0.2622, TrainAcc: 0.9322, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E70\n",
      "Epoch: 80, Time: 0.0078, Loss: 0.2616, TrainAcc: 0.9322, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E80\n",
      "Epoch: 90, Time: 0.0072, Loss: 0.2578, TrainAcc: 0.9319, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E90\n",
      "Epoch: 100, Time: 0.0075, Loss: 0.2560, TrainAcc: 0.9331, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E100\n",
      "Epoch: 110, Time: 0.0076, Loss: 0.2579, TrainAcc: 0.9323, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E110\n",
      "Epoch: 120, Time: 0.0072, Loss: 0.2540, TrainAcc: 0.9320, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E120\n",
      "Epoch: 130, Time: 0.0071, Loss: 0.2524, TrainAcc: 0.9326, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E130\n",
      "Epoch: 140, Time: 0.0078, Loss: 0.2606, TrainAcc: 0.9316, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E140\n",
      "Epoch: 150, Time: 0.0072, Loss: 0.2597, TrainAcc: 0.9322, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E150\n",
      "Epoch: 160, Time: 0.0073, Loss: 0.2552, TrainAcc: 0.9325, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E160\n",
      "Epoch: 170, Time: 0.0078, Loss: 0.2556, TrainAcc: 0.9334, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E170\n",
      "Epoch: 180, Time: 0.0072, Loss: 0.2558, TrainAcc: 0.9320, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E180\n",
      "Epoch: 190, Time: 0.0075, Loss: 0.2552, TrainAcc: 0.9334, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E190\n",
      "Finished running train at 05-15 19:18:24, running time = 1.48s.\n",
      "[MLP + P] ValAcc: 0.9407, TestAcc: 0.9293\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/pubmed2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 132611\n",
      "Start running train at 05-15 19:18:25\n",
      "Epoch: 0, Time: 0.0069, Loss: 1.1038, TrainAcc: 0.4141, ValAcc: 0.5666, ES: 00/50 | BestVal=0.5666@E0\n",
      "Epoch: 10, Time: 0.0067, Loss: 0.1811, TrainAcc: 0.9579, ValAcc: 0.9478, ES: 00/50 | BestVal=0.9478@E10\n",
      "Epoch: 20, Time: 0.0049, Loss: 0.1541, TrainAcc: 0.9605, ValAcc: 0.9472, ES: 06/50 | BestVal=0.9493@E14\n",
      "Epoch: 30, Time: 0.0049, Loss: 0.1446, TrainAcc: 0.9625, ValAcc: 0.9457, ES: 16/50 | BestVal=0.9493@E14\n",
      "Epoch: 40, Time: 0.0049, Loss: 0.1408, TrainAcc: 0.9629, ValAcc: 0.9450, ES: 26/50 | BestVal=0.9493@E14\n",
      "Epoch: 50, Time: 0.0049, Loss: 0.1351, TrainAcc: 0.9637, ValAcc: 0.9457, ES: 36/50 | BestVal=0.9493@E14\n",
      "Epoch: 60, Time: 0.0049, Loss: 0.1278, TrainAcc: 0.9636, ValAcc: 0.9450, ES: 46/50 | BestVal=0.9493@E14\n",
      "Early stopped, loading model from epoch-14\n",
      "Finished running train at 05-15 19:18:25, running time = 0.36s.\n",
      "[MLP + E] ValAcc: 0.9493, TestAcc: 0.9409\n",
      "\n",
      "(TA_P_E) ValAcc: 0.9551, TestAcc: 0.9490\n",
      "\n",
      "Running time: 20.27s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name MLP gnn.train.lr 0.002 gnn.train.dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/pubmed/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 132611\n",
      "Start running train at 05-15 19:18:30\n",
      "Epoch: 0, Time: 0.3746, Loss: 1.3710, TrainAcc: 0.1722, ValAcc: 0.8341, ES: 00/50 | BestVal=0.8341@E0\n",
      "Epoch: 10, Time: 0.0117, Loss: 0.2600, TrainAcc: 0.9156, ValAcc: 0.8128, ES: 06/50 | BestVal=0.8478@E4\n",
      "Epoch: 20, Time: 0.0134, Loss: 0.2023, TrainAcc: 0.9395, ValAcc: 0.8534, ES: 00/50 | BestVal=0.8534@E20\n",
      "Epoch: 30, Time: 0.0130, Loss: 0.1770, TrainAcc: 0.9460, ValAcc: 0.8800, ES: 00/50 | BestVal=0.8800@E30\n",
      "Epoch: 40, Time: 0.0126, Loss: 0.1611, TrainAcc: 0.9493, ValAcc: 0.9148, ES: 00/50 | BestVal=0.9148@E40\n",
      "Epoch: 50, Time: 0.0113, Loss: 0.1708, TrainAcc: 0.9486, ValAcc: 0.9211, ES: 04/50 | BestVal=0.9254@E46\n",
      "Epoch: 60, Time: 0.0108, Loss: 0.1573, TrainAcc: 0.9495, ValAcc: 0.9234, ES: 14/50 | BestVal=0.9254@E46\n",
      "Epoch: 70, Time: 0.0113, Loss: 0.1456, TrainAcc: 0.9543, ValAcc: 0.9282, ES: 09/50 | BestVal=0.9303@E61\n",
      "Epoch: 80, Time: 0.0132, Loss: 0.1356, TrainAcc: 0.9561, ValAcc: 0.9336, ES: 00/50 | BestVal=0.9336@E80\n",
      "Epoch: 90, Time: 0.0108, Loss: 0.1618, TrainAcc: 0.9494, ValAcc: 0.9021, ES: 10/50 | BestVal=0.9336@E80\n",
      "Epoch: 100, Time: 0.0118, Loss: 0.1521, TrainAcc: 0.9527, ValAcc: 0.9221, ES: 20/50 | BestVal=0.9336@E80\n",
      "Epoch: 110, Time: 0.0113, Loss: 0.1403, TrainAcc: 0.9552, ValAcc: 0.9270, ES: 30/50 | BestVal=0.9336@E80\n",
      "Epoch: 120, Time: 0.0108, Loss: 0.1306, TrainAcc: 0.9571, ValAcc: 0.9270, ES: 40/50 | BestVal=0.9336@E80\n",
      "Early stopped, loading model from epoch-80\n",
      "Finished running train at 05-15 19:18:32, running time = 1.90s.\n",
      "[GCN + TA] ValAcc: 0.9336, TestAcc: 0.9288\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/pubmed.csv\n",
      "\n",
      "Number of parameters: 83971\n",
      "Start running train at 05-15 19:18:33\n",
      "Epoch: 0, Time: 0.0151, Loss: 1.3488, TrainAcc: 0.1904, ValAcc: 0.3741, ES: 00/50 | BestVal=0.3741@E0\n",
      "Epoch: 10, Time: 0.0132, Loss: 0.3034, TrainAcc: 0.8939, ValAcc: 0.7550, ES: 00/50 | BestVal=0.7550@E10\n",
      "Epoch: 20, Time: 0.0132, Loss: 0.2587, TrainAcc: 0.9128, ValAcc: 0.8701, ES: 00/50 | BestVal=0.8701@E20\n",
      "Epoch: 30, Time: 0.0136, Loss: 0.2399, TrainAcc: 0.9171, ValAcc: 0.9057, ES: 00/50 | BestVal=0.9057@E30\n",
      "Epoch: 40, Time: 0.0122, Loss: 0.2293, TrainAcc: 0.9225, ValAcc: 0.9092, ES: 06/50 | BestVal=0.9107@E34\n",
      "Epoch: 50, Time: 0.0115, Loss: 0.2237, TrainAcc: 0.9244, ValAcc: 0.9110, ES: 02/50 | BestVal=0.9171@E48\n",
      "Epoch: 60, Time: 0.0122, Loss: 0.2183, TrainAcc: 0.9279, ValAcc: 0.9158, ES: 05/50 | BestVal=0.9188@E55\n",
      "Epoch: 70, Time: 0.0121, Loss: 0.2135, TrainAcc: 0.9287, ValAcc: 0.9176, ES: 05/50 | BestVal=0.9204@E65\n",
      "Epoch: 80, Time: 0.0122, Loss: 0.2116, TrainAcc: 0.9297, ValAcc: 0.9181, ES: 09/50 | BestVal=0.9206@E71\n",
      "Epoch: 90, Time: 0.0123, Loss: 0.2174, TrainAcc: 0.9281, ValAcc: 0.9186, ES: 19/50 | BestVal=0.9206@E71\n",
      "Epoch: 100, Time: 0.0121, Loss: 0.2092, TrainAcc: 0.9318, ValAcc: 0.9125, ES: 05/50 | BestVal=0.9206@E95\n",
      "Epoch: 110, Time: 0.0122, Loss: 0.2107, TrainAcc: 0.9304, ValAcc: 0.9206, ES: 06/50 | BestVal=0.9219@E104\n",
      "Epoch: 120, Time: 0.0122, Loss: 0.1990, TrainAcc: 0.9345, ValAcc: 0.9163, ES: 16/50 | BestVal=0.9219@E104\n",
      "Epoch: 130, Time: 0.0116, Loss: 0.2130, TrainAcc: 0.9294, ValAcc: 0.9206, ES: 26/50 | BestVal=0.9219@E104\n",
      "Epoch: 140, Time: 0.0115, Loss: 0.1946, TrainAcc: 0.9356, ValAcc: 0.9183, ES: 02/50 | BestVal=0.9232@E138\n",
      "Epoch: 150, Time: 0.0121, Loss: 0.2147, TrainAcc: 0.9307, ValAcc: 0.9097, ES: 12/50 | BestVal=0.9232@E138\n",
      "Epoch: 160, Time: 0.0121, Loss: 0.2039, TrainAcc: 0.9333, ValAcc: 0.9191, ES: 22/50 | BestVal=0.9232@E138\n",
      "Epoch: 170, Time: 0.0121, Loss: 0.1954, TrainAcc: 0.9347, ValAcc: 0.9112, ES: 32/50 | BestVal=0.9232@E138\n",
      "Epoch: 180, Time: 0.0121, Loss: 0.1973, TrainAcc: 0.9351, ValAcc: 0.9183, ES: 01/50 | BestVal=0.9239@E179\n",
      "Epoch: 190, Time: 0.0122, Loss: 0.1952, TrainAcc: 0.9345, ValAcc: 0.9206, ES: 03/50 | BestVal=0.9270@E187\n",
      "Finished running train at 05-15 19:18:36, running time = 2.47s.\n",
      "[GCN + P] ValAcc: 0.9270, TestAcc: 0.9201\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/pubmed2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 132611\n",
      "Start running train at 05-15 19:18:37\n",
      "Epoch: 0, Time: 0.0141, Loss: 1.2868, TrainAcc: 0.2996, ValAcc: 0.8283, ES: 00/50 | BestVal=0.8283@E0\n",
      "Epoch: 10, Time: 0.0108, Loss: 0.3007, TrainAcc: 0.8940, ValAcc: 0.5549, ES: 08/50 | BestVal=0.8557@E2\n",
      "Epoch: 20, Time: 0.0112, Loss: 0.2400, TrainAcc: 0.9216, ValAcc: 0.7946, ES: 18/50 | BestVal=0.8557@E2\n",
      "Epoch: 30, Time: 0.0131, Loss: 0.2117, TrainAcc: 0.9309, ValAcc: 0.8876, ES: 00/50 | BestVal=0.8876@E30\n",
      "Epoch: 40, Time: 0.0115, Loss: 0.1944, TrainAcc: 0.9365, ValAcc: 0.9082, ES: 01/50 | BestVal=0.9153@E39\n",
      "Epoch: 50, Time: 0.0113, Loss: 0.1838, TrainAcc: 0.9401, ValAcc: 0.9054, ES: 02/50 | BestVal=0.9270@E48\n",
      "Epoch: 60, Time: 0.0114, Loss: 0.1827, TrainAcc: 0.9405, ValAcc: 0.9148, ES: 12/50 | BestVal=0.9270@E48\n",
      "Epoch: 70, Time: 0.0109, Loss: 0.1714, TrainAcc: 0.9433, ValAcc: 0.9219, ES: 22/50 | BestVal=0.9270@E48\n",
      "Epoch: 80, Time: 0.0132, Loss: 0.1625, TrainAcc: 0.9458, ValAcc: 0.9303, ES: 00/50 | BestVal=0.9303@E80\n",
      "Epoch: 90, Time: 0.0114, Loss: 0.3213, TrainAcc: 0.9174, ValAcc: 0.8826, ES: 10/50 | BestVal=0.9303@E80\n",
      "Epoch: 100, Time: 0.0108, Loss: 0.2107, TrainAcc: 0.9307, ValAcc: 0.9178, ES: 20/50 | BestVal=0.9303@E80\n",
      "Epoch: 110, Time: 0.0113, Loss: 0.1910, TrainAcc: 0.9375, ValAcc: 0.9054, ES: 30/50 | BestVal=0.9303@E80\n",
      "Epoch: 120, Time: 0.0114, Loss: 0.1772, TrainAcc: 0.9389, ValAcc: 0.9194, ES: 40/50 | BestVal=0.9303@E80\n",
      "Early stopped, loading model from epoch-80\n",
      "Finished running train at 05-15 19:18:38, running time = 1.50s.\n",
      "[GCN + E] ValAcc: 0.9303, TestAcc: 0.9295\n",
      "\n",
      "(TA_P_E) ValAcc: 0.9371, TestAcc: 0.9356\n",
      "\n",
      "Running time: 11.38s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/pubmed/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 264067\n",
      "Start running train at 05-15 19:18:43\n",
      "Epoch: 0, Time: 0.3620, Loss: 1.3075, TrainAcc: 0.2051, ValAcc: 0.5394, ES: 00/50 | BestVal=0.5394@E0\n",
      "Epoch: 10, Time: 0.0215, Loss: 0.0958, TrainAcc: 0.9744, ValAcc: 0.9259, ES: 00/50 | BestVal=0.9259@E10\n",
      "Epoch: 20, Time: 0.0171, Loss: 0.0866, TrainAcc: 0.9766, ValAcc: 0.9521, ES: 03/50 | BestVal=0.9523@E17\n",
      "Epoch: 30, Time: 0.0192, Loss: 0.0827, TrainAcc: 0.9766, ValAcc: 0.9543, ES: 00/50 | BestVal=0.9543@E30\n",
      "Epoch: 40, Time: 0.0171, Loss: 0.0797, TrainAcc: 0.9769, ValAcc: 0.9533, ES: 06/50 | BestVal=0.9549@E34\n",
      "Epoch: 50, Time: 0.0170, Loss: 0.0772, TrainAcc: 0.9773, ValAcc: 0.9528, ES: 16/50 | BestVal=0.9549@E34\n",
      "Epoch: 60, Time: 0.0170, Loss: 0.0746, TrainAcc: 0.9778, ValAcc: 0.9531, ES: 26/50 | BestVal=0.9549@E34\n",
      "Epoch: 70, Time: 0.0170, Loss: 0.0719, TrainAcc: 0.9785, ValAcc: 0.9536, ES: 36/50 | BestVal=0.9549@E34\n",
      "Epoch: 80, Time: 0.0171, Loss: 0.0688, TrainAcc: 0.9790, ValAcc: 0.9526, ES: 46/50 | BestVal=0.9549@E34\n",
      "Early stopped, loading model from epoch-34\n",
      "Finished running train at 05-15 19:18:45, running time = 1.89s.\n",
      "[SAGE + TA] ValAcc: 0.9549, TestAcc: 0.9518\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/pubmed.csv\n",
      "\n",
      "Number of parameters: 166275\n",
      "Start running train at 05-15 19:18:46\n",
      "Epoch: 0, Time: 0.0187, Loss: 0.9759, TrainAcc: 0.5098, ValAcc: 0.7089, ES: 00/50 | BestVal=0.7089@E0\n",
      "Epoch: 10, Time: 0.0177, Loss: 0.2293, TrainAcc: 0.9337, ValAcc: 0.9082, ES: 00/50 | BestVal=0.9082@E10\n",
      "Epoch: 20, Time: 0.0182, Loss: 0.2063, TrainAcc: 0.9360, ValAcc: 0.9336, ES: 00/50 | BestVal=0.9336@E20\n",
      "Epoch: 30, Time: 0.0180, Loss: 0.1979, TrainAcc: 0.9365, ValAcc: 0.9356, ES: 00/50 | BestVal=0.9356@E30\n",
      "Epoch: 40, Time: 0.0183, Loss: 0.1931, TrainAcc: 0.9369, ValAcc: 0.9396, ES: 00/50 | BestVal=0.9396@E40\n",
      "Epoch: 50, Time: 0.0176, Loss: 0.1894, TrainAcc: 0.9390, ValAcc: 0.9409, ES: 00/50 | BestVal=0.9409@E50\n",
      "Epoch: 60, Time: 0.0183, Loss: 0.1863, TrainAcc: 0.9401, ValAcc: 0.9419, ES: 00/50 | BestVal=0.9419@E60\n",
      "Epoch: 70, Time: 0.0162, Loss: 0.1835, TrainAcc: 0.9409, ValAcc: 0.9419, ES: 05/50 | BestVal=0.9424@E65\n",
      "Epoch: 80, Time: 0.0159, Loss: 0.1806, TrainAcc: 0.9419, ValAcc: 0.9424, ES: 02/50 | BestVal=0.9427@E78\n",
      "Epoch: 90, Time: 0.0181, Loss: 0.1776, TrainAcc: 0.9423, ValAcc: 0.9427, ES: 00/50 | BestVal=0.9427@E90\n",
      "Epoch: 100, Time: 0.0162, Loss: 0.1745, TrainAcc: 0.9429, ValAcc: 0.9422, ES: 02/50 | BestVal=0.9427@E98\n",
      "Epoch: 110, Time: 0.0163, Loss: 0.1747, TrainAcc: 0.9432, ValAcc: 0.9401, ES: 05/50 | BestVal=0.9427@E105\n",
      "Epoch: 120, Time: 0.0160, Loss: 0.1687, TrainAcc: 0.9448, ValAcc: 0.9379, ES: 15/50 | BestVal=0.9427@E105\n",
      "Epoch: 130, Time: 0.0160, Loss: 0.1651, TrainAcc: 0.9452, ValAcc: 0.9394, ES: 25/50 | BestVal=0.9427@E105\n",
      "Epoch: 140, Time: 0.0161, Loss: 0.1666, TrainAcc: 0.9451, ValAcc: 0.9394, ES: 35/50 | BestVal=0.9427@E105\n",
      "Epoch: 150, Time: 0.0159, Loss: 0.1712, TrainAcc: 0.9441, ValAcc: 0.9379, ES: 45/50 | BestVal=0.9427@E105\n",
      "Early stopped, loading model from epoch-105\n",
      "Finished running train at 05-15 19:18:49, running time = 2.63s.\n",
      "[SAGE + P] ValAcc: 0.9427, TestAcc: 0.9318\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/pubmed2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 264067\n",
      "Start running train at 05-15 19:18:50\n",
      "Epoch: 0, Time: 0.0196, Loss: 1.3597, TrainAcc: 0.2085, ValAcc: 0.5301, ES: 00/50 | BestVal=0.5301@E0\n",
      "Epoch: 10, Time: 0.0194, Loss: 0.1375, TrainAcc: 0.9568, ValAcc: 0.9257, ES: 00/50 | BestVal=0.9257@E10\n",
      "Epoch: 20, Time: 0.0194, Loss: 0.1246, TrainAcc: 0.9598, ValAcc: 0.9475, ES: 00/50 | BestVal=0.9475@E20\n",
      "Epoch: 30, Time: 0.0168, Loss: 0.1182, TrainAcc: 0.9609, ValAcc: 0.9478, ES: 04/50 | BestVal=0.9480@E26\n",
      "Epoch: 40, Time: 0.0169, Loss: 0.1139, TrainAcc: 0.9634, ValAcc: 0.9455, ES: 14/50 | BestVal=0.9480@E26\n",
      "Epoch: 50, Time: 0.0169, Loss: 0.1099, TrainAcc: 0.9643, ValAcc: 0.9447, ES: 24/50 | BestVal=0.9480@E26\n",
      "Epoch: 60, Time: 0.0170, Loss: 0.1062, TrainAcc: 0.9654, ValAcc: 0.9447, ES: 34/50 | BestVal=0.9480@E26\n",
      "Epoch: 70, Time: 0.0169, Loss: 0.1027, TrainAcc: 0.9663, ValAcc: 0.9450, ES: 44/50 | BestVal=0.9480@E26\n",
      "Early stopped, loading model from epoch-26\n",
      "Finished running train at 05-15 19:18:51, running time = 1.37s.\n",
      "[SAGE + E] ValAcc: 0.9480, TestAcc: 0.9419\n",
      "\n",
      "(TA_P_E) ValAcc: 0.9566, TestAcc: 0.9508\n",
      "\n",
      "Running time: 11.39s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name SAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/pubmed/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 892038\n",
      "Start running train at 05-15 19:18:56\n",
      "Epoch: 0, Time: 0.5682, Loss: 2.6145, TrainAcc: 0.2658, ValAcc: 0.2026, ES: 00/50 | BestVal=0.2026@E0\n",
      "Epoch: 10, Time: 0.0538, Loss: 0.2185, TrainAcc: 0.9408, ValAcc: 0.9541, ES: 00/50 | BestVal=0.9541@E10\n",
      "Epoch: 20, Time: 0.0479, Loss: 0.1404, TrainAcc: 0.9661, ValAcc: 0.9549, ES: 04/50 | BestVal=0.9554@E16\n",
      "Epoch: 30, Time: 0.0485, Loss: 0.1166, TrainAcc: 0.9726, ValAcc: 0.9536, ES: 14/50 | BestVal=0.9554@E16\n",
      "Epoch: 40, Time: 0.0480, Loss: 0.1219, TrainAcc: 0.9705, ValAcc: 0.9490, ES: 03/50 | BestVal=0.9554@E37\n",
      "Epoch: 50, Time: 0.0479, Loss: 0.1111, TrainAcc: 0.9731, ValAcc: 0.9475, ES: 01/50 | BestVal=0.9564@E49\n",
      "Epoch: 60, Time: 0.0482, Loss: 0.0918, TrainAcc: 0.9757, ValAcc: 0.9526, ES: 11/50 | BestVal=0.9564@E49\n",
      "Epoch: 70, Time: 0.0485, Loss: 0.0883, TrainAcc: 0.9761, ValAcc: 0.9513, ES: 21/50 | BestVal=0.9564@E49\n",
      "Epoch: 80, Time: 0.0482, Loss: 0.0888, TrainAcc: 0.9762, ValAcc: 0.9526, ES: 31/50 | BestVal=0.9564@E49\n",
      "Epoch: 90, Time: 0.0479, Loss: 0.0811, TrainAcc: 0.9786, ValAcc: 0.9541, ES: 41/50 | BestVal=0.9564@E49\n",
      "Early stopped, loading model from epoch-49\n",
      "Finished running train at 05-15 19:19:01, running time = 5.47s.\n",
      "[RevGAT + TA] ValAcc: 0.9564, TestAcc: 0.9503\n",
      "\n",
      "/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/pubmed.csv\n",
      "\n",
      "Number of parameters: 596870\n",
      "Start running train at 05-15 19:19:03\n",
      "Epoch: 0, Time: 0.0512, Loss: 1.8340, TrainAcc: 0.3809, ValAcc: 0.6262, ES: 00/50 | BestVal=0.6262@E0\n",
      "Epoch: 10, Time: 0.0502, Loss: 0.5659, TrainAcc: 0.8695, ValAcc: 0.9249, ES: 00/50 | BestVal=0.9249@E10\n",
      "Epoch: 20, Time: 0.0500, Loss: 0.3797, TrainAcc: 0.9082, ValAcc: 0.9407, ES: 00/50 | BestVal=0.9407@E20\n",
      "Epoch: 30, Time: 0.0454, Loss: 0.3408, TrainAcc: 0.9126, ValAcc: 0.9389, ES: 09/50 | BestVal=0.9417@E21\n",
      "Epoch: 40, Time: 0.0451, Loss: 0.3428, TrainAcc: 0.9145, ValAcc: 0.9221, ES: 19/50 | BestVal=0.9417@E21\n",
      "Epoch: 50, Time: 0.0448, Loss: 0.2891, TrainAcc: 0.9218, ValAcc: 0.9328, ES: 29/50 | BestVal=0.9417@E21\n",
      "Epoch: 60, Time: 0.0453, Loss: 0.2653, TrainAcc: 0.9256, ValAcc: 0.9297, ES: 39/50 | BestVal=0.9417@E21\n",
      "Epoch: 70, Time: 0.0449, Loss: 0.2478, TrainAcc: 0.9270, ValAcc: 0.9366, ES: 01/50 | BestVal=0.9424@E69\n",
      "Epoch: 80, Time: 0.0454, Loss: 0.2395, TrainAcc: 0.9290, ValAcc: 0.9409, ES: 11/50 | BestVal=0.9424@E69\n",
      "Epoch: 90, Time: 0.0453, Loss: 0.2330, TrainAcc: 0.9283, ValAcc: 0.9412, ES: 07/50 | BestVal=0.9427@E83\n",
      "Epoch: 100, Time: 0.0451, Loss: 0.2275, TrainAcc: 0.9297, ValAcc: 0.9424, ES: 02/50 | BestVal=0.9427@E98\n",
      "Epoch: 110, Time: 0.0447, Loss: 0.2287, TrainAcc: 0.9291, ValAcc: 0.9419, ES: 12/50 | BestVal=0.9427@E98\n",
      "Epoch: 120, Time: 0.0492, Loss: 0.2238, TrainAcc: 0.9287, ValAcc: 0.9432, ES: 00/50 | BestVal=0.9432@E120\n",
      "Epoch: 130, Time: 0.0448, Loss: 0.2205, TrainAcc: 0.9324, ValAcc: 0.9407, ES: 10/50 | BestVal=0.9432@E120\n",
      "Epoch: 140, Time: 0.0462, Loss: 0.2153, TrainAcc: 0.9324, ValAcc: 0.9412, ES: 20/50 | BestVal=0.9432@E120\n",
      "Epoch: 150, Time: 0.0451, Loss: 0.2131, TrainAcc: 0.9341, ValAcc: 0.9412, ES: 30/50 | BestVal=0.9432@E120\n",
      "Epoch: 160, Time: 0.0448, Loss: 0.2145, TrainAcc: 0.9342, ValAcc: 0.9422, ES: 40/50 | BestVal=0.9432@E120\n",
      "Early stopped, loading model from epoch-120\n",
      "Finished running train at 05-15 19:19:10, running time = 7.86s.\n",
      "[RevGAT + P] ValAcc: 0.9432, TestAcc: 0.9323\n",
      "\n",
      "/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/pubmed2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 892038\n",
      "Start running train at 05-15 19:19:12\n",
      "Epoch: 0, Time: 0.0539, Loss: 1.9597, TrainAcc: 0.3779, ValAcc: 0.2716, ES: 00/50 | BestVal=0.2716@E0\n",
      "Epoch: 10, Time: 0.0479, Loss: 0.2565, TrainAcc: 0.9321, ValAcc: 0.9513, ES: 01/50 | BestVal=0.9518@E9\n",
      "Epoch: 20, Time: 0.0479, Loss: 0.1957, TrainAcc: 0.9524, ValAcc: 0.9500, ES: 06/50 | BestVal=0.9533@E14\n",
      "Epoch: 30, Time: 0.0486, Loss: 0.1671, TrainAcc: 0.9538, ValAcc: 0.9460, ES: 16/50 | BestVal=0.9533@E14\n",
      "Epoch: 40, Time: 0.0481, Loss: 0.1583, TrainAcc: 0.9576, ValAcc: 0.9455, ES: 26/50 | BestVal=0.9533@E14\n",
      "Epoch: 50, Time: 0.0482, Loss: 0.1430, TrainAcc: 0.9598, ValAcc: 0.9452, ES: 36/50 | BestVal=0.9533@E14\n",
      "Epoch: 60, Time: 0.0479, Loss: 0.1203, TrainAcc: 0.9633, ValAcc: 0.9472, ES: 46/50 | BestVal=0.9533@E14\n",
      "Early stopped, loading model from epoch-14\n",
      "Finished running train at 05-15 19:19:15, running time = 3.22s.\n",
      "[RevGAT + E] ValAcc: 0.9533, TestAcc: 0.9435\n",
      "\n",
      "(TA_P_E) ValAcc: 0.9571, TestAcc: 0.9506\n",
      "\n",
      "Running time: 22.26s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name RevGAT gnn.train.lr 0.002 gnn.train.dropout 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cora\"\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/cora/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 133127\n",
      "Start running train at 05-15 19:19:30\n",
      "Epoch: 0, Time: 0.3322, Loss: 2.1093, TrainAcc: 0.1324, ValAcc: 0.3579, ES: 00/50 | BestVal=0.3579@E0\n",
      "Epoch: 10, Time: 0.0038, Loss: 0.8035, TrainAcc: 0.7820, ValAcc: 0.7638, ES: 02/50 | BestVal=0.7675@E8\n",
      "Epoch: 20, Time: 0.0047, Loss: 0.5586, TrainAcc: 0.8417, ValAcc: 0.7786, ES: 00/50 | BestVal=0.7786@E20\n",
      "Epoch: 30, Time: 0.0031, Loss: 0.4482, TrainAcc: 0.8799, ValAcc: 0.7731, ES: 08/50 | BestVal=0.7786@E22\n",
      "Epoch: 40, Time: 0.0031, Loss: 0.3534, TrainAcc: 0.8990, ValAcc: 0.7694, ES: 18/50 | BestVal=0.7786@E22\n",
      "Epoch: 50, Time: 0.0031, Loss: 0.3001, TrainAcc: 0.9175, ValAcc: 0.7768, ES: 28/50 | BestVal=0.7786@E22\n",
      "Epoch: 60, Time: 0.0032, Loss: 0.2181, TrainAcc: 0.9446, ValAcc: 0.7731, ES: 05/50 | BestVal=0.7860@E55\n",
      "Epoch: 70, Time: 0.0032, Loss: 0.1809, TrainAcc: 0.9538, ValAcc: 0.7638, ES: 15/50 | BestVal=0.7860@E55\n",
      "Epoch: 80, Time: 0.0032, Loss: 0.1327, TrainAcc: 0.9643, ValAcc: 0.7712, ES: 25/50 | BestVal=0.7860@E55\n",
      "Epoch: 90, Time: 0.0031, Loss: 0.1295, TrainAcc: 0.9643, ValAcc: 0.7749, ES: 35/50 | BestVal=0.7860@E55\n",
      "Epoch: 100, Time: 0.0032, Loss: 0.0969, TrainAcc: 0.9729, ValAcc: 0.7583, ES: 45/50 | BestVal=0.7860@E55\n",
      "Early stopped, loading model from epoch-55\n",
      "Finished running train at 05-15 19:19:30, running time = 0.72s.\n",
      "[MLP + TA] ValAcc: 0.7860, TestAcc: 0.8100\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/cora.csv\n",
      "\n",
      "Number of parameters: 117767\n",
      "Start running train at 05-15 19:19:33\n",
      "Epoch: 0, Time: 0.0061, Loss: 2.0319, TrainAcc: 0.1533, ValAcc: 0.3727, ES: 00/50 | BestVal=0.3727@E0\n",
      "Epoch: 10, Time: 0.0052, Loss: 1.1424, TrainAcc: 0.6127, ValAcc: 0.6679, ES: 00/50 | BestVal=0.6679@E10\n",
      "Epoch: 20, Time: 0.0041, Loss: 1.0024, TrainAcc: 0.6626, ValAcc: 0.6827, ES: 05/50 | BestVal=0.6900@E15\n",
      "Epoch: 30, Time: 0.0035, Loss: 0.9542, TrainAcc: 0.6890, ValAcc: 0.6790, ES: 15/50 | BestVal=0.6900@E15\n",
      "Epoch: 40, Time: 0.0035, Loss: 0.9475, TrainAcc: 0.6872, ValAcc: 0.6771, ES: 25/50 | BestVal=0.6900@E15\n",
      "Epoch: 50, Time: 0.0035, Loss: 0.9161, TrainAcc: 0.6927, ValAcc: 0.6753, ES: 35/50 | BestVal=0.6900@E15\n",
      "Epoch: 60, Time: 0.0041, Loss: 0.9384, TrainAcc: 0.6897, ValAcc: 0.6771, ES: 45/50 | BestVal=0.6900@E15\n",
      "Early stopped, loading model from epoch-15\n",
      "Finished running train at 05-15 19:19:33, running time = 0.30s.\n",
      "[MLP + P] ValAcc: 0.6900, TestAcc: 0.6642\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/cora2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 133127\n",
      "Start running train at 05-15 19:19:35\n",
      "Epoch: 0, Time: 0.0057, Loss: 2.0955, TrainAcc: 0.1336, ValAcc: 0.2768, ES: 00/50 | BestVal=0.2768@E0\n",
      "Epoch: 10, Time: 0.0048, Loss: 0.8788, TrainAcc: 0.7506, ValAcc: 0.7768, ES: 00/50 | BestVal=0.7768@E10\n",
      "Epoch: 20, Time: 0.0031, Loss: 0.7034, TrainAcc: 0.7913, ValAcc: 0.7657, ES: 10/50 | BestVal=0.7768@E10\n",
      "Epoch: 30, Time: 0.0037, Loss: 0.6023, TrainAcc: 0.8233, ValAcc: 0.7583, ES: 20/50 | BestVal=0.7768@E10\n",
      "Epoch: 40, Time: 0.0031, Loss: 0.5095, TrainAcc: 0.8510, ValAcc: 0.7675, ES: 30/50 | BestVal=0.7768@E10\n",
      "Epoch: 50, Time: 0.0049, Loss: 0.4577, TrainAcc: 0.8688, ValAcc: 0.7786, ES: 00/50 | BestVal=0.7786@E50\n",
      "Epoch: 60, Time: 0.0032, Loss: 0.3586, TrainAcc: 0.8892, ValAcc: 0.7786, ES: 09/50 | BestVal=0.7804@E51\n",
      "Epoch: 70, Time: 0.0032, Loss: 0.3218, TrainAcc: 0.9070, ValAcc: 0.7768, ES: 19/50 | BestVal=0.7804@E51\n",
      "Epoch: 80, Time: 0.0031, Loss: 0.2385, TrainAcc: 0.9255, ValAcc: 0.7731, ES: 29/50 | BestVal=0.7804@E51\n",
      "Epoch: 90, Time: 0.0037, Loss: 0.2155, TrainAcc: 0.9323, ValAcc: 0.7786, ES: 02/50 | BestVal=0.7860@E88\n",
      "Epoch: 100, Time: 0.0049, Loss: 0.1979, TrainAcc: 0.9421, ValAcc: 0.7878, ES: 00/50 | BestVal=0.7878@E100\n",
      "Epoch: 110, Time: 0.0038, Loss: 0.1533, TrainAcc: 0.9550, ValAcc: 0.7749, ES: 10/50 | BestVal=0.7878@E100\n",
      "Epoch: 120, Time: 0.0031, Loss: 0.1344, TrainAcc: 0.9557, ValAcc: 0.7804, ES: 20/50 | BestVal=0.7878@E100\n",
      "Epoch: 130, Time: 0.0033, Loss: 0.1141, TrainAcc: 0.9631, ValAcc: 0.7878, ES: 05/50 | BestVal=0.7897@E125\n",
      "Epoch: 140, Time: 0.0032, Loss: 0.1033, TrainAcc: 0.9674, ValAcc: 0.7731, ES: 15/50 | BestVal=0.7897@E125\n",
      "Epoch: 150, Time: 0.0031, Loss: 0.0838, TrainAcc: 0.9754, ValAcc: 0.7694, ES: 25/50 | BestVal=0.7897@E125\n",
      "Epoch: 160, Time: 0.0032, Loss: 0.0807, TrainAcc: 0.9741, ValAcc: 0.7786, ES: 35/50 | BestVal=0.7897@E125\n",
      "Epoch: 170, Time: 0.0033, Loss: 0.0700, TrainAcc: 0.9828, ValAcc: 0.7786, ES: 45/50 | BestVal=0.7897@E125\n",
      "Epoch: 180, Time: 0.0032, Loss: 0.0687, TrainAcc: 0.9760, ValAcc: 0.7841, ES: 05/50 | BestVal=0.7934@E175\n",
      "Epoch: 190, Time: 0.0031, Loss: 0.0561, TrainAcc: 0.9821, ValAcc: 0.7823, ES: 15/50 | BestVal=0.7934@E175\n",
      "Finished running train at 05-15 19:19:36, running time = 0.71s.\n",
      "[MLP + E] ValAcc: 0.7970, TestAcc: 0.7841\n",
      "\n",
      "(TA_P_E) ValAcc: 0.8247, TestAcc: 0.8118\n",
      "\n",
      "Running time: 19.69s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name MLP gnn.train.lr 0.002 gnn.train.dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/cora/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 133127\n",
      "Start running train at 05-15 19:19:43\n",
      "Epoch: 0, Time: 0.3493, Loss: 1.9980, TrainAcc: 0.2562, ValAcc: 0.5941, ES: 00/50 | BestVal=0.5941@E0\n",
      "Epoch: 10, Time: 0.0061, Loss: 0.3165, TrainAcc: 0.9002, ValAcc: 0.8635, ES: 00/50 | BestVal=0.8635@E10\n",
      "Epoch: 20, Time: 0.0051, Loss: 0.2621, TrainAcc: 0.9156, ValAcc: 0.8727, ES: 05/50 | BestVal=0.8764@E15\n",
      "Epoch: 30, Time: 0.0044, Loss: 0.2206, TrainAcc: 0.9249, ValAcc: 0.8561, ES: 15/50 | BestVal=0.8764@E15\n",
      "Epoch: 40, Time: 0.0045, Loss: 0.1794, TrainAcc: 0.9372, ValAcc: 0.8708, ES: 25/50 | BestVal=0.8764@E15\n",
      "Epoch: 50, Time: 0.0044, Loss: 0.1537, TrainAcc: 0.9397, ValAcc: 0.8561, ES: 35/50 | BestVal=0.8764@E15\n",
      "Epoch: 60, Time: 0.0044, Loss: 0.1205, TrainAcc: 0.9563, ValAcc: 0.8598, ES: 45/50 | BestVal=0.8764@E15\n",
      "Early stopped, loading model from epoch-15\n",
      "Finished running train at 05-15 19:19:44, running time = 0.68s.\n",
      "[GCN + TA] ValAcc: 0.8764, TestAcc: 0.8745\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/cora.csv\n",
      "\n",
      "Number of parameters: 117767\n",
      "Start running train at 05-15 19:19:46\n",
      "Epoch: 0, Time: 0.0087, Loss: 2.1152, TrainAcc: 0.1804, ValAcc: 0.3985, ES: 00/50 | BestVal=0.3985@E0\n",
      "Epoch: 10, Time: 0.0076, Loss: 0.4438, TrainAcc: 0.8393, ValAcc: 0.7066, ES: 00/50 | BestVal=0.7066@E10\n",
      "Epoch: 20, Time: 0.0067, Loss: 0.3369, TrainAcc: 0.8824, ValAcc: 0.7712, ES: 00/50 | BestVal=0.7712@E20\n",
      "Epoch: 30, Time: 0.0050, Loss: 0.2767, TrainAcc: 0.9027, ValAcc: 0.7970, ES: 01/50 | BestVal=0.8026@E29\n",
      "Epoch: 40, Time: 0.0049, Loss: 0.2270, TrainAcc: 0.9236, ValAcc: 0.7712, ES: 03/50 | BestVal=0.8173@E37\n",
      "Epoch: 50, Time: 0.0052, Loss: 0.1845, TrainAcc: 0.9390, ValAcc: 0.7878, ES: 03/50 | BestVal=0.8303@E47\n",
      "Epoch: 60, Time: 0.0047, Loss: 0.1392, TrainAcc: 0.9550, ValAcc: 0.8229, ES: 13/50 | BestVal=0.8303@E47\n",
      "Epoch: 70, Time: 0.0047, Loss: 0.2541, TrainAcc: 0.9119, ValAcc: 0.8026, ES: 23/50 | BestVal=0.8303@E47\n",
      "Epoch: 80, Time: 0.0047, Loss: 0.1432, TrainAcc: 0.9526, ValAcc: 0.7952, ES: 33/50 | BestVal=0.8303@E47\n",
      "Epoch: 90, Time: 0.0047, Loss: 0.1075, TrainAcc: 0.9643, ValAcc: 0.8266, ES: 06/50 | BestVal=0.8395@E84\n",
      "Epoch: 100, Time: 0.0047, Loss: 0.0830, TrainAcc: 0.9748, ValAcc: 0.8137, ES: 16/50 | BestVal=0.8395@E84\n",
      "Epoch: 110, Time: 0.0047, Loss: 0.0797, TrainAcc: 0.9760, ValAcc: 0.8118, ES: 04/50 | BestVal=0.8413@E106\n",
      "Epoch: 120, Time: 0.0048, Loss: 0.0649, TrainAcc: 0.9828, ValAcc: 0.8137, ES: 14/50 | BestVal=0.8413@E106\n",
      "Epoch: 130, Time: 0.0047, Loss: 0.0568, TrainAcc: 0.9748, ValAcc: 0.8284, ES: 24/50 | BestVal=0.8413@E106\n",
      "Epoch: 140, Time: 0.0047, Loss: 0.0421, TrainAcc: 0.9871, ValAcc: 0.8192, ES: 34/50 | BestVal=0.8413@E106\n",
      "Epoch: 150, Time: 0.0047, Loss: 1.0431, TrainAcc: 0.8510, ValAcc: 0.6328, ES: 44/50 | BestVal=0.8413@E106\n",
      "Early stopped, loading model from epoch-106\n",
      "Finished running train at 05-15 19:19:47, running time = 0.83s.\n",
      "[GCN + P] ValAcc: 0.8413, TestAcc: 0.8506\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/cora2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 133127\n",
      "Start running train at 05-15 19:19:49\n",
      "Epoch: 0, Time: 0.0084, Loss: 2.0011, TrainAcc: 0.2340, ValAcc: 0.5424, ES: 00/50 | BestVal=0.5424@E0\n",
      "Epoch: 10, Time: 0.0061, Loss: 0.3604, TrainAcc: 0.8879, ValAcc: 0.8450, ES: 00/50 | BestVal=0.8450@E10\n",
      "Epoch: 20, Time: 0.0064, Loss: 0.2959, TrainAcc: 0.9009, ValAcc: 0.8635, ES: 00/50 | BestVal=0.8635@E20\n",
      "Epoch: 30, Time: 0.0045, Loss: 0.2483, TrainAcc: 0.9144, ValAcc: 0.8598, ES: 01/50 | BestVal=0.8672@E29\n",
      "Epoch: 40, Time: 0.0046, Loss: 0.2181, TrainAcc: 0.9243, ValAcc: 0.8598, ES: 11/50 | BestVal=0.8672@E29\n",
      "Epoch: 50, Time: 0.0043, Loss: 0.1853, TrainAcc: 0.9335, ValAcc: 0.8561, ES: 21/50 | BestVal=0.8672@E29\n",
      "Epoch: 60, Time: 0.0044, Loss: 0.1537, TrainAcc: 0.9514, ValAcc: 0.8450, ES: 31/50 | BestVal=0.8672@E29\n",
      "Epoch: 70, Time: 0.0044, Loss: 0.1559, TrainAcc: 0.9446, ValAcc: 0.8229, ES: 41/50 | BestVal=0.8672@E29\n",
      "Early stopped, loading model from epoch-29\n",
      "Finished running train at 05-15 19:19:50, running time = 0.41s.\n",
      "[GCN + E] ValAcc: 0.8672, TestAcc: 0.8727\n",
      "\n",
      "(TA_P_E) ValAcc: 0.8745, TestAcc: 0.8856\n",
      "\n",
      "Running time: 12.21s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/cora/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 265095\n",
      "Start running train at 05-15 19:19:57\n",
      "Epoch: 0, Time: 0.3405, Loss: 1.8477, TrainAcc: 0.2857, ValAcc: 0.5185, ES: 00/50 | BestVal=0.5185@E0\n",
      "Epoch: 10, Time: 0.0057, Loss: 0.2831, TrainAcc: 0.9070, ValAcc: 0.8358, ES: 05/50 | BestVal=0.8616@E5\n",
      "Epoch: 20, Time: 0.0078, Loss: 0.1980, TrainAcc: 0.9335, ValAcc: 0.8616, ES: 00/50 | BestVal=0.8616@E20\n",
      "Epoch: 30, Time: 0.0053, Loss: 0.1279, TrainAcc: 0.9526, ValAcc: 0.8635, ES: 04/50 | BestVal=0.8801@E26\n",
      "Epoch: 40, Time: 0.0053, Loss: 0.0815, TrainAcc: 0.9723, ValAcc: 0.8653, ES: 14/50 | BestVal=0.8801@E26\n",
      "Epoch: 50, Time: 0.0054, Loss: 0.0317, TrainAcc: 0.9938, ValAcc: 0.8579, ES: 24/50 | BestVal=0.8801@E26\n",
      "Epoch: 60, Time: 0.0058, Loss: 0.0120, TrainAcc: 0.9988, ValAcc: 0.8598, ES: 34/50 | BestVal=0.8801@E26\n",
      "Epoch: 70, Time: 0.0058, Loss: 0.0046, TrainAcc: 0.9994, ValAcc: 0.8708, ES: 44/50 | BestVal=0.8801@E26\n",
      "Early stopped, loading model from epoch-26\n",
      "Finished running train at 05-15 19:19:57, running time = 0.79s.\n",
      "[SAGE + TA] ValAcc: 0.8801, TestAcc: 0.8838\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/cora.csv\n",
      "\n",
      "Number of parameters: 233351\n",
      "Start running train at 05-15 19:20:00\n",
      "Epoch: 0, Time: 0.0096, Loss: 1.9218, TrainAcc: 0.2395, ValAcc: 0.3358, ES: 00/50 | BestVal=0.3358@E0\n",
      "Epoch: 10, Time: 0.0086, Loss: 0.4725, TrainAcc: 0.8245, ValAcc: 0.5904, ES: 00/50 | BestVal=0.5904@E10\n",
      "Epoch: 20, Time: 0.0084, Loss: 0.3307, TrainAcc: 0.8892, ValAcc: 0.7269, ES: 00/50 | BestVal=0.7269@E20\n",
      "Epoch: 30, Time: 0.0085, Loss: 0.2369, TrainAcc: 0.9113, ValAcc: 0.8007, ES: 00/50 | BestVal=0.8007@E30\n",
      "Epoch: 40, Time: 0.0061, Loss: 0.1583, TrainAcc: 0.9458, ValAcc: 0.7915, ES: 02/50 | BestVal=0.8100@E38\n",
      "Epoch: 50, Time: 0.0060, Loss: 0.1162, TrainAcc: 0.9612, ValAcc: 0.7989, ES: 12/50 | BestVal=0.8100@E38\n",
      "Epoch: 60, Time: 0.0061, Loss: 0.2998, TrainAcc: 0.9378, ValAcc: 0.7435, ES: 22/50 | BestVal=0.8100@E38\n",
      "Epoch: 70, Time: 0.0060, Loss: 0.1813, TrainAcc: 0.9304, ValAcc: 0.7306, ES: 32/50 | BestVal=0.8100@E38\n",
      "Epoch: 80, Time: 0.0060, Loss: 0.1028, TrainAcc: 0.9661, ValAcc: 0.7749, ES: 42/50 | BestVal=0.8100@E38\n",
      "Early stopped, loading model from epoch-38\n",
      "Finished running train at 05-15 19:20:00, running time = 0.62s.\n",
      "[SAGE + P] ValAcc: 0.8100, TestAcc: 0.7952\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/cora2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 265095\n",
      "Start running train at 05-15 19:20:03\n",
      "Epoch: 0, Time: 0.0093, Loss: 1.8820, TrainAcc: 0.2660, ValAcc: 0.7915, ES: 00/50 | BestVal=0.7915@E0\n",
      "Epoch: 10, Time: 0.0053, Loss: 0.3614, TrainAcc: 0.8787, ValAcc: 0.6993, ES: 07/50 | BestVal=0.8229@E3\n",
      "Epoch: 20, Time: 0.0052, Loss: 0.2848, TrainAcc: 0.9070, ValAcc: 0.7454, ES: 17/50 | BestVal=0.8229@E3\n",
      "Epoch: 30, Time: 0.0074, Loss: 0.2193, TrainAcc: 0.9255, ValAcc: 0.8413, ES: 00/50 | BestVal=0.8413@E30\n",
      "Epoch: 40, Time: 0.0057, Loss: 0.1583, TrainAcc: 0.9458, ValAcc: 0.8487, ES: 09/50 | BestVal=0.8524@E31\n",
      "Epoch: 50, Time: 0.0079, Loss: 0.1085, TrainAcc: 0.9643, ValAcc: 0.8524, ES: 00/50 | BestVal=0.8524@E50\n",
      "Epoch: 60, Time: 0.0052, Loss: 0.0716, TrainAcc: 0.9760, ValAcc: 0.8561, ES: 04/50 | BestVal=0.8616@E56\n",
      "Epoch: 70, Time: 0.0076, Loss: 0.0281, TrainAcc: 0.9945, ValAcc: 0.8672, ES: 00/50 | BestVal=0.8672@E70\n",
      "Epoch: 80, Time: 0.0055, Loss: 0.0105, TrainAcc: 0.9994, ValAcc: 0.8690, ES: 09/50 | BestVal=0.8708@E71\n",
      "Epoch: 90, Time: 0.0052, Loss: 0.0041, TrainAcc: 0.9994, ValAcc: 0.8708, ES: 02/50 | BestVal=0.8745@E88\n",
      "Epoch: 100, Time: 0.0054, Loss: 0.0020, TrainAcc: 1.0000, ValAcc: 0.8727, ES: 02/50 | BestVal=0.8745@E98\n",
      "Epoch: 110, Time: 0.0051, Loss: 0.0011, TrainAcc: 1.0000, ValAcc: 0.8727, ES: 07/50 | BestVal=0.8745@E103\n",
      "Epoch: 120, Time: 0.0050, Loss: 0.0007, TrainAcc: 1.0000, ValAcc: 0.8653, ES: 17/50 | BestVal=0.8745@E103\n",
      "Epoch: 130, Time: 0.0052, Loss: 0.0005, TrainAcc: 1.0000, ValAcc: 0.8653, ES: 27/50 | BestVal=0.8745@E103\n",
      "Epoch: 140, Time: 0.0052, Loss: 0.0004, TrainAcc: 1.0000, ValAcc: 0.8653, ES: 37/50 | BestVal=0.8745@E103\n",
      "Epoch: 150, Time: 0.0057, Loss: 0.0004, TrainAcc: 1.0000, ValAcc: 0.8672, ES: 47/50 | BestVal=0.8745@E103\n",
      "Early stopped, loading model from epoch-103\n",
      "Finished running train at 05-15 19:20:04, running time = 0.88s.\n",
      "[SAGE + E] ValAcc: 0.8745, TestAcc: 0.8690\n",
      "\n",
      "(TA_P_E) ValAcc: 0.8875, TestAcc: 0.8819\n",
      "\n",
      "Running time: 12.44s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name SAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An import exception occurred\n",
      "/media/test/noppanat/miniconda3/envs/cs471/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n",
      "Loading pretrained LM features (title and abstract) ...\n",
      "LM_emb_path: prt_lm/cora/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 895118\n",
      "Start running train at 05-15 19:20:11\n",
      "Epoch: 0, Time: 0.5349, Loss: 3.7306, TrainAcc: 0.1096, ValAcc: 0.1070, ES: 00/50 | BestVal=0.1070@E0\n",
      "Epoch: 10, Time: 0.0352, Loss: 0.6655, TrainAcc: 0.8054, ValAcc: 0.8727, ES: 00/50 | BestVal=0.8727@E10\n",
      "Epoch: 20, Time: 0.0289, Loss: 0.3556, TrainAcc: 0.8916, ValAcc: 0.8764, ES: 03/50 | BestVal=0.8782@E17\n",
      "Epoch: 30, Time: 0.0288, Loss: 0.2597, TrainAcc: 0.9163, ValAcc: 0.8838, ES: 02/50 | BestVal=0.8856@E28\n",
      "Epoch: 40, Time: 0.0290, Loss: 0.2330, TrainAcc: 0.9169, ValAcc: 0.8782, ES: 12/50 | BestVal=0.8856@E28\n",
      "Epoch: 50, Time: 0.0283, Loss: 0.1709, TrainAcc: 0.9470, ValAcc: 0.8653, ES: 22/50 | BestVal=0.8856@E28\n",
      "Epoch: 60, Time: 0.0286, Loss: 0.1165, TrainAcc: 0.9600, ValAcc: 0.8690, ES: 32/50 | BestVal=0.8856@E28\n",
      "Epoch: 70, Time: 0.0282, Loss: 0.0884, TrainAcc: 0.9692, ValAcc: 0.8690, ES: 42/50 | BestVal=0.8856@E28\n",
      "Early stopped, loading model from epoch-28\n",
      "Finished running train at 05-15 19:20:14, running time = 2.91s.\n",
      "[RevGAT + TA] ValAcc: 0.8856, TestAcc: 0.8930\n",
      "\n",
      "Loading top-k prediction features ...\n",
      "Loading topk preds from gpt_preds/cora.csv\n",
      "\n",
      "Number of parameters: 797582\n",
      "Start running train at 05-15 19:20:16\n",
      "Epoch: 0, Time: 0.0364, Loss: 3.8190, TrainAcc: 0.0905, ValAcc: 0.1439, ES: 00/50 | BestVal=0.1439@E0\n",
      "Epoch: 10, Time: 0.0294, Loss: 1.2818, TrainAcc: 0.6324, ValAcc: 0.7399, ES: 01/50 | BestVal=0.7435@E9\n",
      "Epoch: 20, Time: 0.0347, Loss: 0.9033, TrainAcc: 0.7217, ValAcc: 0.7749, ES: 00/50 | BestVal=0.7749@E20\n",
      "Epoch: 30, Time: 0.0295, Loss: 0.7293, TrainAcc: 0.7457, ValAcc: 0.7823, ES: 02/50 | BestVal=0.7915@E28\n",
      "Epoch: 40, Time: 0.0289, Loss: 0.6796, TrainAcc: 0.7629, ValAcc: 0.7915, ES: 05/50 | BestVal=0.8007@E35\n",
      "Epoch: 50, Time: 0.0342, Loss: 0.6357, TrainAcc: 0.7845, ValAcc: 0.8081, ES: 00/50 | BestVal=0.8081@E50\n",
      "Epoch: 60, Time: 0.0329, Loss: 0.5949, TrainAcc: 0.7833, ValAcc: 0.8155, ES: 00/50 | BestVal=0.8155@E60\n",
      "Epoch: 70, Time: 0.0285, Loss: 0.5277, TrainAcc: 0.8110, ValAcc: 0.8155, ES: 03/50 | BestVal=0.8192@E67\n",
      "Epoch: 80, Time: 0.0284, Loss: 0.5116, TrainAcc: 0.8128, ValAcc: 0.8026, ES: 13/50 | BestVal=0.8192@E67\n",
      "Epoch: 90, Time: 0.0287, Loss: 0.5036, TrainAcc: 0.8190, ValAcc: 0.8173, ES: 02/50 | BestVal=0.8210@E88\n",
      "Epoch: 100, Time: 0.0331, Loss: 0.4984, TrainAcc: 0.8393, ValAcc: 0.8358, ES: 00/50 | BestVal=0.8358@E100\n",
      "Epoch: 110, Time: 0.0292, Loss: 0.4658, TrainAcc: 0.8356, ValAcc: 0.8247, ES: 10/50 | BestVal=0.8358@E100\n",
      "Epoch: 120, Time: 0.0297, Loss: 0.4262, TrainAcc: 0.8516, ValAcc: 0.8339, ES: 03/50 | BestVal=0.8358@E117\n",
      "Epoch: 130, Time: 0.0285, Loss: 0.4514, TrainAcc: 0.8313, ValAcc: 0.8229, ES: 13/50 | BestVal=0.8358@E117\n",
      "Epoch: 140, Time: 0.0286, Loss: 0.4180, TrainAcc: 0.8516, ValAcc: 0.8210, ES: 23/50 | BestVal=0.8358@E117\n",
      "Epoch: 150, Time: 0.0288, Loss: 0.4104, TrainAcc: 0.8504, ValAcc: 0.8266, ES: 09/50 | BestVal=0.8358@E141\n",
      "Epoch: 160, Time: 0.0281, Loss: 0.4086, TrainAcc: 0.8473, ValAcc: 0.8173, ES: 04/50 | BestVal=0.8358@E156\n",
      "Epoch: 170, Time: 0.0289, Loss: 0.4101, TrainAcc: 0.8547, ValAcc: 0.8229, ES: 03/50 | BestVal=0.8376@E167\n",
      "Epoch: 180, Time: 0.0290, Loss: 0.4070, TrainAcc: 0.8510, ValAcc: 0.8137, ES: 05/50 | BestVal=0.8395@E175\n",
      "Epoch: 190, Time: 0.0291, Loss: 0.3664, TrainAcc: 0.8602, ValAcc: 0.8247, ES: 15/50 | BestVal=0.8395@E175\n",
      "Finished running train at 05-15 19:20:22, running time = 6.04s.\n",
      "[RevGAT + P] ValAcc: 0.8432, TestAcc: 0.7970\n",
      "\n",
      "Loading pretrained LM features (explanations) ...\n",
      "LM_emb_path: prt_lm/cora2/microsoft/deberta-base-seed0.emb\n",
      "\n",
      "Number of parameters: 895118\n",
      "Start running train at 05-15 19:20:24\n",
      "Epoch: 0, Time: 0.0351, Loss: 3.5858, TrainAcc: 0.1355, ValAcc: 0.1421, ES: 00/50 | BestVal=0.1421@E0\n",
      "Epoch: 10, Time: 0.0349, Loss: 0.8114, TrainAcc: 0.7703, ValAcc: 0.8542, ES: 00/50 | BestVal=0.8542@E10\n",
      "Epoch: 20, Time: 0.0270, Loss: 0.4870, TrainAcc: 0.8547, ValAcc: 0.8690, ES: 04/50 | BestVal=0.8801@E16\n",
      "Epoch: 30, Time: 0.0334, Loss: 0.3899, TrainAcc: 0.8812, ValAcc: 0.8838, ES: 00/50 | BestVal=0.8838@E30\n",
      "Epoch: 40, Time: 0.0267, Loss: 0.3702, TrainAcc: 0.8781, ValAcc: 0.8598, ES: 10/50 | BestVal=0.8838@E30\n",
      "Epoch: 50, Time: 0.0281, Loss: 0.2767, TrainAcc: 0.9150, ValAcc: 0.8653, ES: 20/50 | BestVal=0.8838@E30\n",
      "Epoch: 60, Time: 0.0280, Loss: 0.2485, TrainAcc: 0.9119, ValAcc: 0.8708, ES: 30/50 | BestVal=0.8838@E30\n",
      "Epoch: 70, Time: 0.0281, Loss: 0.1670, TrainAcc: 0.9403, ValAcc: 0.8782, ES: 06/50 | BestVal=0.8838@E64\n",
      "Epoch: 80, Time: 0.0279, Loss: 0.1392, TrainAcc: 0.9538, ValAcc: 0.8672, ES: 16/50 | BestVal=0.8838@E64\n",
      "Epoch: 90, Time: 0.0283, Loss: 0.1214, TrainAcc: 0.9538, ValAcc: 0.8764, ES: 26/50 | BestVal=0.8838@E64\n",
      "Epoch: 100, Time: 0.0290, Loss: 0.0980, TrainAcc: 0.9661, ValAcc: 0.8727, ES: 36/50 | BestVal=0.8838@E64\n",
      "Epoch: 110, Time: 0.0289, Loss: 0.1054, TrainAcc: 0.9631, ValAcc: 0.8764, ES: 46/50 | BestVal=0.8838@E64\n",
      "Early stopped, loading model from epoch-64\n",
      "Finished running train at 05-15 19:20:28, running time = 3.35s.\n",
      "[RevGAT + E] ValAcc: 0.8838, TestAcc: 0.8708\n",
      "\n",
      "(TA_P_E) ValAcc: 0.8893, TestAcc: 0.8930\n",
      "\n",
      "Running time: 22.64s\n"
     ]
    }
   ],
   "source": [
    "!python -m core.trainEnsemble dataset $dataset gnn.model.name RevGAT gnn.train.lr 0.002 gnn.train.dropout 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs471",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
